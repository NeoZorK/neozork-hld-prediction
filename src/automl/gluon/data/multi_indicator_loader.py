"""
<<<<<<< HEAD
Multi-Indicator data Loader for trading Strategy
Data uploader for multiple indicators of trade strategy
=======
Multi-Indicator Data Loader for Trading Strategy
Ð—Ð°Ð³Ñ€ÑƒÐ·Ñ‡Ð¸Ðº Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð¼Ð½Ð¾Ð¶ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ñ… Ð¸Ð½Ð´Ð¸ÐºÐ°Ñ‚Ð¾Ñ€Ð¾Ð² Ñ‚Ð¾Ñ€Ð³Ð¾Ð²Ð¾Ð¹ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ð¸
>>>>>>> origin/master
"""

import pandas as pd
import numpy as np
from typing import Dict, Any, List, Optional, Tuple
from pathlib import Path
import logging
<<<<<<< HEAD
from .universal_loader import UniversaldataLoader
from .auto_data_scanner import AutodataScanner, InteractivedataSelector
=======
from .universal_loader import UniversalDataLoader
from .auto_data_scanner import AutoDataScanner, InteractiveDataSelector
>>>>>>> origin/master

logger = logging.getLogger(__name__)


class MultiIndicatorLoader:
<<<<<<< HEAD
 """
 Loads and combines data from multiple trading indicators (CSVExport, WAVE2, SHORT3).
Uploads and connects data from multiple trade indicators.
 """

 def __init__(self, base_path: str = "data/cache/csv_converted/"):
 """
 Initialize Multi-Indicator Loader.

 Args:
 base_path: Base path to data directory
 """
 self.base_path = Path(base_path)
 self.data_loader = UniversaldataLoader()
 self.scanner = AutodataScanner(base_path)
 self.selector = InteractivedataSelector(self.scanner)

 def load_basic_data(self, symbol: str, Timeframe: str) -> pd.dataFrame:
 """
 Load basic OHLCV data for automatic feature generation.
Load the basic OHLCV data for automatic character generation.

 Args:
 symbol: Trading symbol (e.g., 'BTCUSD')
 Timeframe: Timeframe (e.g., 'D1', 'H1', 'M15')

 Returns:
 dataFrame with basic OHLCV data
 """
 logger.info(f"Loading basic OHLCV data for {symbol} {Timeframe}...")

 # Try to load CSVExport data (contains OHLCV)
 csv_export_file = self.base_path / f"CSVExport_{symbol}_PERIOD_{Timeframe}.parquet"

 if csv_export_file.exists():
 logger.info(f"Loading CSVExport data from {csv_export_file}")
 data = self.data_loader.load_file(str(csv_export_file))

 # Ensure we have basic OHLCV columns
 required_columns = ['Open', 'High', 'Low', 'Close', 'Volume']
 if all(col in data.columns for col in required_columns):
 logger.info(f"âœ… Basic OHLCV data loaded: {len(data)} rows, {len(data.columns)} columns")
 return data
 else:
 logger.warning(f"âš ï¸ CSVExport data Missing required OHLCV columns: {required_columns}")

 # If no CSVExport data, create synthetic data for demonstration
 logger.warning(f"âš ï¸ No basic data found for {symbol} {Timeframe}, Creating synthetic data...")
 return self._create_synthetic_data(symbol, Timeframe)

 def _create_synthetic_data(self, symbol: str, Timeframe: str) -> pd.dataFrame:
 """
 Create synthetic OHLCV data for demonstration.
Create synthetic OHLCV data for demonstration.
 """
 import numpy as np
 from datetime import datetime, timedelta

 # Generate synthetic data
 n_periods = 1000
 dates = pd.date_range(start='2020-01-01', periods=n_periods, freq='D')

 # Generate price data with some trend and volatility
 np.random.seed(42) # For reproducibility
 base_price = 50000 if 'BTC' in symbol else 1.0
 returns = np.random.normal(0, 0.02, n_periods) # 2% daily volatility
 prices = base_price * np.exp(np.cumsum(returns))

 # Generate OHLCV data
 data = pd.dataFrame({
 'Open': prices * (1 + np.random.normal(0, 0.001, n_periods)),
 'High': prices * (1 + np.abs(np.random.normal(0, 0.01, n_periods))),
 'Low': prices * (1 - np.abs(np.random.normal(0, 0.01, n_periods))),
 'Close': prices,
 'Volume': np.random.uniform(1000, 10000, n_periods)
 })

 # Ensure High >= max(Open, Close) and Low <= min(Open, Close)
 data['High'] = np.maximum(data['High'], np.maximum(data['Open'], data['Close']))
 data['Low'] = np.minimum(data['Low'], np.minimum(data['Open'], data['Close']))

 data.index = dates

 logger.info(f"âœ… Synthetic data created: {len(data)} rows")
 return data

 def load_symbol_data(self, symbol: str, Timeframe: str, indicator: str = None) -> Dict[str, pd.dataFrame]:
 """
 Load all indicator data for a specific symbol and Timeframe.
Load all data indicators for a specific symbol and Timeframe.

 Args:
 symbol: Trading symbol (e.g., 'BTCUSD')
 Timeframe: Timeframe (e.g., 'D1', 'H1', 'M15')

 Returns:
 Dictionary with loaded dataframes for each indicator
 """
 logger.info(f"Loading data for {symbol} {Timeframe}...")

 data_sources = {}

 # Define file patterns for each indicator
 file_patterns = {
 'csv_export': f"CSVExport_{symbol}_PERIOD_{Timeframe}.parquet",
 'wave2': f"WAVE2_{symbol}_PERIOD_{Timeframe}.parquet",
 'short3': f"SHORT3_{symbol}_PERIOD_{Timeframe}.parquet"
 }

 for indicator, filename in file_patterns.items():
 file_path = self.base_path / filename

 try:
 if file_path.exists():
 logger.info(f"Loading {indicator} from {file_path}")
 data = self.data_loader.load_file(str(file_path))
 data_sources[indicator] = data
 logger.info(f"âœ… {indicator}: {len(data)} rows, {len(data.columns)} columns")
 else:
 logger.warning(f"âš ï¸ File not found: {file_path}")
 data_sources[indicator] = pd.dataFrame()

 except Exception as e:
 logger.error(f"âŒ Error Loading {indicator}: {e}")
 data_sources[indicator] = pd.dataFrame()

 return data_sources

 def load_multiple_symbols(self, symbols: List[str], Timeframes: List[str]) -> Dict[str, Dict[str, pd.dataFrame]]:
 """
 Load data for multiple symbols and Timeframes.
Load data for multiple characters and Times.

 Args:
 symbols: List of Trading symbols
 Timeframes: List of Timeframes

 Returns:
 Nested dictionary: {symbol: {Timeframe: {indicator: dataframe}}}
 """
 logger.info(f"Loading data for {len(symbols)} symbols and {len(Timeframes)} Timeframes...")

 all_data = {}

 for symbol in symbols:
 all_data[symbol] = {}

 for Timeframe in timeframes:
 logger.info(f"ðŸ“Š Loading {symbol} {Timeframe}...")

 try:
 symbol_data = self.load_symbol_data(symbol, Timeframe)
 all_data[symbol][Timeframe] = symbol_data

 # Log summary
 total_rows = sum(len(df) for df in symbol_data.values() if not df.empty)
 indicators_loaded = sum(1 for df in symbol_data.values() if not df.empty)

 logger.info(f"âœ… {symbol} {Timeframe}: {total_rows} total rows, {indicators_loaded} indicators")

 except Exception as e:
 logger.error(f"âŒ Failed to load {symbol} {Timeframe}: {e}")
 all_data[symbol][Timeframe] = {}

 return all_data

 def combine_indicators(self, data_sources: Dict[str, pd.dataFrame]) -> pd.dataFrame:
 """
 Combine data from multiple indicators into a single dataframe.
Merge data from multiple indicators into one date frame.

 Args:
 data_sources: Dictionary with indicator data

 Returns:
 Combined dataframe
 """
 logger.info("Combining indicator data...")

 if not data_sources or all(df.empty for df in data_sources.values()):
 logger.warning("No data to combine")
 return pd.dataFrame()

 # start with CSVExport as base (has OHLCV data)
 if 'csv_export' in data_sources and not data_sources['csv_export'].empty:
 combined_df = data_sources['csv_export'].copy()
 logger.info(f"Base data from CSVExport: {len(combined_df)} rows")
 else:
 # Use first available indicator as base
 base_indicator = next((k for k, v in data_sources.items() if not v.empty), None)
 if base_indicator is None:
 logger.error("No data available to combine")
 return pd.dataFrame()

 combined_df = data_sources[base_indicator].copy()
 logger.info(f"Base data from {base_indicator}: {len(combined_df)} rows")

 # Add other indicators
 for indicator, df in data_sources.items():
 if indicator == 'csv_export' or df.empty:
 continue

 logger.info(f"Adding {indicator} data...")

 # Select only indicator-specific columns (exclude OHLCV duplicates)
 indicator_columns = [col for col in df.columns
 if col not in ['Close', 'High', 'Open', 'Low', 'Volume']]

 if indicator_columns:
 indicator_df = df[indicator_columns].copy()

 # Add prefix to avoid column name conflicts
 indicator_df.columns = [f"{indicator}_{col}" for col in indicator_df.columns]

 # Merge with combined dataframe
 combined_df = combined_df.join(indicator_df, how='outer')
 logger.info(f"âœ… Added {len(indicator_columns)} columns from {indicator}")
 else:
 logger.warning(f"No unique columns in {indicator}")

 logger.info(f"Combined data: {len(combined_df)} rows, {len(combined_df.columns)} columns")
 return combined_df

 def create_target_variable(self, data: pd.dataFrame, method: str = 'price_direction', problem_type: str = 'regression') -> pd.dataFrame:
 """
 Create target variable for machine learning.
Create a target variable for machine lightning.

 Args:
 data: Input dataframe
 method: Method for Creating target ('price_direction', 'price_change', 'volatility')
 problem_type: Type of problem ('regression', 'binary', 'multiclass')

 Returns:
 dataframe with target variable added
 """
 logger.info(f"Creating target variable Using method: {method}, problem_type: {problem_type}")

 result_df = data.copy()

 if problem_type == 'binary':
 # Binary classification: 1 if price goes up, 0 if down
 result_df['target'] = (result_df['Close'].diff() > 0).astype(int)
 logger.info("âœ… Binary classification target: 1=up, 0=down")

 elif problem_type == 'multiclass':
 # Multiclass classification: 0=down, 1=sideways, 2=up
 price_change = result_df['Close'].pct_change()
 result_df['target'] = pd.cut(price_change,
 bins=[-np.inf, -0.01, 0.01, np.inf],
 labels=[0, 1, 2]).astype(int)
 logger.info("âœ… Multiclass target: 0=down, 1=sideways, 2=up")

 elif problem_type == 'regression':
 if method == 'price_change':
 # Regression: actual price change percentage
 result_df['target'] = result_df['Close'].pct_change()
 logger.info("âœ… Regression target: price change percentage")
 elif method == 'volatility':
 # Regression: rolling volatility
 result_df['target'] = result_df['Close'].rolling(20).std()
 logger.info("âœ… Regression target: rolling volatility")
 else:
 # Default: price change percentage
 result_df['target'] = result_df['Close'].pct_change()
 logger.info("âœ… Regression target: price change percentage (default)")
 else:
 raise ValueError(f"Unknown problem type: {problem_type}")

 # Remove rows with NaN target
 initial_rows = len(result_df)
 result_df = result_df.dropna(subset=['target'])
 removed_rows = initial_rows - len(result_df)

 if removed_rows > 0:
 logger.info(f"Removed {removed_rows} rows with NaN target")

 logger.info(f"Target variable created: {len(result_df)} rows")
 return result_df

 def add_Technical_indicators(self, data: pd.dataFrame) -> pd.dataFrame:
 """
 Add common Technical indicators to the data.
Add general technical indicators to the data.

 Args:
 data: Input dataframe

 Returns:
 dataframe with Technical indicators added
 """
 logger.info("Adding Technical indicators...")

 result_df = data.copy()

 # Moving averages
 for period in [5, 10, 20, 50]:
 result_df[f'sma_{period}'] = result_df['Close'].rolling(period).mean()
 result_df[f'ema_{period}'] = result_df['Close'].ewm(span=period).mean()

 # Price-based indicators
 result_df['price_change'] = result_df['Close'].pct_change()
 result_df['high_low_ratio'] = result_df['High'] / result_df['Low']
 result_df['close_open_ratio'] = result_df['Close'] / result_df['Open']

 # Volatility indicators
 result_df['volatility_20'] = result_df['Close'].rolling(20).std()
 result_df['volatility_50'] = result_df['Close'].rolling(50).std()

 # Volume indicators
 if 'Volume' in result_df.columns:
 result_df['volume_sma_20'] = result_df['Volume'].rolling(20).mean()
 result_df['volume_ratio'] = result_df['Volume'] / result_df['volume_sma_20']

 # RSI
 result_df['rsi_14'] = self._calculate_rsi(result_df['Close'], 14)
 result_df['rsi_21'] = self._calculate_rsi(result_df['Close'], 21)

 # MACD
 macd_line, signal_line, histogram = self._calculate_macd(result_df['Close'])
 result_df['macd'] = macd_line
 result_df['macd_signal'] = signal_line
 result_df['macd_histogram'] = histogram

 logger.info(f"Added {len([col for col in result_df.columns if col not in data.columns])} Technical indicators")
 return result_df

 def _calculate_rsi(self, prices: pd.Series, period: int = 14) -> pd.Series:
 """Calculate RSI indicator."""
 delta = prices.diff()
 gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
 loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
 rs = gain / loss
 rsi = 100 - (100 / (1 + rs))
 return rsi

 def _calculate_macd(self, prices: pd.Series, fast: int = 12, slow: int = 26, signal: int = 9) -> Tuple[pd.Series, pd.Series, pd.Series]:
 """Calculate MACD indicator."""
 ema_fast = prices.ewm(span=fast).mean()
 ema_slow = prices.ewm(span=slow).mean()
 macd_line = ema_fast - ema_slow
 signal_line = macd_line.ewm(span=signal).mean()
 histogram = macd_line - signal_line
 return macd_line, signal_line, histogram

 def get_data_summary(self, data: pd.dataFrame) -> Dict[str, Any]:
 """
 Get comprehensive summary of the data.
Get a comprehensive set of data.

 Args:
 data: Input dataframe

 Returns:
 Dictionary with data summary
 """
 if data.empty:
 return {
 'rows': 0,
 'columns': 0,
 'date_range': None,
 'Missing_values': {},
 'data_types': {},
 'summary': 'No data available'
 }

 summary = {
 'rows': len(data),
 'columns': len(data.columns),
 'date_range': {
 'start': data.index.min() if hasattr(data.index, 'min') else None,
 'end': data.index.max() if hasattr(data.index, 'max') else None
 },
 'Missing_values': data.isnull().sum().to_dict(),
 'data_types': data.dtypes.to_dict(),
 'memory_usage': data.memory_usage(deep=True).sum(),
 'numeric_columns': len(data.select_dtypes(include=[np.number]).columns),
 'categorical_columns': len(data.select_dtypes(include=['object', 'category']).columns)
 }

 # Add target variable info if exists
 if 'target' in data.columns:
 target_info = data['target'].describe().to_dict()
 target_info['unique_values'] = data['target'].nunique()
 summary['target_variable'] = target_info

 return summary

 def auto_scan_and_select(self, interactive: bool = True) -> Dict[str, Any]:
 """
 Automatically scan directory and select data interactively.
Automatically scan the directory and interactively select the data.

 Args:
 interactive: Whether to Use interactive selection

 Returns:
 Dictionary with selection results
 """
 logger.info("ðŸ” Auto-scanning data directory...")

 # Scan directory
 scan_results = self.scanner.scan_directory()

 if not scan_results.get('scan_successful', False):
 logger.error(f"âŒ Scan failed: {scan_results.get('error')}")
 return {'success': False, 'error': scan_results.get('error')}

 if interactive:
 # Interactive selection
 logger.info("ðŸŽ¯ starting interactive selection...")
 selection = self.selector.interactive_selection()

 if not selection.get('success', False):
 logger.error(f"âŒ Selection failed: {selection.get('error')}")
 return selection

 return selection
 else:
 # Auto-select first available combination
 logger.info("ðŸ¤– Auto-selecting first available combination...")

 if not self.scanner.available_data:
 return {'success': False, 'error': 'No data available'}

 # Get first indicator
 first_indicator = List(self.scanner.available_data.keys())[0]

 # Get first symbol for this indicator
 first_symbol = List(self.scanner.available_data[first_indicator].keys())[0]

 # Get all Timeframes for this symbol
 Timeframes = self.scanner.get_symbol_Timeframes(first_indicator, first_symbol)

 selection = {
 'success': True,
 'indicator': first_indicator,
 'symbol': first_symbol,
 'Timeframes': Timeframes,
 'file_paths': {}
 }

 # Get file paths
 for Timeframe in timeframes:
 file_path = self.scanner.get_file_path(first_indicator, first_symbol, Timeframe)
 if file_path:
 selection['file_paths'][Timeframe] = file_path

 logger.info(f"âœ… Auto-selected: {first_indicator} {first_symbol} {Timeframes}")
 return selection

 def load_selected_data(self, selection: Dict[str, Any]) -> pd.dataFrame:
 """
 Load data based on selection results.
Load the data on base of the selection results.

 Args:
 selection: Selection results from auto_scan_and_select

 Returns:
 Combined dataframe with all selected data
 """
 if not selection.get('success', False):
 logger.error(f"âŒ Cannot load data: {selection.get('error')}")
 return pd.dataFrame()

 indicator = selection['indicator']
 symbol = selection['symbol']
 Timeframes = selection['Timeframes']
 file_paths = selection.get('file_paths', {})

 logger.info(f"ðŸ“Š Loading data for {indicator} {symbol} across {len(Timeframes)} Timeframes...")

 all_data = []

 for Timeframe in timeframes:
 if Timeframe in file_paths:
 file_path = file_paths[Timeframe]

 try:
 logger.info(f"ðŸ“ Loading {Timeframe} from {file_path}")
 data = self.data_loader.load_file(file_path)

 # Add metadata
 data['indicator'] = indicator
 data['symbol'] = symbol
 data['Timeframe'] = Timeframe

 # Add Timeframe weight
 Timeframe_weights = {'M1': 1, 'M5': 2, 'M15': 3, 'H1': 4, 'H4': 8, 'D1': 16, 'W1': 32, 'MN1': 64}
 data['Timeframe_weight'] = Timeframe_weights.get(Timeframe, 1)

 all_data.append(data)
 logger.info(f"âœ… {Timeframe}: {len(data)} rows, {len(data.columns)} columns")

 except Exception as e:
 logger.error(f"âŒ Failed to load {Timeframe}: {e}")
 continue
 else:
 logger.warning(f"âš ï¸ No file path for {Timeframe}")

 if not all_data:
 logger.error("âŒ No data loaded successfully")
 return pd.dataFrame()

 # Combine all data
 logger.info("ðŸ”„ Combining all data...")
 combined_data = pd.concat(all_data, ignore_index=True)

 # Add Technical indicators
 combined_data = self.add_Technical_indicators(combined_data)

 logger.info(f"ðŸ“Š Final combined data: {len(combined_data)} rows, {len(combined_data.columns)} columns")

 return combined_data

 def load_multi_indicator_data(self, symbol: str, Timeframes: List[str]) -> pd.dataFrame:
 """
 Load data from multiple indicators (CSVExport/SCHR, WAVE2, SHORT3) for a symbol.
Load data from multiple indicators for symbol.

 Args:
 symbol: Trading symbol
 Timeframes: List of Timeframes

 Returns:
 Combined dataframe with all indicators
 """
 logger.info(f"ðŸ“Š Loading multi-indicator data for {symbol} across {len(Timeframes)} Timeframes...")

 all_combined_data = []

 for Timeframe in timeframes:
 try:
 logger.info(f"ðŸ“Š Loading {symbol} {Timeframe}...")

 # Load all indicators for this symbol/Timeframe
 symbol_data = self.load_symbol_data(symbol, Timeframe)

 # Combine indicators
 combined_symbol_data = self.combine_indicators(symbol_data)

 if not combined_symbol_data.empty:
 # Add metadata
 combined_symbol_data['symbol'] = symbol
 combined_symbol_data['Timeframe'] = Timeframe

 # Add Timeframe weight
 Timeframe_weights = {'M1': 1, 'M5': 2, 'M15': 3, 'H1': 4, 'H4': 8, 'D1': 16, 'W1': 32, 'MN1': 64}
 combined_symbol_data['Timeframe_weight'] = Timeframe_weights.get(Timeframe, 1)

 all_combined_data.append(combined_symbol_data)
 logger.info(f"âœ… {symbol} {Timeframe}: {len(combined_symbol_data)} rows")
 else:
 logger.warning(f"âš ï¸ No data for {symbol} {Timeframe}")

 except Exception as e:
 logger.error(f"âŒ Failed to load {symbol} {Timeframe}: {e}")
 continue

 if not all_combined_data:
 logger.error("âŒ No data loaded successfully")
 return pd.dataFrame()

 # Combine all data
 logger.info("ðŸ”„ Combining all multi-indicator data...")
 final_data = pd.concat(all_combined_data, ignore_index=True)

 # Add Technical indicators
 final_data = self.add_Technical_indicators(final_data)

 logger.info(f"ðŸ“Š Final multi-indicator data: {len(final_data)} rows, {len(final_data.columns)} columns")

 return final_data

 def auto_load_data(self, interactive: bool = True) -> pd.dataFrame:
 """
 Complete auto-Loading process: scan, select, and load data.
Full process of automatic download: scanning, selecting and Loading data.

 Args:
 interactive: Whether to Use interactive selection

 Returns:
 Combined dataframe with all selected data
 """
 logger.info("ðŸš€ starting auto-Loading process...")

 # Step 1: Auto-scan and select
 selection = self.auto_scan_and_select(interactive=interactive)

 if not selection.get('success', False):
 logger.error(f"âŒ Auto-selection failed: {selection.get('error')}")
 return pd.dataFrame()

 # Step 2: Load selected data
 combined_data = self.load_selected_data(selection)

 if combined_data.empty:
 logger.error("âŒ No data loaded")
 return pd.dataFrame()

 logger.info("âœ… Auto-Loading COMPLETED successfully!")
 return combined_data
=======
    """
    Loads and combines data from multiple trading indicators (CSVExport, WAVE2, SHORT3).
    Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ Ð¸ Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÑÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð· Ð¼Ð½Ð¾Ð¶ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ñ… Ñ‚Ð¾Ñ€Ð³Ð¾Ð²Ñ‹Ñ… Ð¸Ð½Ð´Ð¸ÐºÐ°Ñ‚Ð¾Ñ€Ð¾Ð².
    """
    
    def __init__(self, base_path: str = "data/cache/csv_converted/"):
        """
        Initialize Multi-Indicator Loader.
        
        Args:
            base_path: Base path to data directory
        """
        self.base_path = Path(base_path)
        self.data_loader = UniversalDataLoader()
        self.scanner = AutoDataScanner(base_path)
        self.selector = InteractiveDataSelector(self.scanner)
        
    def load_basic_data(self, symbol: str, timeframe: str) -> pd.DataFrame:
        """
        Load basic OHLCV data for automatic feature generation.
        Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ðµ OHLCV Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð»Ñ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð².
        
        Args:
            symbol: Trading symbol (e.g., 'BTCUSD')
            timeframe: Timeframe (e.g., 'D1', 'H1', 'M15')
            
        Returns:
            DataFrame with basic OHLCV data
        """
        logger.info(f"Loading basic OHLCV data for {symbol} {timeframe}...")
        
        # Try to load CSVExport data (contains OHLCV)
        csv_export_file = self.base_path / f"CSVExport_{symbol}_PERIOD_{timeframe}.parquet"
        
        if csv_export_file.exists():
            logger.info(f"Loading CSVExport data from {csv_export_file}")
            data = self.data_loader.load_file(str(csv_export_file))
            
            # Ensure we have basic OHLCV columns
            required_columns = ['Open', 'High', 'Low', 'Close', 'Volume']
            if all(col in data.columns for col in required_columns):
                logger.info(f"âœ… Basic OHLCV data loaded: {len(data)} rows, {len(data.columns)} columns")
                return data
            else:
                logger.warning(f"âš ï¸ CSVExport data missing required OHLCV columns: {required_columns}")
        
        # If no CSVExport data, create synthetic data for demonstration
        logger.warning(f"âš ï¸ No basic data found for {symbol} {timeframe}, creating synthetic data...")
        return self._create_synthetic_data(symbol, timeframe)
    
    def _create_synthetic_data(self, symbol: str, timeframe: str) -> pd.DataFrame:
        """
        Create synthetic OHLCV data for demonstration.
        Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ ÑÐ¸Ð½Ñ‚ÐµÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ OHLCV Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð»Ñ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸.
        """
        import numpy as np
        from datetime import datetime, timedelta
        
        # Generate synthetic data
        n_periods = 1000
        dates = pd.date_range(start='2020-01-01', periods=n_periods, freq='D')
        
        # Generate price data with some trend and volatility
        np.random.seed(42)  # For reproducibility
        base_price = 50000 if 'BTC' in symbol else 1.0
        returns = np.random.normal(0, 0.02, n_periods)  # 2% daily volatility
        prices = base_price * np.exp(np.cumsum(returns))
        
        # Generate OHLCV data
        data = pd.DataFrame({
            'Open': prices * (1 + np.random.normal(0, 0.001, n_periods)),
            'High': prices * (1 + np.abs(np.random.normal(0, 0.01, n_periods))),
            'Low': prices * (1 - np.abs(np.random.normal(0, 0.01, n_periods))),
            'Close': prices,
            'Volume': np.random.uniform(1000, 10000, n_periods)
        })
        
        # Ensure High >= max(Open, Close) and Low <= min(Open, Close)
        data['High'] = np.maximum(data['High'], np.maximum(data['Open'], data['Close']))
        data['Low'] = np.minimum(data['Low'], np.minimum(data['Open'], data['Close']))
        
        data.index = dates
        
        logger.info(f"âœ… Synthetic data created: {len(data)} rows")
        return data

    def load_symbol_data(self, symbol: str, timeframe: str, indicator: str = None) -> Dict[str, pd.DataFrame]:
        """
        Load all indicator data for a specific symbol and timeframe.
        Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð²ÑÐµ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð½Ð´Ð¸ÐºÐ°Ñ‚Ð¾Ñ€Ð¾Ð² Ð´Ð»Ñ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¾Ð³Ð¾ ÑÐ¸Ð¼Ð²Ð¾Ð»Ð° Ð¸ Ñ‚Ð°Ð¹Ð¼Ñ„Ñ€ÐµÐ¹Ð¼Ð°.
        
        Args:
            symbol: Trading symbol (e.g., 'BTCUSD')
            timeframe: Timeframe (e.g., 'D1', 'H1', 'M15')
            
        Returns:
            Dictionary with loaded dataframes for each indicator
        """
        logger.info(f"Loading data for {symbol} {timeframe}...")
        
        data_sources = {}
        
        # Define file patterns for each indicator
        file_patterns = {
            'csv_export': f"CSVExport_{symbol}_PERIOD_{timeframe}.parquet",
            'wave2': f"WAVE2_{symbol}_PERIOD_{timeframe}.parquet", 
            'short3': f"SHORT3_{symbol}_PERIOD_{timeframe}.parquet"
        }
        
        for indicator, filename in file_patterns.items():
            file_path = self.base_path / filename
            
            try:
                if file_path.exists():
                    logger.info(f"Loading {indicator} from {file_path}")
                    data = self.data_loader.load_file(str(file_path))
                    data_sources[indicator] = data
                    logger.info(f"âœ… {indicator}: {len(data)} rows, {len(data.columns)} columns")
                else:
                    logger.warning(f"âš ï¸ File not found: {file_path}")
                    data_sources[indicator] = pd.DataFrame()
                    
            except Exception as e:
                logger.error(f"âŒ Error loading {indicator}: {e}")
                data_sources[indicator] = pd.DataFrame()
        
        return data_sources
    
    def load_multiple_symbols(self, symbols: List[str], timeframes: List[str]) -> Dict[str, Dict[str, pd.DataFrame]]:
        """
        Load data for multiple symbols and timeframes.
        Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð»Ñ Ð¼Ð½Ð¾Ð¶ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ñ… ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð² Ð¸ Ñ‚Ð°Ð¹Ð¼Ñ„Ñ€ÐµÐ¹Ð¼Ð¾Ð².
        
        Args:
            symbols: List of trading symbols
            timeframes: List of timeframes
            
        Returns:
            Nested dictionary: {symbol: {timeframe: {indicator: dataframe}}}
        """
        logger.info(f"Loading data for {len(symbols)} symbols and {len(timeframes)} timeframes...")
        
        all_data = {}
        
        for symbol in symbols:
            all_data[symbol] = {}
            
            for timeframe in timeframes:
                logger.info(f"ðŸ“Š Loading {symbol} {timeframe}...")
                
                try:
                    symbol_data = self.load_symbol_data(symbol, timeframe)
                    all_data[symbol][timeframe] = symbol_data
                    
                    # Log summary
                    total_rows = sum(len(df) for df in symbol_data.values() if not df.empty)
                    indicators_loaded = sum(1 for df in symbol_data.values() if not df.empty)
                    
                    logger.info(f"âœ… {symbol} {timeframe}: {total_rows} total rows, {indicators_loaded} indicators")
                    
                except Exception as e:
                    logger.error(f"âŒ Failed to load {symbol} {timeframe}: {e}")
                    all_data[symbol][timeframe] = {}
        
        return all_data
    
    def combine_indicators(self, data_sources: Dict[str, pd.DataFrame]) -> pd.DataFrame:
        """
        Combine data from multiple indicators into a single dataframe.
        ÐžÐ±ÑŠÐµÐ´Ð¸Ð½Ð¸Ñ‚ÑŒ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð· Ð¼Ð½Ð¾Ð¶ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ñ… Ð¸Ð½Ð´Ð¸ÐºÐ°Ñ‚Ð¾Ñ€Ð¾Ð² Ð² Ð¾Ð´Ð¸Ð½ Ð´Ð°Ñ‚Ð°Ñ„Ñ€ÐµÐ¹Ð¼.
        
        Args:
            data_sources: Dictionary with indicator data
            
        Returns:
            Combined dataframe
        """
        logger.info("Combining indicator data...")
        
        if not data_sources or all(df.empty for df in data_sources.values()):
            logger.warning("No data to combine")
            return pd.DataFrame()
        
        # Start with CSVExport as base (has OHLCV data)
        if 'csv_export' in data_sources and not data_sources['csv_export'].empty:
            combined_df = data_sources['csv_export'].copy()
            logger.info(f"Base data from CSVExport: {len(combined_df)} rows")
        else:
            # Use first available indicator as base
            base_indicator = next((k for k, v in data_sources.items() if not v.empty), None)
            if base_indicator is None:
                logger.error("No data available to combine")
                return pd.DataFrame()
            
            combined_df = data_sources[base_indicator].copy()
            logger.info(f"Base data from {base_indicator}: {len(combined_df)} rows")
        
        # Add other indicators
        for indicator, df in data_sources.items():
            if indicator == 'csv_export' or df.empty:
                continue
                
            logger.info(f"Adding {indicator} data...")
            
            # Select only indicator-specific columns (exclude OHLCV duplicates)
            indicator_columns = [col for col in df.columns 
                               if col not in ['Close', 'High', 'Open', 'Low', 'Volume']]
            
            if indicator_columns:
                indicator_df = df[indicator_columns].copy()
                
                # Add prefix to avoid column name conflicts
                indicator_df.columns = [f"{indicator}_{col}" for col in indicator_df.columns]
                
                # Merge with combined dataframe
                combined_df = combined_df.join(indicator_df, how='outer')
                logger.info(f"âœ… Added {len(indicator_columns)} columns from {indicator}")
            else:
                logger.warning(f"No unique columns in {indicator}")
        
        logger.info(f"Combined data: {len(combined_df)} rows, {len(combined_df.columns)} columns")
        return combined_df
    
    def create_target_variable(self, data: pd.DataFrame, method: str = 'price_direction', problem_type: str = 'regression') -> pd.DataFrame:
        """
        Create target variable for machine learning.
        Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ Ñ†ÐµÐ»ÐµÐ²ÑƒÑŽ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½ÑƒÑŽ Ð´Ð»Ñ Ð¼Ð°ÑˆÐ¸Ð½Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ.
        
        Args:
            data: Input dataframe
            method: Method for creating target ('price_direction', 'price_change', 'volatility')
            problem_type: Type of problem ('regression', 'binary', 'multiclass')
            
        Returns:
            Dataframe with target variable added
        """
        logger.info(f"Creating target variable using method: {method}, problem_type: {problem_type}")
        
        result_df = data.copy()
        
        if problem_type == 'binary':
            # Binary classification: 1 if price goes up, 0 if down
            result_df['target'] = (result_df['Close'].diff() > 0).astype(int)
            logger.info("âœ… Binary classification target: 1=up, 0=down")
            
        elif problem_type == 'multiclass':
            # Multiclass classification: 0=down, 1=sideways, 2=up
            price_change = result_df['Close'].pct_change()
            result_df['target'] = pd.cut(price_change, 
                                       bins=[-np.inf, -0.01, 0.01, np.inf], 
                                       labels=[0, 1, 2]).astype(int)
            logger.info("âœ… Multiclass target: 0=down, 1=sideways, 2=up")
            
        elif problem_type == 'regression':
            if method == 'price_change':
                # Regression: actual price change percentage
                result_df['target'] = result_df['Close'].pct_change()
                logger.info("âœ… Regression target: price change percentage")
            elif method == 'volatility':
                # Regression: rolling volatility
                result_df['target'] = result_df['Close'].rolling(20).std()
                logger.info("âœ… Regression target: rolling volatility")
            else:
                # Default: price change percentage
                result_df['target'] = result_df['Close'].pct_change()
                logger.info("âœ… Regression target: price change percentage (default)")
        else:
            raise ValueError(f"Unknown problem type: {problem_type}")
        
        # Remove rows with NaN target
        initial_rows = len(result_df)
        result_df = result_df.dropna(subset=['target'])
        removed_rows = initial_rows - len(result_df)
        
        if removed_rows > 0:
            logger.info(f"Removed {removed_rows} rows with NaN target")
        
        logger.info(f"Target variable created: {len(result_df)} rows")
        return result_df
    
    def add_technical_indicators(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        Add common technical indicators to the data.
        Ð”Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ Ð¾Ð±Ñ‰Ð¸Ðµ Ñ‚ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð¸Ð½Ð´Ð¸ÐºÐ°Ñ‚Ð¾Ñ€Ñ‹ Ðº Ð´Ð°Ð½Ð½Ñ‹Ð¼.
        
        Args:
            data: Input dataframe
            
        Returns:
            Dataframe with technical indicators added
        """
        logger.info("Adding technical indicators...")
        
        result_df = data.copy()
        
        # Moving averages
        for period in [5, 10, 20, 50]:
            result_df[f'sma_{period}'] = result_df['Close'].rolling(period).mean()
            result_df[f'ema_{period}'] = result_df['Close'].ewm(span=period).mean()
        
        # Price-based indicators
        result_df['price_change'] = result_df['Close'].pct_change()
        result_df['high_low_ratio'] = result_df['High'] / result_df['Low']
        result_df['close_open_ratio'] = result_df['Close'] / result_df['Open']
        
        # Volatility indicators
        result_df['volatility_20'] = result_df['Close'].rolling(20).std()
        result_df['volatility_50'] = result_df['Close'].rolling(50).std()
        
        # Volume indicators
        if 'Volume' in result_df.columns:
            result_df['volume_sma_20'] = result_df['Volume'].rolling(20).mean()
            result_df['volume_ratio'] = result_df['Volume'] / result_df['volume_sma_20']
        
        # RSI
        result_df['rsi_14'] = self._calculate_rsi(result_df['Close'], 14)
        result_df['rsi_21'] = self._calculate_rsi(result_df['Close'], 21)
        
        # MACD
        macd_line, signal_line, histogram = self._calculate_macd(result_df['Close'])
        result_df['macd'] = macd_line
        result_df['macd_signal'] = signal_line
        result_df['macd_histogram'] = histogram
        
        logger.info(f"Added {len([col for col in result_df.columns if col not in data.columns])} technical indicators")
        return result_df
    
    def _calculate_rsi(self, prices: pd.Series, period: int = 14) -> pd.Series:
        """Calculate RSI indicator."""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi
    
    def _calculate_macd(self, prices: pd.Series, fast: int = 12, slow: int = 26, signal: int = 9) -> Tuple[pd.Series, pd.Series, pd.Series]:
        """Calculate MACD indicator."""
        ema_fast = prices.ewm(span=fast).mean()
        ema_slow = prices.ewm(span=slow).mean()
        macd_line = ema_fast - ema_slow
        signal_line = macd_line.ewm(span=signal).mean()
        histogram = macd_line - signal_line
        return macd_line, signal_line, histogram
    
    def get_data_summary(self, data: pd.DataFrame) -> Dict[str, Any]:
        """
        Get comprehensive summary of the data.
        ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÐºÐ¾Ð¼Ð¿Ð»ÐµÐºÑÐ½ÑƒÑŽ ÑÐ²Ð¾Ð´ÐºÑƒ Ð´Ð°Ð½Ð½Ñ‹Ñ….
        
        Args:
            data: Input dataframe
            
        Returns:
            Dictionary with data summary
        """
        if data.empty:
            return {
                'rows': 0,
                'columns': 0,
                'date_range': None,
                'missing_values': {},
                'data_types': {},
                'summary': 'No data available'
            }
        
        summary = {
            'rows': len(data),
            'columns': len(data.columns),
            'date_range': {
                'start': data.index.min() if hasattr(data.index, 'min') else None,
                'end': data.index.max() if hasattr(data.index, 'max') else None
            },
            'missing_values': data.isnull().sum().to_dict(),
            'data_types': data.dtypes.to_dict(),
            'memory_usage': data.memory_usage(deep=True).sum(),
            'numeric_columns': len(data.select_dtypes(include=[np.number]).columns),
            'categorical_columns': len(data.select_dtypes(include=['object', 'category']).columns)
        }
        
        # Add target variable info if exists
        if 'target' in data.columns:
            target_info = data['target'].describe().to_dict()
            target_info['unique_values'] = data['target'].nunique()
            summary['target_variable'] = target_info
        
        return summary
    
    def auto_scan_and_select(self, interactive: bool = True) -> Dict[str, Any]:
        """
        Automatically scan directory and select data interactively.
        ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸ ÑÐºÐ°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸ÑŽ Ð¸ Ð¸Ð½Ñ‚ÐµÑ€Ð°ÐºÑ‚Ð¸Ð²Ð½Ð¾ Ð²Ñ‹Ð±Ñ€Ð°Ñ‚ÑŒ Ð´Ð°Ð½Ð½Ñ‹Ðµ.
        
        Args:
            interactive: Whether to use interactive selection
            
        Returns:
            Dictionary with selection results
        """
        logger.info("ðŸ” Auto-scanning data directory...")
        
        # Scan directory
        scan_results = self.scanner.scan_directory()
        
        if not scan_results.get('scan_successful', False):
            logger.error(f"âŒ Scan failed: {scan_results.get('error')}")
            return {'success': False, 'error': scan_results.get('error')}
        
        if interactive:
            # Interactive selection
            logger.info("ðŸŽ¯ Starting interactive selection...")
            selection = self.selector.interactive_selection()
            
            if not selection.get('success', False):
                logger.error(f"âŒ Selection failed: {selection.get('error')}")
                return selection
            
            return selection
        else:
            # Auto-select first available combination
            logger.info("ðŸ¤– Auto-selecting first available combination...")
            
            if not self.scanner.available_data:
                return {'success': False, 'error': 'No data available'}
            
            # Get first indicator
            first_indicator = list(self.scanner.available_data.keys())[0]
            
            # Get first symbol for this indicator
            first_symbol = list(self.scanner.available_data[first_indicator].keys())[0]
            
            # Get all timeframes for this symbol
            timeframes = self.scanner.get_symbol_timeframes(first_indicator, first_symbol)
            
            selection = {
                'success': True,
                'indicator': first_indicator,
                'symbol': first_symbol,
                'timeframes': timeframes,
                'file_paths': {}
            }
            
            # Get file paths
            for timeframe in timeframes:
                file_path = self.scanner.get_file_path(first_indicator, first_symbol, timeframe)
                if file_path:
                    selection['file_paths'][timeframe] = file_path
            
            logger.info(f"âœ… Auto-selected: {first_indicator} {first_symbol} {timeframes}")
            return selection
    
    def load_selected_data(self, selection: Dict[str, Any]) -> pd.DataFrame:
        """
        Load data based on selection results.
        Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ð²Ñ‹Ð±Ð¾Ñ€Ð°.
        
        Args:
            selection: Selection results from auto_scan_and_select
            
        Returns:
            Combined dataframe with all selected data
        """
        if not selection.get('success', False):
            logger.error(f"âŒ Cannot load data: {selection.get('error')}")
            return pd.DataFrame()
        
        indicator = selection['indicator']
        symbol = selection['symbol']
        timeframes = selection['timeframes']
        file_paths = selection.get('file_paths', {})
        
        logger.info(f"ðŸ“Š Loading data for {indicator} {symbol} across {len(timeframes)} timeframes...")
        
        all_data = []
        
        for timeframe in timeframes:
            if timeframe in file_paths:
                file_path = file_paths[timeframe]
                
                try:
                    logger.info(f"ðŸ“ Loading {timeframe} from {file_path}")
                    data = self.data_loader.load_file(file_path)
                    
                    # Add metadata
                    data['indicator'] = indicator
                    data['symbol'] = symbol
                    data['timeframe'] = timeframe
                    
                    # Add timeframe weight
                    timeframe_weights = {'M1': 1, 'M5': 2, 'M15': 3, 'H1': 4, 'H4': 8, 'D1': 16, 'W1': 32, 'MN1': 64}
                    data['timeframe_weight'] = timeframe_weights.get(timeframe, 1)
                    
                    all_data.append(data)
                    logger.info(f"âœ… {timeframe}: {len(data)} rows, {len(data.columns)} columns")
                    
                except Exception as e:
                    logger.error(f"âŒ Failed to load {timeframe}: {e}")
                    continue
            else:
                logger.warning(f"âš ï¸ No file path for {timeframe}")
        
        if not all_data:
            logger.error("âŒ No data loaded successfully")
            return pd.DataFrame()
        
        # Combine all data
        logger.info("ðŸ”„ Combining all data...")
        combined_data = pd.concat(all_data, ignore_index=True)
        
        # Add technical indicators
        combined_data = self.add_technical_indicators(combined_data)
        
        logger.info(f"ðŸ“Š Final combined data: {len(combined_data)} rows, {len(combined_data.columns)} columns")
        
        return combined_data
    
    def load_multi_indicator_data(self, symbol: str, timeframes: List[str]) -> pd.DataFrame:
        """
        Load data from multiple indicators (CSVExport/SCHR, WAVE2, SHORT3) for a symbol.
        Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð· Ð¼Ð½Ð¾Ð¶ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ñ… Ð¸Ð½Ð´Ð¸ÐºÐ°Ñ‚Ð¾Ñ€Ð¾Ð² Ð´Ð»Ñ ÑÐ¸Ð¼Ð²Ð¾Ð»Ð°.
        
        Args:
            symbol: Trading symbol
            timeframes: List of timeframes
            
        Returns:
            Combined dataframe with all indicators
        """
        logger.info(f"ðŸ“Š Loading multi-indicator data for {symbol} across {len(timeframes)} timeframes...")
        
        all_combined_data = []
        
        for timeframe in timeframes:
            try:
                logger.info(f"ðŸ“Š Loading {symbol} {timeframe}...")
                
                # Load all indicators for this symbol/timeframe
                symbol_data = self.load_symbol_data(symbol, timeframe)
                
                # Combine indicators
                combined_symbol_data = self.combine_indicators(symbol_data)
                
                if not combined_symbol_data.empty:
                    # Add metadata
                    combined_symbol_data['symbol'] = symbol
                    combined_symbol_data['timeframe'] = timeframe
                    
                    # Add timeframe weight
                    timeframe_weights = {'M1': 1, 'M5': 2, 'M15': 3, 'H1': 4, 'H4': 8, 'D1': 16, 'W1': 32, 'MN1': 64}
                    combined_symbol_data['timeframe_weight'] = timeframe_weights.get(timeframe, 1)
                    
                    all_combined_data.append(combined_symbol_data)
                    logger.info(f"âœ… {symbol} {timeframe}: {len(combined_symbol_data)} rows")
                else:
                    logger.warning(f"âš ï¸ No data for {symbol} {timeframe}")
                    
            except Exception as e:
                logger.error(f"âŒ Failed to load {symbol} {timeframe}: {e}")
                continue
        
        if not all_combined_data:
            logger.error("âŒ No data loaded successfully")
            return pd.DataFrame()
        
        # Combine all data
        logger.info("ðŸ”„ Combining all multi-indicator data...")
        final_data = pd.concat(all_combined_data, ignore_index=True)
        
        # Add technical indicators
        final_data = self.add_technical_indicators(final_data)
        
        logger.info(f"ðŸ“Š Final multi-indicator data: {len(final_data)} rows, {len(final_data.columns)} columns")
        
        return final_data
    
    def auto_load_data(self, interactive: bool = True) -> pd.DataFrame:
        """
        Complete auto-loading process: scan, select, and load data.
        ÐŸÐ¾Ð»Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸: ÑÐºÐ°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ, Ð²Ñ‹Ð±Ð¾Ñ€ Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ….
        
        Args:
            interactive: Whether to use interactive selection
            
        Returns:
            Combined dataframe with all selected data
        """
        logger.info("ðŸš€ Starting auto-loading process...")
        
        # Step 1: Auto-scan and select
        selection = self.auto_scan_and_select(interactive=interactive)
        
        if not selection.get('success', False):
            logger.error(f"âŒ Auto-selection failed: {selection.get('error')}")
            return pd.DataFrame()
        
        # Step 2: Load selected data
        combined_data = self.load_selected_data(selection)
        
        if combined_data.empty:
            logger.error("âŒ No data loaded")
            return pd.DataFrame()
        
        logger.info("âœ… Auto-loading completed successfully!")
        return combined_data
>>>>>>> origin/master
