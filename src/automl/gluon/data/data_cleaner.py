#!/usr/bin/env python3
"""
Data Cleaner for AutoGluon Pipeline
clean –¥–∞–Ω–Ω—ã—Ö for –ø–∞–π–ø–ª–∞–π–Ω–∞ AutoGluon

This module handles data cleaning and preprocessing to ensure
high-quality data for AutoGluon training.
"""

import pandas as pd
import numpy as np
import logging
from typing import Tuple, Dict, Any

logger = logging.getLogger(__name__)

class DataCleaner:
 """
 Data cleaner for AutoGluon pipeline.
 clean –¥–∞–Ω–Ω—ã—Ö for –ø–∞–π–ø–ª–∞–π–Ω–∞ AutoGluon.
 """

 def __init__(self):
 """Initialize data cleaner."""
 self.logger = logger

 def clean_data(self, data: pd.DataFrame, target_column: str = None) -> Tuple[pd.DataFrame, Dict[str, Any]]:
 """
 Clean data by removing infinity, NaN values, and other issues.
 –û—á–∏—Å—Ç–∏—Ç—å –¥–∞–Ω–Ω—ã–µ, —É–¥–∞–ª–∏–≤ –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, NaN and –¥—Ä—É–≥–∏–µ –ø—Ä–æ–±–ª–µ–º—ã.

 Args:
 data: Input dataframe
 target_column: Target column name (optional)

 Returns:
 Tuple of (cleaned_data, cleaning_report)
 """
 self.logger.info("üßπ starting data cleaning...")

 original_shape = data.shape
 cleaning_report = {
 'original_shape': original_shape,
 'original_rows': original_shape[0],
 'original_cols': original_shape[1],
 'infinity_removed': 0,
 'nan_removed': 0,
 'duplicates_removed': 0,
 'outliers_removed': 0,
 'final_shape': None,
 'final_rows': 0,
 'final_cols': 0
 }

 # start with a copy
 cleaned_data = data.copy()

 # Step 1: Remove infinity values
 self.logger.info("üîç Step 1: Removing infinity values...")
 inf_mask = np.isinf(cleaned_data.select_dtypes(include=[np.number])).any(axis=1)
 inf_count = inf_mask.sum()
 if inf_count > 0:
 cleaned_data = cleaned_data[~inf_mask]
 cleaning_report['infinity_removed'] = inf_count
 self.logger.info(f"‚úÖ Removed {inf_count} rows with infinity values")

 # Step 2: Handle NaN values
 self.logger.info("üîç Step 2: Handling NaN values...")
 nan_count = cleaned_data.isnull().sum().sum()
 if nan_count > 0:
 # For numeric columns, fill with median
 numeric_cols = cleaned_data.select_dtypes(include=[np.number]).columns
 for col in numeric_cols:
 if cleaned_data[col].isnull().any():
 median_val = cleaned_data[col].median()
 cleaned_data[col].fillna(median_val, inplace=True)

 # For categorical columns, fill with mode
 categorical_cols = cleaned_data.select_dtypes(include=['object']).columns
 for col in categorical_cols:
 if cleaned_data[col].isnull().any():
 mode_val = cleaned_data[col].mode().iloc[0] if not cleaned_data[col].mode().empty else 'Unknown'
 cleaned_data[col].fillna(mode_val, inplace=True)

 cleaning_report['nan_removed'] = nan_count
 self.logger.info(f"‚úÖ Handled {nan_count} NaN values")

 # Step 3: Remove duplicates
 self.logger.info("üîç Step 3: Removing duplicates...")
 duplicate_count = cleaned_data.duplicated().sum()
 if duplicate_count > 0:
 cleaned_data = cleaned_data.drop_duplicates()
 cleaning_report['duplicates_removed'] = duplicate_count
 self.logger.info(f"‚úÖ Removed {duplicate_count} duplicate rows")

 # Step 4: Remove extreme outliers (optional)
 self.logger.info("üîç Step 4: Removing extreme outliers...")
 outlier_count = 0
 numeric_cols = cleaned_data.select_dtypes(include=[np.number]).columns

 for col in numeric_cols:
 if col == target_column:
 continue # Don't remove outliers from target column

 Q1 = cleaned_data[col].quantile(0.25)
 Q3 = cleaned_data[col].quantile(0.75)
 IQR = Q3 - Q1
 lower_bound = Q1 - 3 * IQR # More conservative than 1.5 * IQR
 upper_bound = Q3 + 3 * IQR

 outlier_mask = (cleaned_data[col] < lower_bound) | (cleaned_data[col] > upper_bound)
 if outlier_mask.any():
 outlier_count += outlier_mask.sum()
 cleaned_data = cleaned_data[~outlier_mask]

 if outlier_count > 0:
 cleaning_report['outliers_removed'] = outlier_count
 self.logger.info(f"‚úÖ Removed {outlier_count} extreme outliers")

 # Step 5: Ensure data types are appropriate
 self.logger.info("üîç Step 5: Ensuring appropriate data types...")
 for col in cleaned_data.columns:
 if cleaned_data[col].dtype == 'object':
 # Try to convert to numeric if possible
 try:
 cleaned_data[col] = pd.to_numeric(cleaned_data[col], errors='ignore')
 except:
 pass

 # Final report
 final_shape = cleaned_data.shape
 cleaning_report['final_shape'] = final_shape
 cleaning_report['final_rows'] = final_shape[0]
 cleaning_report['final_cols'] = final_shape[1]

 self.logger.info(f"‚úÖ Data cleaning completed!")
 self.logger.info(f"üìä Original: {original_shape[0]} rows, {original_shape[1]} cols")
 self.logger.info(f"üìä Final: {final_shape[0]} rows, {final_shape[1]} cols")
 self.logger.info(f"üìä Removed: {original_shape[0] - final_shape[0]} rows total")

 return cleaned_data, cleaning_report

 def validate_data(self, data: pd.DataFrame, target_column: str = None) -> Dict[str, Any]:
 """
 Validate data quality after cleaning.
 –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏.

 Args:
 data: Dataframe to validate
 target_column: Target column name (optional)

 Returns:
 Validation report
 """
 self.logger.info("üîç Validating data quality...")

 validation_report = {
 'has_infinity': False,
 'has_nan': False,
 'has_duplicates': False,
 'numeric_cols': 0,
 'categorical_cols': 0,
 'total_rows': len(data),
 'total_cols': len(data.columns),
 'memory_usage_mb': data.memory_usage(deep=True).sum() / 1024 / 1024
 }

 # Check for infinity
 if np.isinf(data.select_dtypes(include=[np.number])).any().any():
 validation_report['has_infinity'] = True
 self.logger.warning("‚ö†Ô∏è Data still contains infinity values")

 # Check for NaN
 if data.isnull().any().any():
 validation_report['has_nan'] = True
 self.logger.warning("‚ö†Ô∏è Data still contains NaN values")

 # Check for duplicates
 if data.duplicated().any():
 validation_report['has_duplicates'] = True
 self.logger.warning("‚ö†Ô∏è Data still contains duplicates")

 # Count column types
 validation_report['numeric_cols'] = len(data.select_dtypes(include=[np.number]).columns)
 validation_report['categorical_cols'] = len(data.select_dtypes(include=['object']).columns)

 # Check target column if provided
 if target_column and target_column in data.columns:
 target_info = {
 'target_unique_values': data[target_column].nunique(),
 'target_missing': data[target_column].isnull().sum(),
 'target_type': str(data[target_column].dtype)
 }
 validation_report['target_info'] = target_info

 self.logger.info(f"‚úÖ Validation completed: {validation_report['total_rows']} rows, {validation_report['total_cols']} cols")
 self.logger.info(f"üìä Memory usage: {validation_report['memory_usage_mb']:.2f} MB")

 return validation_report
