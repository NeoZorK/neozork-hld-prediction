#!/usr/bin/env python3
"""
Simple Pipeline Test - Quick validation without full training
ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ñ‚ĞµÑÑ‚ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ° - Ğ±Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
"""

import sys
import os
import logging
from pathlib import Path

# Add src to path
sys.path.insert(0, os.path.abspath('src'))

from automl.gluon.data.multi_indicator_loader import MultiIndicatorLoader
from automl.gluon.features.updated_feature_engineer import UpdatedCustomFeatureEngineer

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)


def test_data_loading():
    """Test data loading functionality."""
    print("ğŸ” Testing data loading...")
    
    loader = MultiIndicatorLoader()
    
    # Test loading single symbol/timeframe
    data_sources = loader.load_symbol_data('BTCUSD', 'D1')
    
    print(f"ğŸ“Š Data sources loaded: {list(data_sources.keys())}")
    
    for indicator, data in data_sources.items():
        if not data.empty:
            print(f"   {indicator}: {len(data)} rows, {len(data.columns)} columns")
            print(f"      Columns: {list(data.columns)}")
        else:
            print(f"   {indicator}: No data")
    
    return data_sources


def test_data_combination(data_sources):
    """Test data combination."""
    print("\nğŸ”„ Testing data combination...")
    
    loader = MultiIndicatorLoader()
    combined_data = loader.combine_indicators(data_sources)
    
    print(f"ğŸ“Š Combined data: {len(combined_data)} rows, {len(combined_data.columns)} columns")
    print(f"   Columns: {list(combined_data.columns)}")
    
    return combined_data


def test_feature_engineering(combined_data):
    """Test feature engineering."""
    print("\nğŸ”§ Testing feature engineering...")
    
    # Create target variable
    loader = MultiIndicatorLoader()
    data_with_target = loader.create_target_variable(combined_data, method='price_direction')
    
    print(f"ğŸ“Š Data with target: {len(data_with_target)} rows")
    print(f"   Target distribution: {data_with_target['target'].value_counts().to_dict()}")
    
    # Create custom features
    feature_engineer = UpdatedCustomFeatureEngineer()
    
    # Test SCHR features
    if any(col in data_with_target.columns for col in ['pressure', 'pressure_vector', 'predicted_low', 'predicted_high']):
        print("   Creating SCHR features...")
        data_with_schr = feature_engineer.create_schr_features(data_with_target)
        schr_features = [col for col in data_with_schr.columns if 'probability' in col]
        print(f"   SCHR features created: {len(schr_features)}")
        print(f"   SCHR feature names: {schr_features}")
    else:
        print("   âš ï¸ No SCHR columns found")
        data_with_schr = data_with_target
    
    # Test WAVE2 features
    if any(col in data_with_schr.columns for col in ['wave', 'fast_line', 'ma_line', 'direction', 'signal']):
        print("   Creating WAVE2 features...")
        data_with_wave = feature_engineer.create_wave2_features(data_with_schr)
        wave_features = [col for col in data_with_wave.columns if 'wave' in col and 'probability' in col]
        print(f"   WAVE2 features created: {len(wave_features)}")
        print(f"   WAVE2 feature names: {wave_features}")
    else:
        print("   âš ï¸ No WAVE2 columns found")
        data_with_wave = data_with_schr
    
    # Test SHORT3 features
    if any(col in data_with_wave.columns for col in ['short_trend', 'r_trend', 'global', 'direction', 'r_direction', 'signal', 'r_signal', 'g_direction', 'g_signal']):
        print("   Creating SHORT3 features...")
        data_with_short3 = feature_engineer.create_short3_features(data_with_wave)
        short3_features = [col for col in data_with_short3.columns if 'short3' in col and 'probability' in col]
        print(f"   SHORT3 features created: {len(short3_features)}")
        print(f"   SHORT3 feature names: {short3_features}")
    else:
        print("   âš ï¸ No SHORT3 columns found")
        data_with_short3 = data_with_wave
    
    print(f"ğŸ“Š Final data: {len(data_with_short3)} rows, {len(data_with_short3.columns)} columns")
    
    return data_with_short3


def test_technical_indicators(data):
    """Test technical indicators."""
    print("\nğŸ“ˆ Testing technical indicators...")
    
    loader = MultiIndicatorLoader()
    data_with_indicators = loader.add_technical_indicators(data)
    
    new_indicators = [col for col in data_with_indicators.columns if col not in data.columns]
    print(f"   Technical indicators added: {len(new_indicators)}")
    print(f"   Indicator names: {new_indicators}")
    
    return data_with_indicators


def main():
    """Run simple pipeline test."""
    print("ğŸš€ Simple Pipeline Test")
    print("=" * 50)
    
    try:
        # Test 1: Data loading
        data_sources = test_data_loading()
        
        if not any(not df.empty for df in data_sources.values()):
            print("âŒ No data loaded - test failed")
            return False
        
        # Test 2: Data combination
        combined_data = test_data_combination(data_sources)
        
        if combined_data.empty:
            print("âŒ Data combination failed")
            return False
        
        # Test 3: Feature engineering
        data_with_features = test_feature_engineering(combined_data)
        
        if data_with_features.empty:
            print("âŒ Feature engineering failed")
            return False
        
        # Test 4: Technical indicators
        final_data = test_technical_indicators(data_with_features)
        
        if final_data.empty:
            print("âŒ Technical indicators failed")
            return False
        
        # Summary
        print("\n" + "=" * 50)
        print("âœ… SIMPLE PIPELINE TEST RESULTS")
        print("=" * 50)
        
        print(f"ğŸ“Š Final Dataset:")
        print(f"   Rows: {len(final_data):,}")
        print(f"   Columns: {len(final_data.columns)}")
        print(f"   Target distribution: {final_data['target'].value_counts().to_dict()}")
        
        # Count features by type
        schr_features = [col for col in final_data.columns if 'probability' in col and any(x in col for x in ['trend', 'yellow', 'blue', 'pv'])]
        wave_features = [col for col in final_data.columns if 'wave' in col and 'probability' in col]
        short3_features = [col for col in final_data.columns if 'short3' in col and 'probability' in col]
        technical_features = [col for col in final_data.columns if any(x in col for x in ['sma', 'ema', 'rsi', 'macd', 'volatility'])]
        
        print(f"\nğŸ”§ Features Created:")
        print(f"   SCHR features: {len(schr_features)}")
        print(f"   WAVE2 features: {len(wave_features)}")
        print(f"   SHORT3 features: {len(short3_features)}")
        print(f"   Technical indicators: {len(technical_features)}")
        print(f"   Total custom features: {len(schr_features) + len(wave_features) + len(short3_features)}")
        
        print(f"\nğŸ¯ Data Quality:")
        print(f"   Missing values: {final_data.isnull().sum().sum()}")
        print(f"   Data types: {final_data.dtypes.value_counts().to_dict()}")
        
        print(f"\nâœ… All tests passed! Pipeline is working correctly.")
        print(f"   Ready for model training with {len(final_data)} samples and {len(final_data.columns)} features.")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ Test failed: {e}")
        logger.error(f"Test failed: {e}")
        return False


if __name__ == "__main__":
    # Create logs directory if it doesn't exist
    os.makedirs('logs', exist_ok=True)
    
    # Run test
    success = main()
    
    if success:
        print("\nğŸ‰ Simple pipeline test completed successfully!")
    else:
        print("\nğŸ’¥ Simple pipeline test failed!")
        sys.exit(1)
