#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Real User Scenario Test

This test simulates the exact user scenario that was reported:
Load Data -> "8 eurusd" -> EDA Tests -> Comprehensive Data Quality Check
"""

import os
import pandas as pd
import pytest
from pathlib import Path
import sys

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.interactive.data_manager import DataManager
from src.interactive import InteractiveSystem


class TestRealUserScenario:
    """Test the real user scenario that was reported."""
    
    def setup_method(self):
        """Setup method for tests."""
        self.data_manager = DataManager()
        self.system = InteractiveSystem()
        
    def test_real_user_scenario(self):
        """Test the exact user scenario: Load Data -> 8 eurusd -> EDA Tests -> Comprehensive Data Quality Check."""
        print("ğŸ§ª Testing Real User Scenario...")
        print("=" * 60)
        print("Scenario: Load Data -> '8 eurusd' -> EDA Tests -> Comprehensive Data Quality Check")
        print("=" * 60)
        
        # Step 1: Simulate "8 eurusd" command
        print("\n1ï¸âƒ£  Step 1: Loading data with '8 eurusd' command...")
        
        # Simulate the folder selection logic
        data_folder = Path("data")
        subfolders = [data_folder]
        
        # Add subfolders (simulating the folder discovery logic)
        for item in data_folder.iterdir():
            if item.is_dir():
                if 'cache' not in item.name.lower() and item.name != 'mql5_feed':
                    subfolders.append(item)
                    for subitem in item.iterdir():
                        if subitem.is_dir() and 'cache' not in subitem.name.lower():
                            subfolders.append(subitem)
        
        # Add csv_converted folder specifically
        csv_converted_folder = data_folder / "cache" / "csv_converted"
        if csv_converted_folder.exists() and csv_converted_folder.is_dir():
            subfolders.append(csv_converted_folder)
        
        # Simulate user input "8 eurusd" (folder 8 with eurusd mask)
        folder_idx = 8 - 1  # Convert to 0-based index
        mask = "eurusd"
        
        if 0 <= folder_idx < len(subfolders):
            folder_path = subfolders[folder_idx]
            print(f"Selected folder: {folder_path}")
            print(f"Mask: {mask}")
        else:
            pytest.fail(f"Invalid folder index: {folder_idx + 1}")
        
        # Find files matching the mask
        data_files = []
        for ext in ['.csv', '.parquet', '.xlsx', '.xls']:
            if mask:
                pattern = f"*{mask}*{ext}"
                files = list(folder_path.glob(pattern))
                files = [f for f in files if not f.name.startswith('tmp')]
                data_files.extend(files)
                
                pattern = f"*{mask.upper()}*{ext}"
                files = list(folder_path.glob(pattern))
                files = [f for f in files if not f.name.startswith('tmp')]
                data_files.extend(files)
        
        # Remove duplicates
        data_files = list(set(data_files))
        
        print(f"Found {len(data_files)} files matching '{mask}' in {folder_path}:")
        for file in data_files:
            print(f"  â€¢ {file.name}")
        
        if not data_files:
            pytest.fail(f"No files found matching mask '{mask}' in {folder_path}")
        
        # Step 2: Load files using DataManager
        print(f"\n2ï¸âƒ£  Step 2: Loading files using DataManager...")
        
        all_data = []
        for i, file in enumerate(data_files):
            try:
                print(f"Loading file {i+1}/{len(data_files)}: {file.name}")
                df = self.data_manager.load_data_from_file(str(file))
                df['source_file'] = file.name
                all_data.append(df)
                print(f"  âœ… Loaded: {file.name} ({df.shape[0]:,} rows)")
            except Exception as e:
                print(f"  âŒ Error loading {file.name}: {e}")
                continue
        
        if not all_data:
            pytest.fail("No files could be loaded")
        
        # Step 3: Simulate concatenation using DataManager logic
        print(f"\n3ï¸âƒ£  Step 3: Concatenating data using DataManager logic...")
        
        try:
            # Use the same logic as DataManager
            has_datetime_index = any(isinstance(df.index, pd.DatetimeIndex) for df in all_data)
            
            if has_datetime_index:
                print("ğŸ“… Detected DatetimeIndex in loaded DataFrames, preserving during concatenation...")
                
                # Convert DatetimeIndex to 'Timestamp' column for consistent concatenation
                processed_data = []
                for df in all_data:
                    df_copy = df.copy()
                    if isinstance(df_copy.index, pd.DatetimeIndex):
                        # Reset index to make datetime a column
                        df_copy = df_copy.reset_index()
                        # Rename the index column if it's unnamed
                        if df_copy.columns[0] == 'index':
                            df_copy = df_copy.rename(columns={'index': 'Timestamp'})
                    processed_data.append(df_copy)
                
                # Combine DataFrames with consistent column structure
                self.system.current_data = pd.concat(processed_data, ignore_index=True)
                
                print("âœ… Successfully preserved datetime information during concatenation")
            else:
                # Check for mixed structures (some files have Timestamp column, others don't)
                has_timestamp_column = any('Timestamp' in df.columns for df in all_data)
                missing_timestamp_column = any('Timestamp' not in df.columns for df in all_data)
                
                if has_timestamp_column and missing_timestamp_column:
                    print("âš ï¸  Detected mixed file structures (some with Timestamp column, some without)")
                    print("ğŸ“… Normalizing file structures for consistent concatenation...")
                    
                    # Process all DataFrames to ensure consistent structure
                    processed_data = []
                    for i, df in enumerate(all_data):
                        df_copy = df.copy()
                        
                        if 'Timestamp' not in df_copy.columns:
                            # Create a dummy Timestamp column for files without it
                            # This will be filled with NaT (missing values)
                            df_copy['Timestamp'] = pd.NaT
                            print(f"   Added Timestamp column to file {i+1} (will be filled with missing values)")
                        
                        processed_data.append(df_copy)
                    
                    # Combine DataFrames with consistent column structure
                    self.system.current_data = pd.concat(processed_data, ignore_index=True)
                    
                    print("âœ… Successfully normalized file structures for concatenation")
                else:
                    # All files have consistent structure, use standard concatenation
                    self.system.current_data = pd.concat(all_data, ignore_index=True)
            
            print(f"âœ… Combined data loaded successfully!")
            print(f"   Total shape: {self.system.current_data.shape[0]:,} rows Ã— {self.system.current_data.shape[1]} columns")
            print(f"   Files loaded: {len(data_files)}")
            
        except Exception as e:
            pytest.fail(f"Error combining data: {e}")
        
        # Step 4: Simulate Comprehensive Data Quality Check
        print(f"\n4ï¸âƒ£  Step 4: Running Comprehensive Data Quality Check...")
        
        # Check for missing values in Timestamp
        missing_count = 0
        missing_percent = 0
        
        if 'Timestamp' in self.system.current_data.columns:
            missing_count = self.system.current_data['Timestamp'].isna().sum()
            missing_percent = 100 * missing_count / len(self.system.current_data)
        elif self.system.current_data.index.name == 'Timestamp':
            missing_count = self.system.current_data.index.isna().sum()
            missing_percent = 100 * missing_count / len(self.system.current_data)
        
        print(f"ğŸ“Š Data Quality Check Results:")
        print(f"   Missing timestamps: {missing_count:,} ({missing_percent:.2f}%)")
        
        # Show breakdown by source file
        if 'source_file' in self.system.current_data.columns:
            print(f"\nğŸ“‹ Breakdown by source file:")
            for source_file in self.system.current_data['source_file'].unique():
                file_data = self.system.current_data[self.system.current_data['source_file'] == source_file]
                file_missing = 0
                if 'Timestamp' in file_data.columns:
                    file_missing = file_data['Timestamp'].isna().sum()
                file_total = len(file_data)
                file_percent = 100 * file_missing / file_total
                print(f"   {source_file}: {file_missing:,}/{file_total:,} missing ({file_percent:.2f}%)")
        
        # Step 5: Evaluate the results
        print(f"\n5ï¸âƒ£  Step 5: Evaluating results...")
        
        if missing_percent > 50:
            print(f"âš ï¸  High missing timestamps detected ({missing_percent:.2f}%)")
            print(f"   This is expected for mixed file structures")
            print(f"   Files without original Timestamp columns will have missing values")
            
            # Check if this is the expected behavior
            if 'source_file' in self.system.current_data.columns:
                files_with_missing = []
                files_without_missing = []
                
                for source_file in self.system.current_data['source_file'].unique():
                    file_data = self.system.current_data[self.system.current_data['source_file'] == source_file]
                    file_missing = 0
                    if 'Timestamp' in file_data.columns:
                        file_missing = file_data['Timestamp'].isna().sum()
                    file_total = len(file_data)
                    file_percent = 100 * file_missing / file_total
                    
                    if file_percent > 50:
                        files_with_missing.append(source_file)
                    else:
                        files_without_missing.append(source_file)
                
                print(f"\nğŸ“Š File Analysis:")
                print(f"   Files with missing timestamps: {len(files_with_missing)}")
                for file in files_with_missing:
                    print(f"     â€¢ {file}")
                print(f"   Files without missing timestamps: {len(files_without_missing)}")
                for file in files_without_missing:
                    print(f"     â€¢ {file}")
                
                print(f"\nâœ… RESULT: This is the expected behavior!")
                print(f"   The system correctly identifies files with and without timestamp data")
                print(f"   Users can now understand which files need timestamp information")
        else:
            print(f"âœ… SUCCESS: Low missing timestamps ({missing_percent:.2f}%)")
            print(f"   All files have proper timestamp data")


if __name__ == "__main__":
    # Run tests
    test_instance = TestRealUserScenario()
    test_instance.setup_method()
    
    print("ğŸ§ª Testing Real User Scenario...")
    print("=" * 60)
    
    # Test real user scenario
    print("\n1ï¸âƒ£  Testing real user scenario...")
    test_instance.test_real_user_scenario()
    print("âœ… Real user scenario test completed")
    
    print("\nğŸ‰ All tests completed!")
