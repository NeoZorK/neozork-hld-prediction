# –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ AutoML Gluon

**–ê–≤—Ç–æ—Ä:** Shcherbyna Rostyslav  
**–î–∞—Ç–∞:** 2024  

## –ü–æ—á–µ–º—É –ª—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã

**–ü–æ—á–µ–º—É 95% ML-–ø—Ä–æ–µ–∫—Ç–æ–≤ —Ç–µ—Ä–ø—è—Ç –Ω–µ—É–¥–∞—á—É –∏–∑-–∑–∞ –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –ª—É—á—à–∏—Ö –ø—Ä–∞–∫—Ç–∏–∫?** –ü–æ—Ç–æ–º—É —á—Ç–æ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ - —ç—Ç–æ –Ω–µ –ø—Ä–æ—Å—Ç–æ "–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å", –∞ –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞, —Ç—Ä–µ–±—É—é—â–∞—è —Å–æ–±–ª—é–¥–µ–Ω–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –ø—Ä–∞–≤–∏–ª –∏ –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤.

### –ö–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∏—á–µ—Å–∫–∏–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è –ø–ª–æ—Ö–∏—Ö –ø—Ä–∞–∫—Ç–∏–∫
- **Amazon AI-—Ä–µ–∫—Ä—É—Ç–∏–Ω–≥**: –î–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ü–∏—è –∏–∑-–∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –≤ –¥–∞–Ω–Ω—ã—Ö
- **Microsoft Tay**: –†–∞—Å–∏—Å—Ç—Å–∫–∏–µ —Ç–≤–∏—Ç—ã –∏–∑-–∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –º–æ–¥–µ—Ä–∞—Ü–∏–∏
- **Uber —Å–∞–º–æ—É–ø—Ä–∞–≤–ª—è–µ–º—ã–µ –∞–≤—Ç–æ**: –°–º–µ—Ä—Ç—å –ø–µ—à–µ—Ö–æ–¥–∞ –∏–∑-–∑–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
- **Facebook –∞–ª–≥–æ—Ä–∏—Ç–º**: –ü–æ–ª—è—Ä–∏–∑–∞—Ü–∏—è –æ–±—â–µ—Å—Ç–≤–∞ –∏–∑-–∑–∞ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

### –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ª—É—á—à–∏–º –ø—Ä–∞–∫—Ç–∏–∫–∞–º
- **–ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å**: –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ –≤ –ª—é–±—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö
- **–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å**: –õ–µ–≥–∫–æ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –∫ —Ä–æ—Å—Ç—É –Ω–∞–≥—Ä—É–∑–∫–∏
- **–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ—Å—Ç—å**: –ö–æ–º–∞–Ω–¥–∞ –º–æ–∂–µ—Ç –ª–µ–≥–∫–æ —Ä–∞–∑–≤–∏–≤–∞—Ç—å —Å–∏—Å—Ç–µ–º—É
- **–≠—Ç–∏—á–Ω–æ—Å—Ç—å**: –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ

## –í–≤–µ–¥–µ–Ω–∏–µ –≤ –ª—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏

<img src="images/optimized/performance_comparison.png" alt="–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏" style="max-width: 100%; height: auto; display: block; margin: 20px auto;">
*–†–∏—Å—É–Ω–æ–∫ 1: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π*

<img src="images/optimized/robustness_analysis.png" alt="–ê–Ω–∞–ª–∏–∑ —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç–∏" style="max-width: 100%; height: auto; display: block; margin: 20px auto;">
*–†–∏—Å—É–Ω–æ–∫ 2: –ê–Ω–∞–ª–∏–∑ —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç–∏ - —Ä–æ–±–∞—Å—Ç–Ω—ã–µ vs –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã, —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏*

**–ü–æ—á–µ–º—É –ª—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ - —ç—Ç–æ –Ω–µ –ø—Ä–æ—Å—Ç–æ "—Å–¥–µ–ª–∞—Ç—å —Ö–æ—Ä–æ—à–æ"?** –≠—Ç–æ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ—à–µ–Ω–∏—é —Ç–∏–ø–∏—á–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –æ–ø—ã—Ç–µ —Ç—ã—Å—è—á –ø—Ä–æ–µ–∫—Ç–æ–≤. –≠—Ç–æ –∫–∞–∫ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –ø—Ä–æ—Ç–æ–∫–æ–ª—ã - –æ–Ω–∏ —Å–ø–∞—Å–∞—é—Ç –∂–∏–∑–Ω–∏.

**–ü–æ—á–µ–º—É 80% ML-–ø—Ä–æ–µ–∫—Ç–æ–≤ –ø–æ–≤—Ç–æ—Ä—è—é—Ç –æ–¥–Ω–∏ –∏ —Ç–µ –∂–µ –æ—à–∏–±–∫–∏?** –ü–æ—Ç–æ–º—É —á—Ç–æ –∫–æ–º–∞–Ω–¥—ã –Ω–µ –∑–Ω–∞—é—Ç –æ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π:
- **–ü—Ä–æ–±–ª–µ–º—ã —Å –¥–∞–Ω–Ω—ã–º–∏**: –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞, —É—Ç–µ—á–∫–∏, —Å–º–µ—â–µ–Ω–∏—è
- **–ü—Ä–æ–±–ª–µ–º—ã —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π**: –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ, –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ
- **–ü—Ä–æ–±–ª–µ–º—ã —Å –ø—Ä–æ–¥–∞–∫—à–µ–Ω–æ–º**: –ù–µ–≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ —Ä–µ–∞–ª—å–Ω—ã–º —É—Å–ª–æ–≤–∏—è–º
- **–ü—Ä–æ–±–ª–µ–º—ã —Å —ç—Ç–∏–∫–æ–π**: –î–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ü–∏—è, –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç—å, –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å

–õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ - —ç—Ç–æ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π –æ–ø—ã—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è AutoML Gluon, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–æ–∂–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å —Ç–∏–ø–∏—á–Ω—ã—Ö –æ—à–∏–±–æ–∫ –∏ –¥–æ—Å—Ç–∏—á—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏. –í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –≤—Å–µ –∞—Å–ø–µ–∫—Ç—ã –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞.

## –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö

<img src="images/optimized/advanced_topics_overview.png" alt="–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö" style="max-width: 100%; height: auto; display: block; margin: 20px auto;">
*–†–∏—Å—É–Ω–æ–∫ 3: –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è ML*

**–ü–æ—á–µ–º—É –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö?** –ü–æ—Ç–æ–º—É —á—Ç–æ –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –Ω–∞–ø—Ä—è–º—É—é –≤–ª–∏—è–µ—Ç –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏:

- **–û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö**: –£–¥–∞–ª–µ–Ω–∏–µ —à—É–º–∞, –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫
- **–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤**: –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π
- **–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è**: –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∫ –µ–¥–∏–Ω–æ–º—É –º–∞—Å—à—Ç–∞–±—É
- **Feature Engineering**: –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
- **–í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö**: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
- **–î–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ**: –§–∏–∫—Å–∞—Ü–∏—è –≤—Å–µ—Ö –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π

### 1. –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö

**–ü–æ—á–µ–º—É "–º—É—Å–æ—Ä –Ω–∞ –≤—Ö–æ–¥–µ = –º—É—Å–æ—Ä –Ω–∞ –≤—ã—Ö–æ–¥–µ" –æ—Å–æ–±–µ–Ω–Ω–æ –∞–∫—Ç—É–∞–ª—å–Ω–æ –¥–ª—è ML?** –ü–æ—Ç–æ–º—É —á—Ç–æ –º–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è –Ω–∞ –¥–∞–Ω–Ω—ã—Ö, –∏ –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –ø–ª–æ—Ö–∏–µ, –º–æ–¥–µ–ª—å –±—É–¥–µ—Ç –¥–µ–ª–∞—Ç—å –ø–ª–æ—Ö–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è. –≠—Ç–æ –∫–∞–∫ –æ–±—É—á–µ–Ω–∏–µ –≤—Ä–∞—á–∞ –Ω–∞ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –¥–∏–∞–≥–Ω–æ–∑–∞—Ö.

**–ü–æ—á–µ–º—É 60% –≤—Ä–µ–º–µ–Ω–∏ ML-–ø—Ä–æ–µ–∫—Ç–∞ —Ç—Ä–∞—Ç–∏—Ç—Å—è –Ω–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –¥–∞–Ω–Ω—ã—Ö?** –ü–æ—Ç–æ–º—É —á—Ç–æ —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤—Å–µ–≥–¥–∞ "–≥—Ä—è–∑–Ω—ã–µ":
- **–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è**: 30-50% –¥–∞–Ω–Ω—ã—Ö –º–æ–≥—É—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º–∏
- **–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è**: –û–ø–µ—á–∞—Ç–∫–∏, –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
- **–î—É–±–ª–∏–∫–∞—Ç—ã**: –û–¥–∏–Ω–∞–∫–æ–≤—ã–µ –∑–∞–ø–∏—Å–∏ –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö
- **–í—ã–±—Ä–æ—Å—ã**: –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–∫–∞–∂–∞—é—Ç –º–æ–¥–µ–ª—å

**–¢–∏–ø—ã –ø—Ä–æ–±–ª–µ–º —Å –¥–∞–Ω–Ω—ã–º–∏:**
- **–°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã**: –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö, —Ñ–æ—Ä–º–∞—Ç—ã
- **–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã**: –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏
- **–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã**: –°–º–µ—â–µ–Ω–∏—è, –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏, –≤—ã–±—Ä–æ—Å—ã
- **–≠—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã**: –î–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ü–∏—è, –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç—å

```python
import pandas as pd
import numpy as np
from autogluon.tabular import TabularPredictor
import matplotlib.pyplot as plt
import seaborn as sns

def data_quality_check(data: pd.DataFrame) -> Dict[str, Any]:
    """–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö - –ø–µ—Ä–≤—ã–π —à–∞–≥ –∫ —É—Å–ø–µ—à–Ω–æ–º—É ML"""
    
    quality_report = {
        'shape': data.shape,                    # –†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞
        'missing_values': data.isnull().sum().to_dict(),  # –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        'data_types': data.dtypes.to_dict(),   # –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
        'duplicates': data.duplicated().sum(),  # –î—É–±–ª–∏–∫–∞—Ç—ã
        'outliers': {},                         # –í—ã–±—Ä–æ—Å—ã
        'correlations': {}                      # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
    }
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    missing_percent = (data.isnull().sum() / len(data)) * 100
    quality_report['missing_percent'] = missing_percent.to_dict()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤ –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
    numeric_columns = data.select_dtypes(include=[np.number]).columns
    for col in numeric_columns:
        Q1 = data[col].quantile(0.25)
        Q3 = data[col].quantile(0.75)
        IQR = Q3 - Q1
        outliers = data[(data[col] < Q1 - 1.5 * IQR) | (data[col] > Q3 + 1.5 * IQR)]
        quality_report['outliers'][col] = len(outliers)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
    if len(numeric_columns) > 1:
        correlation_matrix = data[numeric_columns].corr()
        quality_report['correlations'] = correlation_matrix.to_dict()
    
    return quality_report

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
quality_report = data_quality_check(train_data)
print("Data Quality Report:")
for key, value in quality_report.items():
    print(f"{key}: {value}")
```

### 2. –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π

```python
def handle_missing_values(data: pd.DataFrame, strategy: str = 'auto') -> pd.DataFrame:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π"""
    
    if strategy == 'auto':
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è
        for col in data.columns:
            if data[col].dtype == 'object':
                # –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö - –º–æ–¥–∞
                data[col].fillna(data[col].mode()[0] if not data[col].mode().empty else 'Unknown', inplace=True)
            else:
                # –î–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö - –º–µ–¥–∏–∞–Ω–∞
                data[col].fillna(data[col].median(), inplace=True)
    
    elif strategy == 'drop':
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
        data = data.dropna()
    
    elif strategy == 'interpolate':
        # –ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
        data = data.interpolate(method='linear')
    
    return data

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
train_data_clean = handle_missing_values(train_data, strategy='auto')
```

### 3. –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤

```python
def handle_outliers(data: pd.DataFrame, method: str = 'iqr') -> pd.DataFrame:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤"""
    
    numeric_columns = data.select_dtypes(include=[np.number]).columns
    
    if method == 'iqr':
        # –ú–µ—Ç–æ–¥ –º–µ–∂–∫–≤–∞—Ä—Ç–∏–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–∞—Ö–∞
        for col in numeric_columns:
            Q1 = data[col].quantile(0.25)
            Q3 = data[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            # –ó–∞–º–µ–Ω–∞ –≤—ã–±—Ä–æ—Å–æ–≤ –Ω–∞ –≥—Ä–∞–Ω–∏—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            data[col] = np.where(data[col] < lower_bound, lower_bound, data[col])
            data[col] = np.where(data[col] > upper_bound, upper_bound, data[col])
    
    elif method == 'zscore':
        # –ú–µ—Ç–æ–¥ Z-—Å–∫–æ—Ä
        for col in numeric_columns:
            z_scores = np.abs((data[col] - data[col].mean()) / data[col].std())
            data = data[z_scores < 3]  # –£–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤
    
    elif method == 'winsorize':
        # –í–∏–Ω–∑–æ—Ä–∏–∑–∞—Ü–∏—è
        for col in numeric_columns:
            lower_percentile = data[col].quantile(0.05)
            upper_percentile = data[col].quantile(0.95)
            data[col] = np.where(data[col] < lower_percentile, lower_percentile, data[col])
            data[col] = np.where(data[col] > upper_percentile, upper_percentile, data[col])
    
    return data

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
train_data_no_outliers = handle_outliers(train_data, method='iqr')
```

## –í—ã–±–æ—Ä –º–µ—Ç—Ä–∏–∫

### 1. –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏

```python
def select_classification_metrics(problem_type: str, data_balance: str = 'balanced') -> List[str]:
    """–í—ã–±–æ—Ä –º–µ—Ç—Ä–∏–∫ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    
    if problem_type == 'binary':
        if data_balance == 'balanced':
            return ['accuracy', 'f1', 'roc_auc', 'precision', 'recall']
        elif data_balance == 'imbalanced':
            return ['f1', 'roc_auc', 'precision', 'recall', 'balanced_accuracy']
        else:
            return ['accuracy', 'f1', 'roc_auc']
    
    elif problem_type == 'multiclass':
        if data_balance == 'balanced':
            return ['accuracy', 'f1_macro', 'f1_micro', 'precision_macro', 'recall_macro']
        elif data_balance == 'imbalanced':
            return ['f1_macro', 'f1_micro', 'balanced_accuracy', 'precision_macro', 'recall_macro']
        else:
            return ['accuracy', 'f1_macro', 'f1_micro']
    
    else:
        return ['accuracy', 'f1', 'roc_auc']

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
metrics = select_classification_metrics('binary', 'imbalanced')
predictor = TabularPredictor(
    label='target',
    problem_type='binary',
    eval_metric=metrics[0]  # –û—Å–Ω–æ–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞
)
```

### 2. –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏

```python
def select_regression_metrics(problem_type: str, target_distribution: str = 'normal') -> List[str]:
    """–í—ã–±–æ—Ä –º–µ—Ç—Ä–∏–∫ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏"""
    
    if target_distribution == 'normal':
        return ['rmse', 'mae', 'r2']
    elif target_distribution == 'skewed':
        return ['mae', 'mape', 'smape']
    elif target_distribution == 'outliers':
        return ['mae', 'huber_loss']
    else:
        return ['rmse', 'mae']

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
metrics = select_regression_metrics('regression', 'normal')
predictor = TabularPredictor(
    label='target',
    problem_type='regression',
    eval_metric=metrics[0]
)
```

## –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

### 1. –°—Ç—Ä–∞—Ç–µ–≥–∏—è –ø–æ–∏—Å–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

```python
def create_hyperparameter_strategy(data_size: int, problem_type: str) -> Dict[str, Any]:
    """–°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ–∏—Å–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
    
    if data_size < 1000:
        # –ú–∞–ª–µ–Ω—å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç - –ø—Ä–æ—Å—Ç—ã–µ –º–æ–¥–µ–ª–∏
        return {
            'GBM': [{'num_boost_round': 100, 'learning_rate': 0.1}],
            'RF': [{'n_estimators': 100, 'max_depth': 10}],
            'XGB': [{'n_estimators': 100, 'max_depth': 6}]
        }
    
    elif data_size < 10000:
        # –°—Ä–µ–¥–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç - —É–º–µ—Ä–µ–Ω–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å
        return {
            'GBM': [
                {'num_boost_round': 200, 'learning_rate': 0.1},
                {'num_boost_round': 300, 'learning_rate': 0.05}
            ],
            'RF': [
                {'n_estimators': 200, 'max_depth': 15},
                {'n_estimators': 300, 'max_depth': 20}
            ],
            'XGB': [
                {'n_estimators': 200, 'max_depth': 8},
                {'n_estimators': 300, 'max_depth': 10}
            ]
        }
    
    else:
        # –ë–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç - —Å–ª–æ–∂–Ω—ã–µ –º–æ–¥–µ–ª–∏
        return {
            'GBM': [
                {'num_boost_round': 500, 'learning_rate': 0.1},
                {'num_boost_round': 1000, 'learning_rate': 0.05}
            ],
            'RF': [
                {'n_estimators': 500, 'max_depth': 20},
                {'n_estimators': 1000, 'max_depth': 25}
            ],
            'XGB': [
                {'n_estimators': 500, 'max_depth': 10},
                {'n_estimators': 1000, 'max_depth': 12}
            ],
            'CAT': [
                {'iterations': 500, 'learning_rate': 0.1},
                {'iterations': 1000, 'learning_rate': 0.05}
            ]
        }

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
hyperparameters = create_hyperparameter_strategy(len(train_data), 'binary')
predictor.fit(train_data, hyperparameters=hyperparameters)
```

### 2. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–∏ –æ–±—É—á–µ–Ω–∏—è

```python
def optimize_training_time(data_size: int, available_time: int) -> Dict[str, Any]:
    """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–∏ –æ–±—É—á–µ–Ω–∏—è"""
    
    # –†–∞—Å—á–µ—Ç –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ –º–æ–¥–µ–ª—å
    time_per_model = available_time / 10  # 10 –º–æ–¥–µ–ª–µ–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    if data_size < 1000:
        # –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ
        return {
            'time_limit': time_per_model,
            'presets': 'optimize_for_deployment',
            'num_bag_folds': 3,
            'num_bag_sets': 1
        }
    
    elif data_size < 10000:
        # –£–º–µ—Ä–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
        return {
            'time_limit': time_per_model,
            'presets': 'medium_quality',
            'num_bag_folds': 5,
            'num_bag_sets': 1
        }
    
    else:
        # –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
        return {
            'time_limit': time_per_model,
            'presets': 'high_quality',
            'num_bag_folds': 5,
            'num_bag_sets': 2
        }

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
training_config = optimize_training_time(len(train_data), 3600)  # 1 —á–∞—Å
predictor.fit(train_data, **training_config)
```

## –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

<img src="images/optimized/validation_methods.png" alt="–í–∞–ª–∏–¥–∞—Ü–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ" style="max-width: 100%; height: auto; display: block; margin: 20px auto;">
*–†–∏—Å—É–Ω–æ–∫ 4: –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è ML-–º–æ–¥–µ–ª–µ–π*

**–ü–æ—á–µ–º—É –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è?** –ü–æ—Ç–æ–º—É —á—Ç–æ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—é –∏ –Ω–µ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º –æ—Ü–µ–Ω–∫–∞–º:

- **–°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏**: –í—ã–±–æ—Ä –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ –º–µ—Ç–æ–¥–∞ –¥–ª—è –∑–∞–¥–∞—á–∏
- **–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö**: –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/validation/test
- **–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è**: –ù–∞–¥–µ–∂–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- **–í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã**: –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- **A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ**: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö
- **–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ—Å—Ç—ã**: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ —Ä–∞–∑–ª–∏—á–∏–π

### 1. –°—Ç—Ä–∞—Ç–µ–≥–∏—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏

```python
def select_validation_strategy(data_size: int, problem_type: str, 
                             data_type: str = 'tabular') -> Dict[str, Any]:
    """–í—ã–±–æ—Ä —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
    
    if data_type == 'time_series':
        return {
            'validation_strategy': 'time_series_split',
            'n_splits': 5,
            'test_size': 0.2
        }
    
    elif data_size < 1000:
        return {
            'validation_strategy': 'holdout',
            'holdout_frac': 0.3
        }
    
    elif data_size < 10000:
        return {
            'validation_strategy': 'kfold',
            'num_bag_folds': 5,
            'num_bag_sets': 1
        }
    
    else:
        return {
            'validation_strategy': 'kfold',
            'num_bag_folds': 10,
            'num_bag_sets': 1
        }

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
validation_config = select_validation_strategy(len(train_data), 'binary')
predictor.fit(train_data, **validation_config)
```

### 2. –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è

```python
def perform_cross_validation(predictor, data: pd.DataFrame, 
                           n_folds: int = 5) -> Dict[str, Any]:
    """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
    
    from sklearn.model_selection import KFold
    import numpy as np
    
    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)
    
    fold_results = []
    
    for fold, (train_idx, val_idx) in enumerate(kf.split(data)):
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        train_fold = data.iloc[train_idx]
        val_fold = data.iloc[val_idx]
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        fold_predictor = TabularPredictor(
            label=predictor.label,
            problem_type=predictor.problem_type,
            eval_metric=predictor.eval_metric
        )
        
        fold_predictor.fit(train_fold, time_limit=300)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        predictions = fold_predictor.predict(val_fold)
        
        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        performance = fold_predictor.evaluate(val_fold)
        
        fold_results.append({
            'fold': fold + 1,
            'performance': performance
        })
    
    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    all_metrics = {}
    for result in fold_results:
        for metric, value in result['performance'].items():
            if metric not in all_metrics:
                all_metrics[metric] = []
            all_metrics[metric].append(value)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    cv_results = {}
    for metric, values in all_metrics.items():
        cv_results[metric] = {
            'mean': np.mean(values),
            'std': np.std(values),
            'min': np.min(values),
            'max': np.max(values)
        }
    
    return cv_results

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
cv_results = perform_cross_validation(predictor, train_data, n_folds=5)
print("Cross-validation results:")
for metric, stats in cv_results.items():
    print(f"{metric}: {stats['mean']:.4f} ¬± {stats['std']:.4f}")
```

## –†–∞–±–æ—Ç–∞ —Å –∞–Ω—Å–∞–º–±–ª—è–º–∏

### 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∞–Ω—Å–∞–º–±–ª–µ–π

```python
def configure_ensemble(data_size: int, problem_type: str) -> Dict[str, Any]:
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∞–Ω—Å–∞–º–±–ª—è"""
    
    if data_size < 1000:
        # –ü—Ä–æ—Å—Ç–æ–π –∞–Ω—Å–∞–º–±–ª—å
        return {
            'num_bag_folds': 3,
            'num_bag_sets': 1,
            'num_stack_levels': 0
        }
    
    elif data_size < 10000:
        # –£–º–µ—Ä–µ–Ω–Ω—ã–π –∞–Ω—Å–∞–º–±–ª—å
        return {
            'num_bag_folds': 5,
            'num_bag_sets': 1,
            'num_stack_levels': 1
        }
    
    else:
        # –°–ª–æ–∂–Ω—ã–π –∞–Ω—Å–∞–º–±–ª—å
        return {
            'num_bag_folds': 5,
            'num_bag_sets': 2,
            'num_stack_levels': 2
        }

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
ensemble_config = configure_ensemble(len(train_data), 'binary')
predictor.fit(train_data, **ensemble_config)
```

### 2. –ê–Ω–∞–ª–∏–∑ –∞–Ω—Å–∞–º–±–ª—è

```python
def analyze_ensemble(predictor) -> Dict[str, Any]:
    """–ê–Ω–∞–ª–∏–∑ –∞–Ω—Å–∞–º–±–ª—è"""
    
    # –õ–∏–¥–µ—Ä–±–æ—Ä–¥ –º–æ–¥–µ–ª–µ–π
    leaderboard = predictor.leaderboard()
    
    # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    ensemble_analysis = {
        'total_models': len(leaderboard),
        'best_model': leaderboard.iloc[0]['model'],
        'best_score': leaderboard.iloc[0]['score_val'],
        'model_diversity': calculate_model_diversity(leaderboard),
        'performance_gap': leaderboard.iloc[0]['score_val'] - leaderboard.iloc[-1]['score_val']
    }
    
    return ensemble_analysis

def calculate_model_diversity(leaderboard: pd.DataFrame) -> float:
    """–†–∞—Å—á–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –º–æ–¥–µ–ª–µ–π"""
    
    # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –ø–æ —Ç–∏–ø–∞–º –º–æ–¥–µ–ª–µ–π
    model_types = leaderboard['model'].str.split('_').str[0].value_counts()
    diversity = len(model_types) / len(leaderboard)
    
    return diversity

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
ensemble_analysis = analyze_ensemble(predictor)
print("Ensemble Analysis:")
for key, value in ensemble_analysis.items():
    print(f"{key}: {value}")
```

## –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

<img src="images/optimized/metrics_detailed.png" alt="–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏" style="max-width: 100%; height: auto; display: block; margin: 20px auto;">
*–†–∏—Å—É–Ω–æ–∫ 5: –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ ML-–º–æ–¥–µ–ª–µ–π*

**–ü–æ—á–µ–º—É –≤–∞–∂–Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏?** –ü–æ—Ç–æ–º—É —á—Ç–æ –º–µ–¥–ª–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ:

- **–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤**: –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CPU, –ø–∞–º—è—Ç–∏, GPU
- **–ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—è**: –£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
- **–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ**: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
- **–ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ**: –í—ã—è–≤–ª–µ–Ω–∏–µ —É–∑–∫–∏—Ö –º–µ—Å—Ç –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥**: –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- **–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ**: –ê–¥–∞–ø—Ç–∞—Ü–∏—è –∫ —Ä–æ—Å—Ç—É –Ω–∞–≥—Ä—É–∑–∫–∏

### 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤

```python
def optimize_resources(data_size: int, available_resources: Dict[str, int]) -> Dict[str, Any]:
    """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤"""
    
    # –†–∞—Å—á–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    if data_size < 1000:
        num_cpus = min(2, available_resources.get('cpus', 4))
        memory_limit = min(4, available_resources.get('memory', 8))
    elif data_size < 10000:
        num_cpus = min(4, available_resources.get('cpus', 8))
        memory_limit = min(8, available_resources.get('memory', 16))
    else:
        num_cpus = min(8, available_resources.get('cpus', 16))
        memory_limit = min(16, available_resources.get('memory', 32))
    
    return {
        'num_cpus': num_cpus,
        'num_gpus': available_resources.get('gpus', 0),
        'memory_limit': memory_limit
    }

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
resources = optimize_resources(len(train_data), {'cpus': 8, 'memory': 16, 'gpus': 1})
predictor.fit(train_data, ag_args_fit=resources)
```

### 2. –ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—è

```python
def configure_parallelization(data_size: int, problem_type: str) -> Dict[str, Any]:
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏–∏"""
    
    if data_size < 1000:
        # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
        return {
            'parallel_folds': False,
            'parallel_models': False
        }
    
    elif data_size < 10000:
        # –£–º–µ—Ä–µ–Ω–Ω–∞—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—è
        return {
            'parallel_folds': True,
            'parallel_models': False
        }
    
    else:
        # –ü–æ–ª–Ω–∞—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—è
        return {
            'parallel_folds': True,
            'parallel_models': True
        }

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
parallel_config = configure_parallelization(len(train_data), 'binary')
# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ ag_args_fit
```

## –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

<img src="images/optimized/production_architecture.png" alt="–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ" style="max-width: 100%; height: auto; display: block; margin: 20px auto;">
*–†–∏—Å—É–Ω–æ–∫ 6: –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è ML-—Å–∏—Å—Ç–µ–º*

**–ü–æ—á–µ–º—É –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–µ–Ω –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ ML-—Å–∏—Å—Ç–µ–º?** –ü–æ—Ç–æ–º—É —á—Ç–æ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –¥–µ–≥—Ä–∞–¥–∏—Ä–æ–≤–∞—Ç—å –∏ —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ:

- **–°–∏—Å—Ç–µ–º–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è**: –î–µ—Ç–∞–ª—å–Ω–∞—è —Ñ–∏–∫—Å–∞—Ü–∏—è –≤—Å–µ—Ö —Å–æ–±—ã—Ç–∏–π
- **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–∞—á–µ—Å—Ç–≤–∞**: –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- **–î–µ—Ç–µ–∫—Ü–∏—è –¥—Ä–µ–π—Ñ–∞**: –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –¥–∞–Ω–Ω—ã—Ö
- **–ê–ª–µ—Ä—Ç–∏–Ω–≥**: –£–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –æ –ø—Ä–æ–±–ª–µ–º–∞—Ö –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
- **–î–∞—à–±–æ—Ä–¥—ã**: –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã
- **–ê–Ω–∞–ª–∏–∑ –ª–æ–≥–æ–≤**: –ü–æ–∏—Å–∫ –ø—Ä–∏—á–∏–Ω –ø—Ä–æ–±–ª–µ–º –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

### 1. –°–∏—Å—Ç–µ–º–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è

```python
import logging
from datetime import datetime
import json

class AutoGluonLogger:
    """–°–∏—Å—Ç–µ–º–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è AutoGluon"""
    
    def __init__(self, log_file: str = 'autogluon.log'):
        self.log_file = log_file
        self.setup_logging()
    
    def setup_logging(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(self.log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def log_training_start(self, data_info: Dict[str, Any]):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞—á–∞–ª–∞ –æ–±—É—á–µ–Ω–∏—è"""
        self.logger.info(f"Training started: {data_info}")
    
    def log_training_progress(self, progress: Dict[str, Any]):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
        self.logger.info(f"Training progress: {progress}")
    
    def log_training_complete(self, results: Dict[str, Any]):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è"""
        self.logger.info(f"Training completed: {results}")
    
    def log_prediction(self, input_data: Dict, prediction: Any, 
                      processing_time: float):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"""
        log_entry = {
            'timestamp': datetime.now().isoformat(),
            'input_data': input_data,
            'prediction': prediction,
            'processing_time': processing_time
        }
        self.logger.info(f"Prediction: {log_entry}")
    
    def log_error(self, error: Exception, context: Dict[str, Any]):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–æ–∫"""
        error_entry = {
            'timestamp': datetime.now().isoformat(),
            'error': str(error),
            'context': context
        }
        self.logger.error(f"Error: {error_entry}")

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
logger = AutoGluonLogger()
logger.log_training_start({'data_size': len(train_data), 'features': len(train_data.columns)})
```

### 2. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

```python
import psutil
import time
from typing import Dict, Any

class PerformanceMonitor:
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    
    def __init__(self):
        self.metrics_history = []
    
    def get_system_metrics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫"""
        return {
            'cpu_percent': psutil.cpu_percent(),
            'memory_percent': psutil.virtual_memory().percent,
            'disk_percent': psutil.disk_usage('/').percent,
            'timestamp': datetime.now().isoformat()
        }
    
    def monitor_training(self, predictor, data: pd.DataFrame):
        """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ–±—É—á–µ–Ω–∏—è"""
        start_time = time.time()
        
        # –ù–∞—á–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        initial_metrics = self.get_system_metrics()
        self.metrics_history.append(initial_metrics)
        
        # –û–±—É—á–µ–Ω–∏–µ —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º
        predictor.fit(data, time_limit=3600)
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        final_metrics = self.get_system_metrics()
        final_metrics['training_time'] = time.time() - start_time
        self.metrics_history.append(final_metrics)
        
        return final_metrics
    
    def analyze_performance(self) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        if len(self.metrics_history) < 2:
            return {}
        
        # –ê–Ω–∞–ª–∏–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤
        cpu_usage = [m['cpu_percent'] for m in self.metrics_history]
        memory_usage = [m['memory_percent'] for m in self.metrics_history]
        
        return {
            'avg_cpu_usage': sum(cpu_usage) / len(cpu_usage),
            'max_cpu_usage': max(cpu_usage),
            'avg_memory_usage': sum(memory_usage) / len(memory_usage),
            'max_memory_usage': max(memory_usage),
            'training_time': self.metrics_history[-1].get('training_time', 0)
        }

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
monitor = PerformanceMonitor()
final_metrics = monitor.monitor_training(predictor, train_data)
performance_analysis = monitor.analyze_performance()
print(f"Performance analysis: {performance_analysis}")
```

## –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫

### 1. –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–π

```python
def safe_training(predictor, data: pd.DataFrame, **kwargs) -> Dict[str, Any]:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    
    try:
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        predictor.fit(data, **kwargs)
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        if hasattr(predictor, 'evaluate'):
            performance = predictor.evaluate(data)
            return {
                'status': 'success',
                'performance': performance,
                'error': None
            }
        else:
            return {
                'status': 'success',
                'performance': None,
                'error': None
            }
    
    except MemoryError as e:
        return {
            'status': 'error',
            'performance': None,
            'error': f'Memory error: {str(e)}',
            'suggestion': 'Reduce data size or increase memory'
        }
    
    except TimeoutError as e:
        return {
            'status': 'error',
            'performance': None,
            'error': f'Timeout error: {str(e)}',
            'suggestion': 'Increase time_limit or reduce model complexity'
        }
    
    except Exception as e:
        return {
            'status': 'error',
            'performance': None,
            'error': f'Unexpected error: {str(e)}',
            'suggestion': 'Check data quality and parameters'
        }

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
result = safe_training(predictor, train_data, time_limit=3600)
if result['status'] == 'success':
    print(f"Training successful: {result['performance']}")
else:
    print(f"Training failed: {result['error']}")
    print(f"Suggestion: {result['suggestion']}")
```

### 2. –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ –æ—à–∏–±–æ–∫

```python
def resilient_training(predictor, data: pd.DataFrame, 
                      fallback_strategies: List[Dict[str, Any]]) -> Dict[str, Any]:
    """–£—Å—Ç–æ–π—á–∏–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å fallback —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏"""
    
    for i, strategy in enumerate(fallback_strategies):
        try:
            # –ü–æ–ø—ã—Ç–∫–∞ –æ–±—É—á–µ–Ω–∏—è —Å —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π
            predictor.fit(data, **strategy)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            if validate_model(predictor):
                return {
                    'status': 'success',
                    'strategy_used': i,
                    'strategy_config': strategy
                }
            else:
                continue
        
        except Exception as e:
            print(f"Strategy {i} failed: {str(e)}")
            continue
    
    return {
        'status': 'error',
        'error': 'All strategies failed',
        'suggestions': [
            'Check data quality',
            'Reduce model complexity',
            'Increase time limits',
            'Use simpler algorithms'
        ]
    }

# Fallback —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
fallback_strategies = [
    {'presets': 'best_quality', 'time_limit': 3600},
    {'presets': 'high_quality', 'time_limit': 1800},
    {'presets': 'medium_quality', 'time_limit': 900},
    {'presets': 'optimize_for_deployment', 'time_limit': 300}
]

result = resilient_training(predictor, train_data, fallback_strategies)
```

## –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞

### 1. –°–∂–∞—Ç–∏–µ –º–æ–¥–µ–ª–∏

```python
def optimize_for_production(predictor, target_size_mb: int = 100) -> Dict[str, Any]:
    """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞"""
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏
    current_size = get_model_size(predictor)
    
    if current_size <= target_size_mb:
        return {
            'status': 'already_optimized',
            'current_size': current_size,
            'target_size': target_size_mb
        }
    
    # –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    optimization_strategies = [
        {
            'name': 'reduce_models',
            'config': {
                'excluded_model_types': ['KNN', 'NN_TORCH'],
                'presets': 'optimize_for_deployment'
            }
        },
        {
            'name': 'compress_models',
            'config': {
                'save_space': True,
                'compress': True
            }
        },
        {
            'name': 'simplify_ensemble',
            'config': {
                'num_bag_folds': 3,
                'num_bag_sets': 1,
                'num_stack_levels': 0
            }
        }
    ]
    
    for strategy in optimization_strategies:
        try:
            # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
            optimized_predictor = apply_optimization_strategy(predictor, strategy)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞
            optimized_size = get_model_size(optimized_predictor)
            
            if optimized_size <= target_size_mb:
                return {
                    'status': 'optimized',
                    'strategy': strategy['name'],
                    'original_size': current_size,
                    'optimized_size': optimized_size,
                    'compression_ratio': optimized_size / current_size
                }
        
        except Exception as e:
            print(f"Optimization strategy {strategy['name']} failed: {e}")
            continue
    
    return {
        'status': 'failed',
        'error': 'Could not achieve target size',
        'suggestions': [
            'Increase target size',
            'Use simpler algorithms',
            'Reduce training data',
            'Use model compression techniques'
        ]
    }

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
optimization_result = optimize_for_production(predictor, target_size_mb=50)
print(f"Optimization result: {optimization_result}")
```

### 2. –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π

```python
import hashlib
import json
from typing import Optional

class PredictionCache:
    """–ö—ç—à –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"""
    
    def __init__(self, cache_size: int = 1000):
        self.cache_size = cache_size
        self.cache = {}
        self.access_count = {}
    
    def _generate_cache_key(self, data: Dict) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –∫—ç—à–∞"""
        data_str = json.dumps(data, sort_keys=True)
        return hashlib.md5(data_str.encode()).hexdigest()
    
    def get_prediction(self, data: Dict) -> Optional[Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏–∑ –∫—ç—à–∞"""
        cache_key = self._generate_cache_key(data)
        
        if cache_key in self.cache:
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—á–µ—Ç—á–∏–∫–∞ –¥–æ—Å—Ç—É–ø–∞
            self.access_count[cache_key] = self.access_count.get(cache_key, 0) + 1
            return self.cache[cache_key]
        
        return None
    
    def set_prediction(self, data: Dict, prediction: Any):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ –∫—ç—à"""
        cache_key = self._generate_cache_key(data)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –∫—ç—à–∞
        if len(self.cache) >= self.cache_size:
            # –£–¥–∞–ª–µ–Ω–∏–µ –Ω–∞–∏–º–µ–Ω–µ–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
            least_used_key = min(self.access_count.keys(), key=self.access_count.get)
            del self.cache[least_used_key]
            del self.access_count[least_used_key]
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
        self.cache[cache_key] = prediction
        self.access_count[cache_key] = 1
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞"""
        return {
            'cache_size': len(self.cache),
            'max_cache_size': self.cache_size,
            'hit_rate': self.calculate_hit_rate(),
            'most_accessed': max(self.access_count.items(), key=lambda x: x[1]) if self.access_count else None
        }
    
    def calculate_hit_rate(self) -> float:
        """–†–∞—Å—á–µ—Ç hit rate –∫—ç—à–∞"""
        if not self.access_count:
            return 0.0
        
        total_accesses = sum(self.access_count.values())
        cache_hits = len(self.cache)
        return cache_hits / total_accesses if total_accesses > 0 else 0.0

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
cache = PredictionCache(cache_size=1000)

def cached_predict(predictor, data: Dict) -> Any:
    """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ"""
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
    cached_prediction = cache.get_prediction(data)
    if cached_prediction is not None:
        return cached_prediction
    
    # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    prediction = predictor.predict(pd.DataFrame([data]))
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
    cache.set_prediction(data, prediction)
    
    return prediction
```

## –≠—Ç–∏–∫–∞ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å

<img src="images/optimized/metrics_comparison.png" alt="–≠—Ç–∏–∫–∞ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å" style="max-width: 100%; height: auto; display: block; margin: 20px auto;">
*–†–∏—Å—É–Ω–æ–∫ 7: –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ —ç—Ç–∏–∫–∏ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –≤ ML*

**–ü–æ—á–µ–º—É –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã —ç—Ç–∏–∫–∞ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –≤ ML?** –ü–æ—Ç–æ–º—É —á—Ç–æ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –º–æ–≥—É—Ç –Ω–∞–Ω–µ—Å—Ç–∏ —Å–µ—Ä—å–µ–∑–Ω—ã–π –≤—Ä–µ–¥:

- **–°–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç—å**: –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ü–∏–∏ –∏ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏
- **–ü—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å**: –û–±—ä—è—Å–Ω–∏–º–æ—Å—Ç—å —Ä–µ—à–µ–Ω–∏–π –º–æ–¥–µ–ª–∏
- **–ö–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å**: –ó–∞—â–∏—Ç–∞ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- **–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å**: –ó–∞—â–∏—Ç–∞ –æ—Ç –∞—Ç–∞–∫ –∏ –∑–ª–æ—É–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–π
- **–û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å**: –ß–µ—Ç–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –∑–∞ —Ä–µ—à–µ–Ω–∏—è
- **–†–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ**: –°–æ–±–ª—é–¥–µ–Ω–∏–µ –ø—Ä–∞–≤–æ–≤—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π

### üéØ –ö–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã —ç—Ç–∏—á–Ω–æ–≥–æ ML

**–ü–æ—á–µ–º—É —Å–ª–µ–¥—É—é—Ç —ç—Ç–∏—á–µ—Å–∫–∏–º –ø—Ä–∏–Ω—Ü–∏–ø–∞–º?** –ü–æ—Ç–æ–º—É —á—Ç–æ —ç—Ç–æ –æ—Å–Ω–æ–≤–∞ –¥–æ–≤–µ—Ä–∏—è –∏ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ —É—Å–ø–µ—Ö–∞:

- **–ü—Ä–∏–Ω—Ü–∏–ø "–°–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç–∏"**: –†–∞–≤–Ω–æ–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∫–æ –≤—Å–µ–º –≥—Ä—É–ø–ø–∞–º
- **–ü—Ä–∏–Ω—Ü–∏–ø "–ü—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç–∏"**: –ü–æ–Ω—è—Ç–Ω–æ—Å—Ç—å —Ä–µ—à–µ–Ω–∏–π –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
- **–ü—Ä–∏–Ω—Ü–∏–ø "–ö–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏"**: –ó–∞—â–∏—Ç–∞ –ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- **–ü—Ä–∏–Ω—Ü–∏–ø "–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"**: –ó–∞—â–∏—Ç–∞ –æ—Ç –∑–ª–æ–Ω–∞–º–µ—Ä–µ–Ω–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
- **–ü—Ä–∏–Ω—Ü–∏–ø "–û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏"**: –ß–µ—Ç–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏
- **–ü—Ä–∏–Ω—Ü–∏–ø "–ß–µ–ª–æ–≤–µ—á–Ω–æ—Å—Ç–∏"**: –£–≤–∞–∂–µ–Ω–∏–µ –∫ –ø—Ä–∞–≤–∞–º –∏ –¥–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤—É –ª—é–¥–µ–π

## –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏

–ü–æ—Å–ª–µ –æ—Å–≤–æ–µ–Ω–∏—è –ª—É—á—à–∏—Ö –ø—Ä–∞–∫—Ç–∏–∫ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç–µ –∫:
- [–ü—Ä–∏–º–µ—Ä–∞–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è](./09_examples.md)
- [Troubleshooting](./10_troubleshooting.md)
