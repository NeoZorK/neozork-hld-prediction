# –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è AutoML Gluon

**–ê–≤—Ç–æ—Ä:** Shcherbyna Rostyslav  
**–î–∞—Ç–∞:** 2024  

## –ü–æ—á–µ–º—É –ø—Ä–∏–º–µ—Ä—ã –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã

**–ü–æ—á–µ–º—É 90% —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ –Ω–∞—á–∏–Ω–∞—é—Ç —Å –ø—Ä–∏–º–µ—Ä–æ–≤, –∞ –Ω–µ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏?** –ü–æ—Ç–æ–º—É —á—Ç–æ –ø—Ä–∏–º–µ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, –∫–∞–∫ —Ç–µ–æ—Ä–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ. –≠—Ç–æ –∫–∞–∫ –æ–±—É—á–µ–Ω–∏–µ –≤–æ–∂–¥–µ–Ω–∏—é - —Å–Ω–∞—á–∞–ª–∞ —Å–º–æ—Ç—Ä–∏—à—å, –∫–∞–∫ –µ–∑–¥—è—Ç –¥—Ä—É–≥–∏–µ.

### –ü—Ä–æ–±–ª–µ–º—ã –±–µ–∑ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
- **–î–æ–ª–≥–æ–µ –∏–∑—É—á–µ–Ω–∏–µ**: –ú–µ—Å—è—Ü—ã –Ω–∞ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π
- **–û—à–∏–±–∫–∏ –≤ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏**: –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ API
- **–ù–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è**: –ò–∑–æ–±—Ä–µ—Ç–µ–Ω–∏–µ –≤–µ–ª–æ—Å–∏–ø–µ–¥–∞
- **–†–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω–∏–µ**: –°–ª–æ–∂–Ω–æ—Å—Ç—å –æ—Ç–ø—É–≥–∏–≤–∞–µ—Ç –Ω–æ–≤–∏—á–∫–æ–≤

### –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ —Ö–æ—Ä–æ—à–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
- **–ë—ã—Å—Ç—Ä–æ–µ —Å—Ç–∞—Ä—Ç**: –û—Ç –∏–¥–µ–∏ –¥–æ —Ä–∞–±–æ—Ç–∞—é—â–µ–≥–æ –∫–æ–¥–∞ –∑–∞ —á–∞—Å—ã
- **–ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã**: –ò–∑—É—á–µ–Ω–∏–µ –ª—É—á—à–∏—Ö –ø—Ä–∞–∫—Ç–∏–∫ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö
- **–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å**: –ü–æ–Ω–∏–º–∞–Ω–∏–µ —Ç–æ–≥–æ, –∫–∞–∫ –≤—Å–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
- **–í–¥–æ—Ö–Ω–æ–≤–µ–Ω–∏–µ**: –ò–¥–µ–∏ –¥–ª—è —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤

## –í–≤–µ–¥–µ–Ω–∏–µ –≤ –ø—Ä–∏–º–µ—Ä—ã

<img src="images/optimized/monte_carlo_analysis.png" alt="Monte Carlo –∞–Ω–∞–ª–∏–∑" style="max-width: 100%; height: auto; display: block; margin: 20px auto;">
*–†–∏—Å—É–Ω–æ–∫ 1: Monte Carlo –∞–Ω–∞–ª–∏–∑ - —Ä–æ–±–∞—Å—Ç–Ω—ã–µ vs –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã, —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–±—ã–ª–∏, risk-return –ø—Ä–æ—Ñ–∏–ª—å*

**–ü–æ—á–µ–º—É –ø—Ä–∏–º–µ—Ä—ã - —ç—Ç–æ —è–∑—ã–∫ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è?** –ü–æ—Ç–æ–º—É —á—Ç–æ –æ–Ω–∏ –ø–µ—Ä–µ–≤–æ–¥—è—Ç —Å–ª–æ–∂–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –≤ –ø–æ–Ω—è—Ç–Ω—ã–µ —á–∏—Å–ª–∞. –≠—Ç–æ –∫–∞–∫ –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫ –º–µ–∂–¥—É —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ –¥–µ—Ç–∞–ª—è–º–∏ –∏ –±–∏–∑–Ω–µ—Å-—Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏.

**–¢–∏–ø—ã –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ AutoML Gluon:**
- **–ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã**: –ü—Ä–æ—Å—Ç—ã–µ –∑–∞–¥–∞—á–∏ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –æ—Å–Ω–æ–≤
- **–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø—Ä–∏–º–µ—Ä—ã**: –°–ª–æ–∂–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –¥–ª—è –æ–ø—ã—Ç–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
- **–†–µ–∞–ª—å–Ω—ã–µ –ø—Ä–æ–µ–∫—Ç—ã**: –ü–æ–ª–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è —Ä–µ–∞–ª—å–Ω—ã—Ö –±–∏–∑–Ω–µ—Å-–∑–∞–¥–∞—á
- **–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã**: –î–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤ (–º–µ–¥–∏—Ü–∏–Ω–∞, —Ñ–∏–Ω–∞–Ω—Å—ã)

–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è AutoML Gluon –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ö–∞–∂–¥—ã–π –ø—Ä–∏–º–µ—Ä –≤–∫–ª—é—á–∞–µ—Ç –ø–æ–ª–Ω—ã–π –∫–æ–¥, –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –∏ –ª—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏.

## –ü—Ä–∏–º–µ—Ä 1: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤ –±–∞–Ω–∫–∞

<img src="images/optimized/robustness_analysis.png" alt="–ë–∞–Ω–∫–æ–≤—Å–∫–∏–π –ø—Ä–∏–º–µ—Ä" style="max-width: 100%; height: auto; display: block; margin: 20px auto;">
*–†–∏—Å—É–Ω–æ–∫ 2: –ü—Ä–∏–º–µ—Ä –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤ –±–∞–Ω–∫–∞ - –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–µ—Ñ–æ–ª—Ç–∞*

**–ü–æ—á–µ–º—É –Ω–∞—á–∏–Ω–∞–µ–º —Å –±–∞–Ω–∫–æ–≤—Å–∫–æ–π –∑–∞–¥–∞—á–∏?** –ü–æ—Ç–æ–º—É —á—Ç–æ —ç—Ç–æ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –ø—Ä–∏–º–µ—Ä ML –≤ —Ñ–∏–Ω–∞–Ω—Å–∞—Ö - –ø–æ–Ω—è—Ç–Ω—ã–π, –≤–∞–∂–Ω—ã–π –∏ —Å —á–µ—Ç–∫–∏–º–∏ –±–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫–∞–º–∏.

**–ö–ª—é—á–µ–≤—ã–µ –∞—Å–ø–µ–∫—Ç—ã –±–∞–Ω–∫–æ–≤—Å–∫–æ–≥–æ ML:**
- **–§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Ä–∏—Å–∫–∏**: –û—Ü–µ–Ω–∫–∞ –∫—Ä–µ–¥–∏—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤
- **–†–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ**: –°–æ–±–ª—é–¥–µ–Ω–∏–µ –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤
- **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å**: –û–±—ä—è—Å–Ω–∏–º–æ—Å—Ç—å —Ä–µ—à–µ–Ω–∏–π –¥–ª—è —Ä–µ–≥—É–ª—è—Ç–æ—Ä–æ–≤
- **–°–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç—å**: –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ü–∏–∏
- **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥**: –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
- **A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ**: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

### –ó–∞–¥–∞—á–∞
**–ü–æ—á–µ–º—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–µ—Ñ–æ–ª—Ç–∞ —Ç–∞–∫ –≤–∞–∂–Ω–æ?** –ü–æ—Ç–æ–º—É —á—Ç–æ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –º–æ–∂–µ—Ç —Å—Ç–æ–∏—Ç—å –±–∞–Ω–∫—É –º–∏–ª–ª–∏–æ–Ω—ã –¥–æ–ª–ª–∞—Ä–æ–≤. –≠—Ç–æ –∫–∞–∫ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞, –Ω–æ –¥–ª—è –¥–µ–Ω–µ–≥.

–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–µ—Ñ–æ–ª—Ç–∞ –∫–ª–∏–µ–Ω—Ç–∞ –±–∞–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π.

**–ë–∏–∑–Ω–µ—Å-–∫–æ–Ω—Ç–µ–∫—Å—Ç:**
- **–¶–µ–ª—å**: –ú–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Ç–µ—Ä–∏ –æ—Ç –ø–ª–æ—Ö–∏—Ö –∫—Ä–µ–¥–∏—Ç–æ–≤
- **–ú–µ—Ç—Ä–∏–∫–∞**: ROC-AUC (–≤–∞–∂–Ω–µ–µ —Ç–æ—á–Ω–æ—Å—Ç—å –¥–ª—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤)
- **–°—Ç–æ–∏–º–æ—Å—Ç—å –æ—à–∏–±–∫–∏**: –õ–æ–∂–Ω—ã–π –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–æ—Ä–æ–∂–µ –ª–æ–∂–Ω–æ–≥–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ
- **–û–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö**: –û–±—ã—á–Ω–æ 100K-1M –∑–∞–ø–∏—Å–µ–π

### –î–∞–Ω–Ω—ã–µ
**–ü–æ—á–µ–º—É –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ?** –ü–æ—Ç–æ–º—É —á—Ç–æ —Ä–µ–∞–ª—å–Ω—ã–µ –±–∞–Ω–∫–æ–≤—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω—ã, –Ω–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –æ—Å—Ç–∞—é—Ç—Å—è —Ç–µ–º–∏ –∂–µ.

```python
import pandas as pd
import numpy as np
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from autogluon.tabular import TabularPredictor
import matplotlib.pyplot as plt
import seaborn as sns

# –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±–∞–Ω–∫–æ–≤—Å–∫–æ–π –∑–∞–¥–∞—á–∏
def create_bank_data(n_samples=10000):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö - –∏–º–∏—Ç–∞—Ü–∏—è —Ä–µ–∞–ª—å–Ω—ã—Ö –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —Å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    X, y = make_classification(
        n_samples=n_samples,        # –†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏
        n_features=20,             # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        n_informative=15,          # –ò–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–≤–∞–∂–Ω—ã–µ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è)
        n_redundant=5,             # –ò–∑–±—ã—Ç–æ—á–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ)
        n_classes=2,               # –ë–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (–¥–µ—Ñ–æ–ª—Ç/–Ω–µ –¥–µ—Ñ–æ–ª—Ç)
        random_state=42            # –í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ DataFrame —Å –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏
    feature_names = [
        'age', 'income', 'credit_score', 'debt_ratio', 'employment_years',
        'loan_amount', 'interest_rate', 'payment_history', 'savings_balance',
        'investment_value', 'credit_cards', 'late_payments', 'bankruptcies',
        'foreclosures', 'collections', 'inquiries', 'credit_utilization',
        'account_age', 'payment_frequency', 'credit_mix'
    ]
    
    data = pd.DataFrame(X, columns=feature_names)
    data['default_risk'] = y
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
    data['employment_status'] = np.random.choice(['employed', 'unemployed', 'self_employed'], n_samples)
    data['education'] = np.random.choice(['high_school', 'bachelor', 'master', 'phd'], n_samples)
    data['marital_status'] = np.random.choice(['single', 'married', 'divorced'], n_samples)
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
    data['application_date'] = pd.date_range('2020-01-01', periods=n_samples, freq='D')
    
    return data

# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
bank_data = create_bank_data(10000)
print("Bank data shape:", bank_data.shape)
print("Default rate:", bank_data['default_risk'].mean())
```

### –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
```python
def prepare_bank_data(data):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    data = data.fillna(data.median())
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    data['debt_to_income'] = data['debt_ratio'] * data['income']
    data['credit_utilization_ratio'] = data['credit_utilization'] / (data['credit_score'] + 1)
    data['payment_stability'] = data['payment_history'] / (data['late_payments'] + 1)
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤
    numeric_columns = data.select_dtypes(include=[np.number]).columns
    for col in numeric_columns:
        if col != 'default_risk':
            Q1 = data[col].quantile(0.25)
            Q3 = data[col].quantile(0.75)
            IQR = Q3 - Q1
            data[col] = np.where(data[col] < Q1 - 1.5 * IQR, Q1 - 1.5 * IQR, data[col])
            data[col] = np.where(data[col] > Q3 + 1.5 * IQR, Q3 + 1.5 * IQR, data[col])
    
    return data

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
bank_data_processed = prepare_bank_data(bank_data)
```

### –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
```python
def train_bank_model(data):
    """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –±–∞–Ω–∫–æ–≤—Å–∫–æ–π –∑–∞–¥–∞—á–∏"""
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
    train_data, test_data = train_test_split(data, test_size=0.2, random_state=42, stratify=data['default_risk'])
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞
    predictor = TabularPredictor(
        label='default_risk',
        problem_type='binary',
        eval_metric='roc_auc',
        path='./bank_models'
    )
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –±–∞–Ω–∫–æ–≤—Å–∫–æ–π –∑–∞–¥–∞—á–∏
    hyperparameters = {
        'GBM': [
            {
                'num_boost_round': 200,
                'learning_rate': 0.1,
                'num_leaves': 31,
                'feature_fraction': 0.9,
                'bagging_fraction': 0.8,
                'min_data_in_leaf': 20
            }
        ],
        'XGB': [
            {
                'n_estimators': 200,
                'learning_rate': 0.1,
                'max_depth': 6,
                'subsample': 0.8,
                'colsample_bytree': 0.8
            }
        ],
        'CAT': [
            {
                'iterations': 200,
                'learning_rate': 0.1,
                'depth': 6,
                'l2_leaf_reg': 3.0
            }
        ]
    }
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    predictor.fit(
        train_data,
        hyperparameters=hyperparameters,
        time_limit=1800,  # 30 –º–∏–Ω—É—Ç
        presets='high_quality',
        num_bag_folds=5,
        num_bag_sets=1
    )
    
    return predictor, test_data

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
bank_predictor, bank_test_data = train_bank_model(bank_data_processed)
```

### –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
```python
def evaluate_bank_model(predictor, test_data):
    """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –±–∞–Ω–∫–æ–≤—Å–∫–æ–π –º–æ–¥–µ–ª–∏"""
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    predictions = predictor.predict(test_data)
    probabilities = predictor.predict_proba(test_data)
    
    # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
    performance = predictor.evaluate(test_data)
    
    # –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_importance = predictor.feature_importance()
    
    # –õ–∏–¥–µ—Ä–±–æ—Ä–¥ –º–æ–¥–µ–ª–µ–π
    leaderboard = predictor.leaderboard(test_data)
    
    return {
        'performance': performance,
        'feature_importance': feature_importance,
        'leaderboard': leaderboard,
        'predictions': predictions,
        'probabilities': probabilities
    }

# –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
bank_results = evaluate_bank_model(bank_predictor, bank_test_data)

print("Bank Model Performance:")
for metric, value in bank_results['performance'].items():
    print(f"{metric}: {value:.4f}")

print("\nTop 10 Feature Importance:")
print(bank_results['feature_importance'].head(10))
```

### –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
```python
def visualize_bank_results(results, test_data):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–∞–Ω–∫–æ–≤—Å–∫–æ–π –º–æ–¥–µ–ª–∏"""
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # ROC –∫—Ä–∏–≤–∞—è
    from sklearn.metrics import roc_curve, auc
    fpr, tpr, _ = roc_curve(test_data['default_risk'], results['probabilities'][1])
    roc_auc = auc(fpr, tpr)
    
    axes[0, 0].plot(fpr, tpr, label=f'ROC AUC = {roc_auc:.3f}')
    axes[0, 0].plot([0, 1], [0, 1], 'k--')
    axes[0, 0].set_xlabel('False Positive Rate')
    axes[0, 0].set_ylabel('True Positive Rate')
    axes[0, 0].set_title('ROC Curve')
    axes[0, 0].legend()
    
    # Precision-Recall –∫—Ä–∏–≤–∞—è
    from sklearn.metrics import precision_recall_curve
    precision, recall, _ = precision_recall_curve(test_data['default_risk'], results['probabilities'][1])
    
    axes[0, 1].plot(recall, precision)
    axes[0, 1].set_xlabel('Recall')
    axes[0, 1].set_ylabel('Precision')
    axes[0, 1].set_title('Precision-Recall Curve')
    
    # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    results['feature_importance'].head(10).plot(kind='barh', ax=axes[1, 0])
    axes[1, 0].set_title('Top 10 Feature Importance')
    
    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
    axes[1, 1].hist(results['probabilities'][1], bins=50, alpha=0.7)
    axes[1, 1].set_xlabel('Default Probability')
    axes[1, 1].set_ylabel('Frequency')
    axes[1, 1].set_title('Distribution of Default Probabilities')
    
    plt.tight_layout()
    plt.show()

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
visualize_bank_results(bank_results, bank_test_data)
```

## –ü—Ä–∏–º–µ—Ä 2: –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Ü–µ–Ω –Ω–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å

<img src="images/optimized/performance_comparison.png" alt="–ü—Ä–∏–º–µ—Ä –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏" style="max-width: 100%; height: auto; display: block; margin: 20px auto;">
*–†–∏—Å—É–Ω–æ–∫ 3: –ü—Ä–∏–º–µ—Ä –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è —Ü–µ–Ω –Ω–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å - —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–∞—è –∑–∞–¥–∞—á–∞*

**–ü–æ—á–µ–º—É –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å - –æ—Ç–ª–∏—á–Ω—ã–π –ø—Ä–∏–º–µ—Ä –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏?** –ü–æ—Ç–æ–º—É —á—Ç–æ —ç—Ç–æ –ø–æ–Ω—è—Ç–Ω–∞—è –∑–∞–¥–∞—á–∞ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–æ–º —Ñ–∞–∫—Ç–æ—Ä–æ–≤ –≤–ª–∏—è–Ω–∏—è:

**–ö–ª—é—á–µ–≤—ã–µ –∞—Å–ø–µ–∫—Ç—ã —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –≤ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏:**
- **–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã**: –ü–ª–æ—â–∞–¥—å, —Ä–∞–π–æ–Ω, —ç—Ç–∞–∂, –≥–æ–¥ –ø–æ—Å—Ç—Ä–æ–π–∫–∏
- **–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–∞—è —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è**: –¶–µ–Ω–∞ –≤ —Ä—É–±–ª—è—Ö
- **Feature Engineering**: –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö
- **–í–∞–ª–∏–¥–∞—Ü–∏—è**: –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ
- **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å**: –ü–æ–Ω–∏–º–∞–Ω–∏–µ –≤–ª–∏—è–Ω–∏—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–∫—Ç–æ—Ä–∞
- **–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞**: RMSE, MAE, R¬≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏

### –ó–∞–¥–∞—á–∞
–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ü–µ–Ω—ã –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –æ–±—ä–µ–∫—Ç–∞.

### –î–∞–Ω–Ω—ã–µ
```python
def create_real_estate_data(n_samples=5000):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –æ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏"""
    
    np.random.seed(42)
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
    data = pd.DataFrame({
        'area': np.random.normal(120, 30, n_samples),
        'bedrooms': np.random.poisson(3, n_samples),
        'bathrooms': np.random.poisson(2, n_samples),
        'age': np.random.exponential(10, n_samples),
        'garage': np.random.binomial(1, 0.7, n_samples),
        'pool': np.random.binomial(1, 0.2, n_samples),
        'garden': np.random.binomial(1, 0.6, n_samples)
    })
    
    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
    data['location'] = np.random.choice(['downtown', 'suburbs', 'rural'], n_samples)
    data['property_type'] = np.random.choice(['house', 'apartment', 'townhouse'], n_samples)
    data['condition'] = np.random.choice(['excellent', 'good', 'fair', 'poor'], n_samples)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π (—Ü–µ–Ω–∞)
    base_price = 100000
    price = (base_price + 
             data['area'] * 1000 +
             data['bedrooms'] * 10000 +
             data['bathrooms'] * 5000 +
             data['garage'] * 15000 +
             data['pool'] * 25000 +
             data['garden'] * 10000 -
             data['age'] * 2000)
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —à—É–º–∞
    price += np.random.normal(0, 20000, n_samples)
    data['price'] = np.maximum(price, 50000)  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞
    
    return data

# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
real_estate_data = create_real_estate_data(5000)
print("Real estate data shape:", real_estate_data.shape)
print("Price statistics:")
print(real_estate_data['price'].describe())
```

### –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
```python
def prepare_real_estate_data(data):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏"""
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    data['area_per_bedroom'] = data['area'] / (data['bedrooms'] + 1)
    data['total_rooms'] = data['bedrooms'] + data['bathrooms']
    data['age_category'] = pd.cut(data['age'], bins=[0, 5, 15, 30, 100], labels=['new', 'recent', 'old', 'very_old'])
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤
    data['area'] = np.where(data['area'] > 300, 300, data['area'])
    data['age'] = np.where(data['age'] > 50, 50, data['age'])
    
    return data

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
real_estate_processed = prepare_real_estate_data(real_estate_data)
```

### –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
```python
def train_real_estate_model(data):
    """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏"""
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
    train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞
    predictor = TabularPredictor(
        label='price',
        problem_type='regression',
        eval_metric='rmse',
        path='./real_estate_models'
    )
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
    hyperparameters = {
        'GBM': [
            {
                'num_boost_round': 300,
                'learning_rate': 0.1,
                'num_leaves': 31,
                'feature_fraction': 0.9,
                'bagging_fraction': 0.8,
                'min_data_in_leaf': 20
            }
        ],
        'XGB': [
            {
                'n_estimators': 300,
                'learning_rate': 0.1,
                'max_depth': 6,
                'subsample': 0.8,
                'colsample_bytree': 0.8
            }
        ],
        'RF': [
            {
                'n_estimators': 200,
                'max_depth': 15,
                'min_samples_split': 5,
                'min_samples_leaf': 2
            }
        ]
    }
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    predictor.fit(
        train_data,
        hyperparameters=hyperparameters,
        time_limit=1800,  # 30 –º–∏–Ω—É—Ç
        presets='high_quality',
        num_bag_folds=5,
        num_bag_sets=1
    )
    
    return predictor, test_data

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
real_estate_predictor, real_estate_test_data = train_real_estate_model(real_estate_processed)
```

### –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
```python
def evaluate_real_estate_model(predictor, test_data):
    """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏"""
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    predictions = predictor.predict(test_data)
    
    # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
    performance = predictor.evaluate(test_data)
    
    # –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_importance = predictor.feature_importance()
    
    # –õ–∏–¥–µ—Ä–±–æ—Ä–¥ –º–æ–¥–µ–ª–µ–π
    leaderboard = predictor.leaderboard(test_data)
    
    # –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫
    errors = test_data['price'] - predictions
    mae = np.mean(np.abs(errors))
    mape = np.mean(np.abs(errors / test_data['price'])) * 100
    
    return {
        'performance': performance,
        'feature_importance': feature_importance,
        'leaderboard': leaderboard,
        'predictions': predictions,
        'mae': mae,
        'mape': mape,
        'errors': errors
    }

# –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
real_estate_results = evaluate_real_estate_model(real_estate_predictor, real_estate_test_data)

print("Real Estate Model Performance:")
for metric, value in real_estate_results['performance'].items():
    print(f"{metric}: {value:.4f}")

print(f"\nMAE: {real_estate_results['mae']:.2f}")
print(f"MAPE: {real_estate_results['mape']:.2f}%")
```

### –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
```python
def visualize_real_estate_results(results, test_data):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏"""
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è vs –§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
    axes[0, 0].scatter(test_data['price'], results['predictions'], alpha=0.6)
    axes[0, 0].plot([test_data['price'].min(), test_data['price'].max()], 
                   [test_data['price'].min(), test_data['price'].max()], 'r--')
    axes[0, 0].set_xlabel('Actual Price')
    axes[0, 0].set_ylabel('Predicted Price')
    axes[0, 0].set_title('Predictions vs Actual')
    
    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫
    axes[0, 1].hist(results['errors'], bins=50, alpha=0.7)
    axes[0, 1].set_xlabel('Prediction Error')
    axes[0, 1].set_ylabel('Frequency')
    axes[0, 1].set_title('Distribution of Prediction Errors')
    
    # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    results['feature_importance'].head(10).plot(kind='barh', ax=axes[1, 0])
    axes[1, 0].set_title('Top 10 Feature Importance')
    
    # –û—à–∏–±–∫–∏ –ø–æ —Ü–µ–Ω–µ
    axes[1, 1].scatter(test_data['price'], results['errors'], alpha=0.6)
    axes[1, 1].set_xlabel('Actual Price')
    axes[1, 1].set_ylabel('Prediction Error')
    axes[1, 1].set_title('Errors by Price Range')
    axes[1, 1].axhline(y=0, color='r', linestyle='--')
    
    plt.tight_layout()
    plt.show()

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
visualize_real_estate_results(real_estate_results, real_estate_test_data)
```

## –ü—Ä–∏–º–µ—Ä 3: –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤

<img src="images/optimized/walk_forward_analysis.png" alt="–ü—Ä–∏–º–µ—Ä –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤" style="max-width: 100%; height: auto; display: block; margin: 20px auto;">
*–†–∏—Å—É–Ω–æ–∫ 4: –ü—Ä–∏–º–µ—Ä –∞–Ω–∞–ª–∏–∑–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ - –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–¥–∞–∂*

**–ü–æ—á–µ–º—É –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã - –æ—Å–æ–±—ã–π —Ç–∏–ø –∑–∞–¥–∞—á?** –ü–æ—Ç–æ–º—É —á—Ç–æ –æ–Ω–∏ –∏–º–µ—é—Ç –≤—Ä–µ–º–µ–Ω–Ω—É—é –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –∏ —Ç—Ä–µ–±—É—é—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤:

**–ö–ª—é—á–µ–≤—ã–µ –∞—Å–ø–µ–∫—Ç—ã –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤:**
- **–í—Ä–µ–º–µ–Ω–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å**: –ë—É–¥—É—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –∑–∞–≤–∏—Å—è—Ç –æ—Ç –ø—Ä–æ—à–ª—ã—Ö
- **–°–µ–∑–æ–Ω–Ω–æ—Å—Ç—å**: –ü–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤–æ –≤—Ä–µ–º–µ–Ω–∏
- **–¢—Ä–µ–Ω–¥—ã**: –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
- **–°—Ç–∞–Ω—Ü–∏–æ–Ω–∞—Ä–Ω–æ—Å—Ç—å**: –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Å–≤–æ–π—Å—Ç–≤
- **–í–∞–ª–∏–¥–∞—Ü–∏—è**: –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- **–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ**: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –±—É–¥—É—â–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π

### –ó–∞–¥–∞—á–∞
–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–¥–∞–∂ —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö.

### –î–∞–Ω–Ω—ã–µ
```python
def create_sales_data(n_days=365, n_products=10):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –æ –ø—Ä–æ–¥–∞–∂–∞—Ö"""
    
    np.random.seed(42)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞
    dates = pd.date_range('2023-01-01', periods=n_days, freq='D')
    
    data = []
    for product_id in range(n_products):
        # –ë–∞–∑–æ–≤—ã–π —Ç—Ä–µ–Ω–¥
        trend = np.linspace(100, 150, n_days)
        
        # –°–µ–∑–æ–Ω–Ω–æ—Å—Ç—å (–µ–∂–µ–Ω–µ–¥–µ–ª—å–Ω–∞—è)
        seasonality = 20 * np.sin(2 * np.pi * np.arange(n_days) / 7)
        
        # –°–ª—É—á–∞–π–Ω—ã–π —à—É–º
        noise = np.random.normal(0, 10, n_days)
        
        # –ü—Ä–æ–¥–∞–∂–∏
        sales = trend + seasonality + noise
        sales = np.maximum(sales, 0)  # –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ–¥–∞–∂–∏ –Ω–µ–≤–æ–∑–º–æ–∂–Ω—ã
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π
        for i, (date, sale) in enumerate(zip(dates, sales)):
            data.append({
                'date': date,
                'product_id': f'product_{product_id}',
                'sales': sale,
                'day_of_week': date.dayofweek,
                'month': date.month,
                'quarter': date.quarter
            })
    
    return pd.DataFrame(data)

# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
sales_data = create_sales_data(365, 10)
print("Sales data shape:", sales_data.shape)
print("Sales statistics:")
print(sales_data['sales'].describe())
```

### –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
```python
def prepare_sales_data(data):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ –ø—Ä–æ–¥–∞–∂–∞—Ö –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤"""
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ª–∞–≥–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    data = data.sort_values(['product_id', 'date'])
    
    for lag in [1, 2, 3, 7, 14, 30]:
        data[f'sales_lag_{lag}'] = data.groupby('product_id')['sales'].shift(lag)
    
    # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ
    for window in [7, 14, 30]:
        data[f'sales_ma_{window}'] = data.groupby('product_id')['sales'].rolling(window=window).mean().reset_index(0, drop=True)
    
    # –¢—Ä–µ–Ω–¥—ã
    data['sales_trend'] = data.groupby('product_id')['sales'].rolling(window=7).mean().reset_index(0, drop=True)
    
    # –°–µ–∑–æ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    data['is_weekend'] = (data['day_of_week'] >= 5).astype(int)
    data['is_month_start'] = (data['date'].dt.day <= 7).astype(int)
    data['is_month_end'] = (data['date'].dt.day >= 25).astype(int)
    
    return data

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
sales_processed = prepare_sales_data(sales_data)
```

### –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
```python
def train_sales_model(data):
    """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–¥–∞–∂"""
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π –¥–ª—è —Ç–µ—Å—Ç–∞)
    split_date = data['date'].max() - pd.Timedelta(days=30)
    train_data = data[data['date'] <= split_date]
    test_data = data[data['date'] > split_date]
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞
    predictor = TabularPredictor(
        label='sales',
        problem_type='regression',
        eval_metric='rmse',
        path='./sales_models'
    )
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
    hyperparameters = {
        'GBM': [
            {
                'num_boost_round': 200,
                'learning_rate': 0.1,
                'num_leaves': 31,
                'feature_fraction': 0.9,
                'bagging_fraction': 0.8,
                'min_data_in_leaf': 20
            }
        ],
        'XGB': [
            {
                'n_estimators': 200,
                'learning_rate': 0.1,
                'max_depth': 6,
                'subsample': 0.8,
                'colsample_bytree': 0.8
            }
        ]
    }
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    predictor.fit(
        train_data,
        hyperparameters=hyperparameters,
        time_limit=1800,  # 30 –º–∏–Ω—É—Ç
        presets='high_quality',
        num_bag_folds=3,  # –ú–µ–Ω—å—à–µ —Ñ–æ–ª–¥–æ–≤ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
        num_bag_sets=1
    )
    
    return predictor, test_data

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
sales_predictor, sales_test_data = train_sales_model(sales_processed)
```

### –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
```python
def evaluate_sales_model(predictor, test_data):
    """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ –ø—Ä–æ–¥–∞–∂"""
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    predictions = predictor.predict(test_data)
    
    # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
    performance = predictor.evaluate(test_data)
    
    # –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_importance = predictor.feature_importance()
    
    # –ê–Ω–∞–ª–∏–∑ –ø–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º
    product_performance = {}
    for product_id in test_data['product_id'].unique():
        product_data = test_data[test_data['product_id'] == product_id]
        product_predictions = predictions[test_data['product_id'] == product_id]
        
        mae = np.mean(np.abs(product_data['sales'] - product_predictions))
        mape = np.mean(np.abs((product_data['sales'] - product_predictions) / product_data['sales'])) * 100
        
        product_performance[product_id] = {
            'mae': mae,
            'mape': mape
        }
    
    return {
        'performance': performance,
        'feature_importance': feature_importance,
        'product_performance': product_performance,
        'predictions': predictions
    }

# –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
sales_results = evaluate_sales_model(sales_predictor, sales_test_data)

print("Sales Model Performance:")
for metric, value in sales_results['performance'].items():
    print(f"{metric}: {value:.4f}")

print("\nProduct Performance:")
for product, perf in sales_results['product_performance'].items():
    print(f"{product}: MAE={perf['mae']:.2f}, MAPE={perf['mape']:.2f}%")
```

### –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
```python
def visualize_sales_results(results, test_data):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –º–æ–¥–µ–ª–∏ –ø—Ä–æ–¥–∞–∂"""
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # –í—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥ –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞
    product_id = test_data['product_id'].iloc[0]
    product_data = test_data[test_data['product_id'] == product_id]
    product_predictions = results['predictions'][test_data['product_id'] == product_id]
    
    axes[0, 0].plot(product_data['date'], product_data['sales'], label='Actual', alpha=0.7)
    axes[0, 0].plot(product_data['date'], product_predictions, label='Predicted', alpha=0.7)
    axes[0, 0].set_title(f'Sales Forecast for {product_id}')
    axes[0, 0].set_xlabel('Date')
    axes[0, 0].set_ylabel('Sales')
    axes[0, 0].legend()
    
    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫
    errors = test_data['sales'] - results['predictions']
    axes[0, 1].hist(errors, bins=30, alpha=0.7)
    axes[0, 1].set_xlabel('Prediction Error')
    axes[0, 1].set_ylabel('Frequency')
    axes[0, 1].set_title('Distribution of Prediction Errors')
    
    # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    results['feature_importance'].head(10).plot(kind='barh', ax=axes[1, 0])
    axes[1, 0].set_title('Top 10 Feature Importance')
    
    # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º
    products = list(results['product_performance'].keys())
    maes = [results['product_performance'][p]['mae'] for p in products]
    
    axes[1, 1].bar(products, maes)
    axes[1, 1].set_xlabel('Product')
    axes[1, 1].set_ylabel('MAE')
    axes[1, 1].set_title('Performance by Product')
    axes[1, 1].tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    plt.show()

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
visualize_sales_results(sales_results, sales_test_data)
```

## –ü—Ä–∏–º–µ—Ä 4: –ú–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è

<img src="images/optimized/advanced_topics_overview.png" alt="–ü—Ä–∏–º–µ—Ä –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏" style="max-width: 100%; height: auto; display: block; margin: 20px auto;">
*–†–∏—Å—É–Ω–æ–∫ 5: –ü—Ä–∏–º–µ—Ä –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ - –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π*

**–ü–æ—á–µ–º—É –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å–ª–æ–∂–Ω–µ–µ –±–∏–Ω–∞—Ä–Ω–æ–π?** –ü–æ—Ç–æ–º—É —á—Ç–æ –Ω—É–∂–Ω–æ —Ä–∞–∑–ª–∏—á–∞—Ç—å –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ:

**–ö–ª—é—á–µ–≤—ã–µ –∞—Å–ø–µ–∫—Ç—ã –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:**
- **–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã**: –ë–æ–ª–µ–µ 2 –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
- **–ò–º–ø–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤**: –ù–µ—Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–æ–≤
- **–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞**: Accuracy, Precision, Recall –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
- **Feature Engineering**: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
- **–í–∞–ª–∏–¥–∞—Ü–∏—è**: –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
- **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å**: –ü–æ–Ω–∏–º–∞–Ω–∏–µ —Ä–µ—à–µ–Ω–∏–π –º–æ–¥–µ–ª–∏

### –ó–∞–¥–∞—á–∞
–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.

### –î–∞–Ω–Ω—ã–µ
```python
def create_image_data(n_samples=5000, n_features=100):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
    
    np.random.seed(42)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    features = np.random.randn(n_samples, n_features)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤—ã—Ö –∫–ª–∞—Å—Å–æ–≤
    n_classes = 5
    classes = ['cat', 'dog', 'bird', 'car', 'tree']
    y = np.random.choice(n_classes, n_samples)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ DataFrame
    feature_names = [f'feature_{i}' for i in range(n_features)]
    data = pd.DataFrame(features, columns=feature_names)
    data['class'] = [classes[i] for i in y]
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    data['image_size'] = np.random.choice(['small', 'medium', 'large'], n_samples)
    data['color_channels'] = np.random.choice([1, 3], n_samples)
    data['resolution'] = np.random.choice(['low', 'medium', 'high'], n_samples)
    
    return data

# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
image_data = create_image_data(5000, 100)
print("Image data shape:", image_data.shape)
print("Class distribution:")
print(image_data['class'].value_counts())
```

### –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
```python
def prepare_image_data(data):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    data['feature_sum'] = data.select_dtypes(include=[np.number]).sum(axis=1)
    data['feature_mean'] = data.select_dtypes(include=[np.number]).mean(axis=1)
    data['feature_std'] = data.select_dtypes(include=[np.number]).std(axis=1)
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    numeric_columns = data.select_dtypes(include=[np.number]).columns
    for col in numeric_columns:
        if col != 'color_channels':
            data[col] = (data[col] - data[col].mean()) / data[col].std()
    
    return data

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
image_processed = prepare_image_data(image_data)
```

### –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
```python
def train_image_model(data):
    """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
    train_data, test_data = train_test_split(data, test_size=0.2, random_state=42, stratify=data['class'])
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞
    predictor = TabularPredictor(
        label='class',
        problem_type='multiclass',
        eval_metric='accuracy',
        path='./image_models'
    )
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    hyperparameters = {
        'GBM': [
            {
                'num_boost_round': 200,
                'learning_rate': 0.1,
                'num_leaves': 31,
                'feature_fraction': 0.9,
                'bagging_fraction': 0.8,
                'min_data_in_leaf': 20
            }
        ],
        'XGB': [
            {
                'n_estimators': 200,
                'learning_rate': 0.1,
                'max_depth': 6,
                'subsample': 0.8,
                'colsample_bytree': 0.8
            }
        ],
        'RF': [
            {
                'n_estimators': 200,
                'max_depth': 15,
                'min_samples_split': 5,
                'min_samples_leaf': 2
            }
        ]
    }
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    predictor.fit(
        train_data,
        hyperparameters=hyperparameters,
        time_limit=1800,  # 30 –º–∏–Ω—É—Ç
        presets='high_quality',
        num_bag_folds=5,
        num_bag_sets=1
    )
    
    return predictor, test_data

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
image_predictor, image_test_data = train_image_model(image_processed)
```

### –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
```python
def evaluate_image_model(predictor, test_data):
    """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    predictions = predictor.predict(test_data)
    probabilities = predictor.predict_proba(test_data)
    
    # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
    performance = predictor.evaluate(test_data)
    
    # –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_importance = predictor.feature_importance()
    
    # –õ–∏–¥–µ—Ä–±–æ—Ä–¥ –º–æ–¥–µ–ª–µ–π
    leaderboard = predictor.leaderboard(test_data)
    
    # –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–ª–∞—Å—Å–∞–º
    from sklearn.metrics import classification_report, confusion_matrix
    
    class_report = classification_report(test_data['class'], predictions, output_dict=True)
    conf_matrix = confusion_matrix(test_data['class'], predictions)
    
    return {
        'performance': performance,
        'feature_importance': feature_importance,
        'leaderboard': leaderboard,
        'predictions': predictions,
        'probabilities': probabilities,
        'classification_report': class_report,
        'confusion_matrix': conf_matrix
    }

# –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
image_results = evaluate_image_model(image_predictor, image_test_data)

print("Image Model Performance:")
for metric, value in image_results['performance'].items():
    print(f"{metric}: {value:.4f}")

print("\nClassification Report:")
for class_name, metrics in image_results['classification_report'].items():
    if isinstance(metrics, dict):
        print(f"{class_name}: {metrics}")
```

### –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
```python
def visualize_image_results(results, test_data):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
    import seaborn as sns
    sns.heatmap(results['confusion_matrix'], annot=True, fmt='d', cmap='Blues', ax=axes[0, 0])
    axes[0, 0].set_title('Confusion Matrix')
    axes[0, 0].set_xlabel('Predicted')
    axes[0, 0].set_ylabel('Actual')
    
    # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    results['feature_importance'].head(15).plot(kind='barh', ax=axes[0, 1])
    axes[0, 1].set_title('Top 15 Feature Importance')
    
    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    prediction_counts = pd.Series(results['predictions']).value_counts()
    prediction_counts.plot(kind='bar', ax=axes[1, 0])
    axes[1, 0].set_title('Distribution of Predictions')
    axes[1, 0].set_xlabel('Class')
    axes[1, 0].set_ylabel('Count')
    axes[1, 0].tick_params(axis='x', rotation=45)
    
    # –¢–æ—á–Ω–æ—Å—Ç—å –ø–æ –∫–ª–∞—Å—Å–∞–º
    class_accuracy = []
    for class_name in test_data['class'].unique():
        class_data = test_data[test_data['class'] == class_name]
        class_predictions = results['predictions'][test_data['class'] == class_name]
        accuracy = (class_data['class'] == class_predictions).mean()
        class_accuracy.append(accuracy)
    
    axes[1, 1].bar(test_data['class'].unique(), class_accuracy)
    axes[1, 1].set_title('Accuracy by Class')
    axes[1, 1].set_xlabel('Class')
    axes[1, 1].set_ylabel('Accuracy')
    axes[1, 1].tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    plt.show()

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
visualize_image_results(image_results, image_test_data)
```

## –ü—Ä–∏–º–µ—Ä 5: –ü—Ä–æ–¥–∞–∫—à–µ–Ω —Å–∏—Å—Ç–µ–º–∞

### –ü–æ–ª–Ω–∞—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω —Å–∏—Å—Ç–µ–º–∞
```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import pandas as pd
import numpy as np
from autogluon.tabular import TabularPredictor
import logging
from datetime import datetime
from typing import Dict, List, Any
import asyncio
import aiohttp

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –°–æ–∑–¥–∞–Ω–∏–µ FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
app = FastAPI(title="AutoML Gluon Production API", version="1.0.0")

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
models = {}
model_metadata = {}

class PredictionRequest(BaseModel):
    model_name: str
    data: List[Dict[str, Any]]

class PredictionResponse(BaseModel):
    predictions: List[Any]
    probabilities: List[Dict[str, float]] = None
    model_info: Dict[str, Any]
    timestamp: str

class ModelInfo(BaseModel):
    model_name: str
    model_type: str
    performance: Dict[str, float]
    features: List[str]
    created_at: str

@app.on_event("startup")
async def load_models():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ"""
    global models, model_metadata
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–Ω–∫–æ–≤—Å–∫–æ–π –º–æ–¥–µ–ª–∏
    try:
        models['bank_default'] = TabularPredictor.load('./bank_models')
        model_metadata['bank_default'] = {
            'model_type': 'binary_classification',
            'target': 'default_risk',
            'features': ['age', 'income', 'credit_score', 'debt_ratio', 'employment_years']
        }
        logger.info("Bank model loaded successfully")
    except Exception as e:
        logger.error(f"Failed to load bank model: {e}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏
    try:
        models['real_estate'] = TabularPredictor.load('./real_estate_models')
        model_metadata['real_estate'] = {
            'model_type': 'regression',
            'target': 'price',
            'features': ['area', 'bedrooms', 'bathrooms', 'age', 'location']
        }
        logger.info("Real estate model loaded successfully")
    except Exception as e:
        logger.error(f"Failed to load real estate model: {e}")

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    loaded_models = list(models.keys())
    return {
        "status": "healthy" if loaded_models else "unhealthy",
        "loaded_models": loaded_models,
        "timestamp": datetime.now().isoformat()
    }

@app.post("/predict", response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    """Endpoint –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"""
    
    if request.model_name not in models:
        raise HTTPException(status_code=404, detail=f"Model {request.model_name} not found")
    
    try:
        model = models[request.model_name]
        metadata = model_metadata[request.model_name]
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        df = pd.DataFrame(request.data)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        predictions = model.predict(df)
        
        # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)
        probabilities = None
        if hasattr(model, 'predict_proba'):
            proba = model.predict_proba(df)
            probabilities = proba.to_dict('records')
        
        return PredictionResponse(
            predictions=predictions.tolist(),
            probabilities=probabilities,
            model_info={
                "model_name": request.model_name,
                "model_type": metadata['model_type'],
                "target": metadata['target'],
                "features": metadata['features']
            },
            timestamp=datetime.now().isoformat()
        )
        
    except Exception as e:
        logger.error(f"Prediction error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/models")
async def list_models():
    """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    return {
        "models": list(models.keys()),
        "metadata": model_metadata
    }

@app.get("/models/{model_name}")
async def get_model_info(model_name: str):
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏"""
    if model_name not in models:
        raise HTTPException(status_code=404, detail=f"Model {model_name} not found")
    
    model = models[model_name]
    metadata = model_metadata[model_name]
    
    return {
        "model_name": model_name,
        "model_type": metadata['model_type'],
        "target": metadata['target'],
        "features": metadata['features'],
        "performance": model.evaluate(pd.DataFrame([{f: 0 for f in metadata['features']}]))
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### –ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
```python
import requests
import json

def test_production_api():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–¥–∞–∫—à–µ–Ω API"""
    
    base_url = "http://localhost:8000"
    
    # Health check
    response = requests.get(f"{base_url}/health")
    print("Health check:", response.json())
    
    # –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
    response = requests.get(f"{base_url}/models")
    print("Available models:", response.json())
    
    # –¢–µ—Å—Ç –±–∞–Ω–∫–æ–≤—Å–∫–æ–π –º–æ–¥–µ–ª–∏
    bank_data = {
        "model_name": "bank_default",
        "data": [
            {
                "age": 35,
                "income": 50000,
                "credit_score": 750,
                "debt_ratio": 0.3,
                "employment_years": 5
            }
        ]
    }
    
    response = requests.post(f"{base_url}/predict", json=bank_data)
    print("Bank prediction:", response.json())
    
    # –¢–µ—Å—Ç –º–æ–¥–µ–ª–∏ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏
    real_estate_data = {
        "model_name": "real_estate",
        "data": [
            {
                "area": 120,
                "bedrooms": 3,
                "bathrooms": 2,
                "age": 10,
                "location": "downtown"
            }
        ]
    }
    
    response = requests.post(f"{base_url}/predict", json=real_estate_data)
    print("Real estate prediction:", response.json())

# –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤
if __name__ == "__main__":
    test_production_api()
```

## –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø—Ä–∏–º–µ—Ä—ã

<img src="images/optimized/production_comparison.png" alt="–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø—Ä–∏–º–µ—Ä—ã" style="max-width: 100%; height: auto; display: block; margin: 20px auto;">
*–†–∏—Å—É–Ω–æ–∫ 6: –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è AutoML Gluon*

**–ü–æ—á–µ–º—É –≤–∞–∂–Ω—ã –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø—Ä–∏–º–µ—Ä—ã?** –ü–æ—Ç–æ–º—É —á—Ç–æ –æ–Ω–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, –∫–∞–∫ —Ä–µ—à–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ —Ä–µ–∞–ª—å–Ω—ã–µ –∑–∞–¥–∞—á–∏:

**–¢–∏–ø—ã –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤:**
- **–ê–Ω—Å–∞–º–±–ª–∏ –º–æ–¥–µ–ª–µ–π**: –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
- **Feature Engineering**: –°–æ–∑–¥–∞–Ω–∏–µ —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
- **–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- **–ù–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ**: –†–∞–±–æ—Ç–∞ —Å —Ä–µ–¥–∫–∏–º–∏ –∫–ª–∞—Å—Å–∞–º–∏
- **–ë–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ**: –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ —Ä–∞–∑–º–µ—Ä–æ–º –≤ –≥–∏–≥–∞–±–∞–π—Ç—ã
- **–ü—Ä–æ–¥–∞–∫—à–µ–Ω –¥–µ–ø–ª–æ–π**: –†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö

### üöÄ –ü—Ä–∏–º–µ—Ä: –ê–Ω—Å–∞–º–±–ª—å –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è

**–ü–æ—á–µ–º—É –∞–Ω—Å–∞–º–±–ª–∏ —á–∞—Å—Ç–æ —Ä–∞–±–æ—Ç–∞—é—Ç –ª—É—á—à–µ –æ–¥–∏–Ω–æ—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π?** –ü–æ—Ç–æ–º—É —á—Ç–æ –æ–Ω–∏ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã —Ä–∞–∑–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤:

- **–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –º–æ–¥–µ–ª–µ–π**: –†–∞–∑–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –Ω–∞—Ö–æ–¥—è—Ç —Ä–∞–∑–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
- **–°–Ω–∏–∂–µ–Ω–∏–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è**: –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ —Å–Ω–∏–∂–∞–µ—Ç —Ä–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
- **–ü–æ–≤—ã—à–µ–Ω–∏–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏**: –†–µ–∑—É–ª—å—Ç–∞—Ç –º–µ–Ω–µ–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏
- **–õ—É—á—à–∞—è –æ–±–æ–±—â–∞—é—â–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å**: –ú–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä**: AutoML —Å–∞–º –≤—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à–∏–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏
- **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å**: –ú–æ–∂–Ω–æ –ø–æ–Ω—è—Ç—å –≤–∫–ª–∞–¥ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏

### üéØ –ü—Ä–∏–º–µ—Ä: –†–∞–±–æ—Ç–∞ —Å –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏

**–ü–æ—á–µ–º—É –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ - —á–∞—Å—Ç–∞—è –ø—Ä–æ–±–ª–µ–º–∞?** –ü–æ—Ç–æ–º—É —á—Ç–æ –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ —Ä–µ–¥–∫–∏–µ —Å–æ–±—ã—Ç–∏—è –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è —Ä–µ–¥–∫–æ:

- **–°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏**: SMOTE, undersampling, oversampling
- **–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞**: F1-score, Precision, Recall –≤–º–µ—Å—Ç–æ Accuracy
- **–°—Ç–æ–∏–º–æ—Å—Ç—å –æ—à–∏–±–æ–∫**: –†–∞–∑–Ω–∞—è —Ü–µ–Ω–∞ –∑–∞ —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã –æ—à–∏–±–æ–∫
- **–í–∞–ª–∏–¥–∞—Ü–∏—è**: –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
- **–ê–Ω—Å–∞–º–±–ª–∏**: –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
- **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥**: –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ —Ä–µ–¥–∫–∏—Ö –∫–ª–∞—Å—Å–∞—Ö

## –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏

–ü–æ—Å–ª–µ –∏–∑—É—á–µ–Ω–∏—è –ø—Ä–∏–º–µ—Ä–æ–≤ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç–µ –∫:
- [Troubleshooting](./10_troubleshooting.md)
- [–õ—É—á—à–∏–º –ø—Ä–∞–∫—Ç–∏–∫–∞–º](./08_best_practices.md)
