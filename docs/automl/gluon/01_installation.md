# installation AutoML Gluon

**Author:** NeoZorK (Shcherbyna Rostyslav)
**Date:** 2025
**Location:** Ukraine, Zaporizhzhya
**Version:** 1.0

## Why Proper installation is Critical

**Why is it that 70% of AutoML Gluon issues are related to improper installation?** BecaUse machine learning requires precise environment Settings. Incorrect installation can lead to unstable operation, errors and loss of time.

### üö® Real Consequences of Incorrect installation

**Case 1: NumPy Version Conflict **
```python
# What happens when there is a version conflict
import numpy as np
# Error: "numpy.core.multiarray failed to import"
# Result: AutoML Gluon not Launching
```

**Case 2: Issues with CUDA**
```python
# What happens without the right CUDA
import torch
print(torch.cuda.is_available()) # False
# Result: Learning 100x slower
```

**Case 3: Out of memory**
```python
# What happens when there is a shortage of RAM
import pandas as pd
df = pd.read_csv('large_dataset.csv') # MemoryError
# Result: Impossible to work with big data
```

### What Happens with Incorrect installation?
- ** Dependency conflicts **: Different versions of libraries caUse errors
- *example*: NumPy 1.19 vs 1.21 - different APIs, code breaks
- *Solution*: Use virtual environments
- **Issues with performance**: Models Working Slowly or Not Working at all
- *example*: Training 1 hour instead of 5 minutes
- *Reason*: Suboptimal versions of libraries
- ** Compilation errors **: Some algorithms cannot be compiled
- *example*: XGBoost is not compiled on older systems
- *Solution*: Update compiler and dependencies
- **Issues with GPU**: CUDA not Working, training is only on CPU
- *example*: Training 10 hours instead of 1 hour
- *Solution*: Correct installation of CUDA and cuDNN

### What does the right installation do?
- **Stable Working**: all components work without errors
- *Result*: 99.9% failure-free time
- *Save*: Don't waste time on debugging
- **Optimal performance**: Maximum learning speed
- *Result*: Learning 10-100 times faster
- *savings*: Hours instead of days
- **Ease of Use**: all functions are available out of the box
- *Result*: You can start ML projects right away
- *Savings*: Don't learn the setup
- **Easy to update**: Easy to update to new versions
- *Result*: Always up-to-date opportunities
- *Savings*: You don't have to reinstall everything

system requirements

<img src="images/optimized/installation_flowchart.png" alt="AutoML Gluon installation" style="max-width: 100%; height: auto; display: block; margin: 20px auto;">
*Figure 1: AutoML Gluon installation Flowchart *

### AutoML Gluon üèóÔ∏è Architecture

<img src="images/optimized/architecture_diagram.png" alt="–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ AutoML Gluon" style="max-width: 100%; height: auto; display: block; margin: 20px auto;">
*Figure 2: AutoML Gluon Architectural Diagram *

**Why is it important to understand architecture?** BecaUse it helps to understand how AutoML Gluon works inside and why it is so effective:

- **TabularPredictor**: The main component for Working with tabular data
- **TimeSeriesPredictor**: Specialized component for time series
- **ImagePredictor**: Component for Working with images
- **TextPredictor**: A component for word processing
- **Ensemble Methods**: Methods of combining models for improving accuracy
- **Feature Engineering**: Automatically create new features
- **Hyperparameter Tuning**: Automatic configuration of model parameters

Minimum requirements
**Why are minimum requirements important?** BecaUse they determine if you can run AutoML Gluon at all:

- **Python**: 3.7, 3.8, 3.9, 3.10, 3.11
- *Why these versions?* BecaUse AutoML Gluon Uses modern Python capabilities
- *What happens with Python 3.6?* Compilation errors, library incompatibilities
- *What's going on with Python 3.12?* Some dependencies are not yet supported
- *Recommendation*: Use Python 3.9 or 3.10 for stability
- **OS**: Linux, macOS, Windows
- *Why are all OS supported?* BecaUse ML development is carried out on different platforms
- *Linux*: Better performance, more features
- *macOS*: Ease of development, good performance
- *Windows*: Easy to Use but possible Issues with some libraries
- **RAM**: 4GB (8GB+ recommended)
- *Why do you need a lot of memory?* BecaUse ML models load large datasets in memory
- *What happens to 2GB RAM?* system freezes, training is interrupted
- *What happens to 16GB+ RAM?* You can process datasets in 10 times more
- *Practical example*: 1GB dataset requires 4GB RAM for processing
- **CPU**: 2 cores (4+ cores recommended)
- *Why are kernels important?* BecaUse AutoML Gluon Uses parallel computing
- *What happens to 1 core?* Training is 4 times slower
- *What happens to 8+ cores?* Training 4-8 times faster
- *Practical example*: Training 1 hour on 2 cores = 15 minutes on 8 cores
- **Disk**: 2GB free space
- *Why do we need space?* BecaUse models and data take up a lot of space
- *What takes up space?* Models (500MB-2GB), cache (1-5GB), data (depends on size)
- *Practical example*: A project with 10 models takes 5-10GB

Compare Performance

<img src="images/optimized/performance_comparison.png" alt="–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏" style="max-width: 100%; height: auto; display: block; margin: 20px auto;">
*Figure 3: Comparison of AutoML Gluon performance on different configurations*

**Why is it important to understand performance?** BecaUse it helps to choose the optimal configuration for your tasks:

- **CPU vs GPU**: GPU speeds up learning in 10-100 times for neural networks
- **Memory**: More RAM = ability to handle large datasets
- **Cores**: More cores = parallel training of several models
- ** Training time **: from 10 minutes to several hours in dependencies from configuration

### Model Quality üéØ Metrics

<img src="images/optimized/metrics_comparison.png" alt="–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫" style="max-width: 100%; height: auto; display: block; margin: 20px auto;">
*Figure 4: Comparison of different model quality metrics *

**Why is it important to understand metrics?** BecaUse different tasks require different metrics for quality assessment:

- **Accuracy**: Percentage of correct predictions (for balanced data)
- **Precision**: Accuracy of positive predictions (important at high cost of errors)
- **Recall**: Completeness of positive predictions (important not to miss important cases)
- **F1-Score**: Harmonic mean of precision and recall (balanced metric)
- **AUC-ROC**: Area under the ROC curve (quality of class separation)
- **RMSE**: Root of RMSE (for regression)

### Recommended requirements
**Why do the recommended requirements provide the best experience?** BecaUse they provide optimal performance:

- **Python**: 3.9 or 3.10
- *Why these versions?* BecaUse they are the most stable and fast
- *Benefits*: Better performance, stability, compatibility
- *Practical example*: Learning Python 3.10 on 15% faster than on 3.8
- **RAM**: 16GB+
- *Why a lot of memory?* BecaUse large datasets require a lot of RAM
- *What can I do with 16GB?* Process datasets up to 10GB, train complex models
- *What can I do with 32GB+?* Process datasets up to 50GB, train model ensembles
- *Practical example*: 5GB dataset requires 20GB RAM for comfortable operation
- **CPU**: 8+ cores
- *Why so many cores?* BecaUse AutoML Gluon Uses all available cores
- *What happens to the 8 cores?* Training 4-8 times faster than with 2 cores
- *What happens to 16+ cores?* Training is 8-16 times faster
- *Practical example*: Training 1 hour on 2 cores = 7 minutes on 16 cores
- **GPU**: NVIDIA GPU with CUDA support (optional)
- *Why is the GPU important?* BecaUse it speeds up learning in 10-100 times
- *Minimum GPU requirements *: GTX 1060 6GB or better
- *Recommended GPUs*: RTX 3070, RTX 4080, A100 for professional operation
- *Practical example*: Training 10 hours on CPU = 1 hour on RTX 3070
- **Disk**: 10GB+ free space
- *Why so much space?* BecaUse models and cache take up so much space
- *SSD vs HDD*: SSD in 5-10 times faster for data Loading
- *Practical example*: A project with 50 models takes 20-50GB

## AutoML Gluon üîÑ Workflows

<img src="images/optimized/retraining_workflow.png" alt="–†–∞–±–æ—á–∏–π –ø—Ä–æ—Ü–µ—Å—Å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è" style="max-width: 100%; height: auto; display: block; margin: 20px auto;">
*Figure 5: Model retraining workflow diagram *

**Why is it important to understand workflows?** BecaUse it helps to understand how AutoML Gluon automates the entire machine learning process:

- ** data preparation **: Automatic clean and preprocessing
- **Feature Engineering**: create new features from existing ones
- **Selection of algorithms**: Automatic selection of the best algorithms for the problem
- **training models**: parallel training of multiple models
- **Validation**: Automatic model quality assessment
- **Ensemble**: Combining the best models for improving accuracy
- **Deploy**: Ready-made models for production

## installation via pip

**Why is pip the most popular installation method?** BecaUse it is simple, reliable and automatically solves dependencies.

## üöÄ installation via uv (Recommended)

**Why is uv better than pip?** BecaUse uv is 10-100 times faster, more reliable, and better at managing addictions.

### What is uv?
**uv** is a modern Python package manager written on Rust. It solves all pip problems:

- **Speed**: in 10-100 times faster than pip
- **Reliability**: Better resolves dependency conflicts
- **Security**: checks package integrity
- **Compatibility**: Full compatibility with pip

### installation uv
```bash
# installation uv via pip (if you already have Python)
pip install uv

# or via curl (recommended)
curl -LsSf https://astral.sh/uv/install.sh | sh

# or via homebrew on macOS
brew install uv
```

**What happens when uv is installed?**
- DownLoading binary file uv (5-10MB)
- installed in system PATH
- Configuration file is created
- Configures the package cache

### AutoML Gluon installation via uv
```bash
# Basic installation
uv add autogluon

# installation with additional components
uv add autogluon.tabular
uv add autogluon.timeseries
uv add autogluon.vision

# installation in virtual environment
uv venv
uv pip install autogluon
```

**Advantages of uv over pip:**
- **Speed**: installation in 10 times faster
- **Reliability**: Fewer dependency conflicts
- **Caching**: Smart Packet Caching
- **Parallelism**: installation of multiple packages simultaneously

### üöÄ Basic installation
**Why start with a basic setup?** BecaUse it gives you everything you need to get started:

```bash
pip install autogluon
```

**What happens with this team?**
- main AutoML Gluon package is installed
- all necessary dependencies are automatically set
- An environment for Working with tabular data is created
- Basic configuration is configured

**Detailed installation process:**
```python
# What happens inside pip install autogluon
# 1 - Package Download (50-100MB)
# 2. installation of dependencies:
# - numpy, pandas, scikit-learn
# - xgboost, lightgbm, catboost
# - torch, torchvision
# - matplotlib, seaborn
# 3. check version compatibility
# 4. create configuration files
# 5. Unit testing
```

Set-up time
- Fast internet: 5-10 minutes
- Slow internet: 30-60 minutes
- First installation: Longer due to compilation
- Subsequent updates: Faster

### üéØ installation with additional dependencies
**Why do I need additional components?** BecaUse different tasks require different tools:

#### üìä for Working with tabular data
```bash
pip install autogluon.tabular
```

**What is autogluon.tabular?**
- Optimized algorithms for tabular data
- Automatic processing of categorical variables
- Built-in validation and metrics
- Support for large datasets

**Detailed Opportunities:**
```python
# What autogluon.tabular includes
from autogluon.tabular import TabularPredictor

algos
# - XGBoost, LightGBM, CatBoost
# - Random Forest, Extra Trees
# - Neural networks
# - Linear Models
# - Ensemble Methods

# Automatic Capabilities:
# - Feature Engineering
# - Hyperparameter Tuning
# - Model Selection
# - Cross-Validation
```

WHEN TO Use IT
- Classification and regression
- Tabular data (CSV, Excel, SQL)
- Structured data
Business <ph type="Structure-only" x="0"/>Analytics

#### ·êà for Working with time series
```bash
pip install autogluon.timeseries
```

**What is autogluon.timeseries?**
- Special algorithms for time series
- Automatic determination of seasonality
- Multidimensional time series support
- Built-in Prediction

**Detailed Opportunities:**
```python
# What autogluon.timeseries includes
from autogluon.timeseries import TimeSeriesPredictor

algos
# - ARIMA, SARIMA
# - Prophet, ETS
# - Deep Learning (LSTM, Transformer)
# - Ensemble Methods

# Automatic Capabilities:
# - Seasonality Detection
# - Trend Analysis
# - Anomaly Detection
# - Multi-step Forecasting
```

WHEN TO Use IT
Sales forecasting
time series analysi
- Financial data
- IoT data

#### üñºÔ∏è for Working with images
```bash
pip install autogluon.vision
```

**What is autogluon.vision?**
- Ready-made CNN architectures
- Automatic data enlargement
- Prebuilt models
- GPU acceleration support

```bash
# for Working with text
pip install autogluon.text
```
**What is autogluon.text?**
- Modern NLP models
- Automatic tokenization
- Pre-purchased embeddings
- Support for Transformers

```bash
# Complete installation of all components
pip install autogluon[all]
```
**Why is full installation convenient?** BecaUse you get all the opportunities at once, but it takes more space and time.

## installation via conda

### create new environment
```bash
# create environments with Python 3.9
conda create -n autogluon python=3.9
conda activate autogluon

# installation AutoGluon
conda install -c conda-forge autogluon
```

### installation with GPU support
```bash
# create environments with CUDA
conda create -n autogluon-gpu python=3.9
conda activate autogluon-gpu

# installation PyTorch with CUDA
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia

# installation AutoGluon
pip install autogluon
```

## installation from source code

### Repository Cloning
```bash
git clone https://github.com/autogluon/autogluon.git
cd autogluon
```

### installation in development mode
```bash
# installation of dependencies
pip install -e .

# or for a specific module
pip install -e ./tabular
```

## Validation and testing üìã Methods

<img src="images/optimized/validation_methods.png" alt="–ú–µ—Ç–æ–¥—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏" style="max-width: 100%; height: auto; display: block; margin: 20px auto;">
*Figure 6: Various model validation methods *

**Why is validation important?** BecaUse it ensures the reliability and quality of models:

- **Holdout Validation**: Simple separation on train/test (70/30)
- **Cross-Validation**: K-fold cross-validation for more reliable evaluation
- **Time Series Split**: Special validation for time series
- **Stratified Split**: Saving class proportions when splitting
- **Walk-Forward Analysis**: Sliding window for time series

### Troubleshooting üîß Diagram

<img src="images/optimized/Troubleshooting_flowchart.png" alt="–î–∏–∞–≥—Ä–∞–º–º–∞ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º" style="max-width: 100%; height: auto; display: block; margin: 20px auto;">
*Figure 7: installation Troubleshooting step-by-step diagram *

**Why do I need a Troubleshooting chart?** BecaUse it helps solve 90% of problems quickly:

- **Issues with dependencies**: Library version conflicts
- **Issues with memory**: Lack of RAM for large datasets
- **Issues with GPU**: Incorrect configuration CUDA
- **Issues with performance**: Suboptimal Settings

## installation checks

Baseline test
```python
import autogluon as ag
print(f"AutoGluon Version: {ag.__version__}")

# Core Module import Test
from autogluon.tabular import TabularPredictor
from autogluon.timeseries import TimeSeriesPredictor
from autogluon.vision import ImagePredictor
from autogluon.text import TextPredictor

print("all modules imported successfully!")
```

### Test with a simple example
```python
from autogluon.tabular import TabularPredictor
import pandas as pd
import numpy as np

# create test data
data = pd.dataFrame({
 'feature1': np.random.randn(100),
 'feature2': np.random.randn(100),
 'target': np.random.randint(0, 2, 100)
})

# Training Test
predictor = TabularPredictor(label='target')
predictor.fit(data, time_limit=10) # 10 seconds for a quick test
print("installation test passed!")
```

## installation of additional dependencies

### for Working with GPUs
```bash
# installation CUDA toolkit (Ubuntu/Debian)
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin
sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600
wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda-repository-ubuntu2004-11-8-local_11.8.0-520.61.05-1_amd64.deb
sudo dpkg -i cuda-repository-ubuntu2004-11-8-local_11.8.0-520.61.05-1_amd64.deb
sudo apt-key add /var/cuda-repository-ubuntu2004-11-8-local/7fa2af80.pub
sudo apt-get update
sudo apt-get -y install cuda

# installation PyTorch with CUDA
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### for Working with large datasets
```bash
# installation of additional libraries for big data processing
pip install dask[complete]
pip install ray[default]
pip install modin[all]
```

#### üìä Detailed describe of libraries for large datasets

**Dask - Distributed Computing for Big data**

Intended purpose
- Parallel processing of data that is not stored in memory
- Distributed computing across multiple cores/nodes
- integration with pandas, numpy, scikit-learn

ADVANTAGES
- **Scalability**: data processing in 10-100 times more available memory
- **Compatibility**: API similar to on pandas/numpy, easy to migrate code
- **Flexibility**: Works on a single computer or cluster
- **integration**: Integrates well with AutoML Gluon
- **Failover**: Automatic disaster recovery

Deficiencies
- **Complexity of Settings**: Requires understanding of distributed systems
- **Overhead**: for small data may be slower than pandas
- **Debugging * *: It's harder to debug distributed code
- **dependencies**: Many additional packages

**Practical examples of Use:**
```python
# Processing large CSV files
import dask.dataframe as dd

# 50GB file upload (not fit in RAM)
df = dd.read_csv('huge_dataset.csv') # Loaded on parts

# Operations are performed lazily
result = df.groupby('category').sum().compute() # executed only when compute()

# integration with AutoML Gluon
from autogluon.tabular import TabularPredictor
predictor = TabularPredictor(label='target')
predictor.fit(df, time_limit=3600) # Works with Dask dataFrame
```

**Ray - Distributed Framework for ML**

Intended purpose
- Distributed machine learning
- Parallel task processing
- Management of resources in the cluster

ADVANTAGES
- **Performance**: Very fast distributed computing
- **ML optimization **: Specially created for machine learning
- **Automatic scaling**: Automatically Uses available resources
- **Fault tolerance**: Built-in error handling
- **Flexibility**: Supports any Python functions

Deficiencies
- **Difficulty**: Harder to learn than Dask
- **Resources**: Requires more memory for coordination
- **Debugging * *: It's harder to debug distributed tasks
- **dependencies**: Many system dependencies

**Practical examples of Use:**
```python
import ray
from autogluon.tabular import TabularPredictor

# Ray Initialization
ray.init()

# Distributed model training
@ray.remote
def train_model(data_chunk):
 predictor = TabularPredictor(label='target')
 predictor.fit(data_chunk, time_limit=1800)
 return predictor

# parallel training on different parts of the data
futures = [train_model.remote(chunk) for chunk in data_chunks]
models = ray.get(futures)

# Model Ensemble
ensemble_predictions = []
for model in models:
 pred = model.predict(test_data)
 ensemble_predictions.append(pred)
```

**Modin - Accelerated Pandas**

Intended purpose
- acceleration of pandas operations by 2-10 times
- Automatic Use of all available cores
- Transparent pandas replacement

ADVANTAGES
- **Simplicity**: Direct replacement of pandas, minimal code changes
- **Speed**: Automatic acceleration of pandas operations
- **Compatibility**: Fully compatible with pandas API
- **Performance**: Uses all available cores
- **integration**: Easily integrates with existing code

Deficiencies
- **Limited functionality**: not all pandas functions are supported
- **Memory**: Can Use more memory than pandas
- **Stability**: Less stable than original pandas
- **dependencies**: Requires Ray or Dask as backend

**Practical examples of Use:**
```python
# Easy replacement of pandas on modin
import modin.pandas as pd # Instead of import pandas as pd

# all operations are automatically accelerated
df = pd.read_csv('large_dataset.csv') # 2-5 times faster
result = df.groupby('category').sum() # in 3-8 times faster

# integration with AutoML Gluon
from autogluon.tabular import TabularPredictor
predictor = TabularPredictor(label='target')
predictor.fit(df, time_limit=3600) # Works with Modin dataFrame
```

**Comparison of Libraries for Big data:**

| Library | data Size | Difficulty | Speed | Stability |
|------------|---------------|-----------|----------|--------------|
| **Dask** | 10GB - 1TB+ | Medium | High | High |
| **Ray** | 1GB - 100GB+ | High | Very High | Medium |
| **Modin** | 100MB - 10GB | Low | Medium | Medium |

**Recommendations for choosing:**

**Use Dask if:**
- data more available memory
- Maximum compatibility with pandas is required
- Working with a cluster
- Fault tolerance is required

**Use Ray if:**
- Maximum performance is required
- Working with ML tasks
- Experience with distributed systems
- Automatic scaling is required

**Use Modin if:**
- data are placed in memory
- Minimal code change required
- Working on one computer
- Need rapid prototyping

### for Working with time series
```bash
# Special libraries for time series
pip install gluonts
pip install mxnet
pip install statsmodels
```

#### # Detailed describe of libraries for time series

**GluonTS - Specialized Library for Time Series**

Intended purpose
- Deep learning for time series Prediction
- Ready-made models for various types of time series
- integration with MXNet and PyTorch
- Automatic detection of seasonality and trends

 Facilities
- **Finished models**: DeepAR, Transformer, WaveNet, MQ-CNN
- **Automatic processing**: Determination of seasonality, trends, anomalies
- **Multidimensional series**: Working with multiple linked time series
- **Uncertainty**: Quantile predictions and confidence intervals
- **Scalability**: Processing thousands of time series simultaneously

**Practical examples of Use:**
```python
import gluonts
from gluonts.dataset import common
from gluonts.model.deepar import DeepAREstimator
from gluonts.trainer import Trainer

# create dataset for time series
dataset = common.Listdataset(
 data_iter=[{"start": "2020-01-01", "target": [1, 2, 3, 4, 5]}],
 freq="D"
)

# DeepAR Model Training
estimator = DeepAREstimator(
 freq="D",
 Prediction_length=7,
 trainer=Trainer(epochs=10)
)

# Learning and Forecasting
predictor = estimator.train(dataset)
forecast = predictor.predict(dataset)

# integration with AutoML Gluon
from autogluon.timeseries import TimeSeriesPredictor
predictor = TimeSeriesPredictor(
 target="sales",
 Prediction_length=24,
 freq="H"
)
predictor.fit(train_data, time_limit=3600)
```

**MXNet - Deep Learning for Time Series**

Intended purpose
- Flexible framework for deep learning
- Optimization for time series
- GPU and distributed computing support
- integration with GluonTS

 Facilities
- **Flexible architecture**: create custom models for time series
- **GPU acceleration**: Quick learning on GPU
- **Distribution**: Training on the cluster
- **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
- **integration**: –•–æ—Ä–æ—à–æ Working–µ—Ç with GluonTS

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ examples –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:**
```python
import mxnet as mx
from mxnet import gluon, autograd
import numpy as np

# create LSTM –º–æ–¥–µ–ª–∏ for –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
class LSTMPredictor(gluon.Block):
 def __init__(self, hidden_size, output_size):
 super(LSTMPredictor, self).__init__()
 self.lstm = gluon.rnn.LSTM(hidden_size)
 self.dense = gluon.nn.Dense(output_size)

 def forward(self, x):
 output = self.lstm(x)
 return self.dense(output[-1])

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
model = LSTMPredictor(hidden_size=50, output_size=1)
model.initialize()

# integration with AutoML Gluon
from autogluon.timeseries import TimeSeriesPredictor
predictor = TimeSeriesPredictor(
 target="value",
 Prediction_length=12,
 freq="M"
)
predictor.fit(train_data, time_limit=1800)
```

**Statsmodels - –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏ for –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤**

**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:**
- –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏
- –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
- –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ—Å—Ç–∏
- –°–µ–∑–æ–Ω–Ω–∞—è –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è

**–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:**
- **ARIMA/SARIMA**: –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–∏
- **ETS**: Exponential Smoothing –º–æ–¥–µ–ª–∏
- **–°–µ–∑–æ–Ω–Ω–∞—è –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è**: STL, X-13ARIMA-SEATS
- **–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ**: ADF, KPSS —Ç–µ—Å—Ç—ã —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ—Å—Ç–∏
- **–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞**: ACF, PACF, Ljung-Box —Ç–µ—Å—Ç—ã

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ examples –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:**
```python
import statsmodels.api as sm
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller

# –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ—Å—Ç–∏
def check_stationarity(timeseries):
 result = adfuller(timeseries)
 print(f'ADF Statistic: {result[0]}')
 print(f'p-value: {result[1]}')
 return result[1] < 0.05

# –°–µ–∑–æ–Ω–Ω–∞—è –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è
decomposition = seasonal_decompose(timeseries, model='additive')
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid

# ARIMA –º–æ–¥–µ–ª—å
model = ARIMA(timeseries, order=(1,1,1))
fitted_model = model.fit()
forecast = fitted_model.forecast(steps=12)

# integration with AutoML Gluon
from autogluon.timeseries import TimeSeriesPredictor
predictor = TimeSeriesPredictor(
 target="price",
 Prediction_length=30,
 freq="D"
)
predictor.fit(train_data, time_limit=3600)
```

**–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫ for –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤:**

| –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ | –¢–∏–ø –º–æ–¥–µ–ª–µ–π | –°–ª–æ–∂–Ω–æ—Å—Ç—å | –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å | –¢–æ—á–Ω–æ—Å—Ç—å |
|------------|-------------|-----------|-------------------|----------|
| **GluonTS** | Deep Learning | –í—ã—Å–æ–∫–∞—è | –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è | –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è |
| **MXNet** | Custom Deep Learning | –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è | –í—ã—Å–æ–∫–∞—è | –í—ã—Å–æ–∫–∞—è |
| **Statsmodels** | Statistical | –ù–∏–∑–∫–∞—è | –°—Ä–µ–¥–Ω—è—è | –°—Ä–µ–¥–Ω—è—è |

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ on –≤—ã–±–æ—Ä—É:**

**Use GluonTS –µ—Å–ª–∏:**
- –ù—É–∂–Ω—ã —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ deep learning –º–æ–¥–µ–ª–∏
- Working–µ—Ç–µ with –±–æ–ª—å—à–∏–º–∏ –æ–±—ä–µ–º–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö
- –ù—É–∂–Ω—ã –∫–≤–∞–Ω—Ç–∏–ª—å–Ω—ã–µ –ø—Ä–æ–≥–Ω–æ–∑—ã
- –¢—Ä–µ–±—É–µ—Ç—Å—è –≤—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å

**Use MXNet –µ—Å–ª–∏:**
- –ù—É–∂–Ω—ã –∫–∞—Å—Ç–æ–º–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
- –¢—Ä–µ–±—É–µ—Ç—Å—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–∏–±–∫–æ—Å—Ç—å
- Working–µ—Ç–µ with GPU
- –ù—É–∂–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ

**Use Statsmodels –µ—Å–ª–∏:**
- –ù—É–∂–Ω—ã –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏
- –¢—Ä–µ–±—É–µ—Ç—Å—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å
- Working–µ—Ç–µ with –º–∞–ª—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
- –ù—É–∂–µ–Ω –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑

**integration with AutoML Gluon for –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤:**

```python
from autogluon.timeseries import TimeSeriesPredictor
import pandas as pd

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
train_data = pd.dataFrame({
 'timestamp': pd.date_range('2020-01-01', periods=1000, freq='H'),
 'target': np.random.randn(1000).cumsum(),
 'feature1': np.random.randn(1000),
 'feature2': np.random.randn(1000)
})

# create –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞
predictor = TimeSeriesPredictor(
 target="target",
 Prediction_length=24, # –ü—Ä–æ–≥–Ω–æ–∑ on 24 —á–∞—Å–∞
 freq="H", # –ü–æ—á–∞—Å–æ–≤—ã–µ data
 eval_metric="MAPE"
)

# –û–±—É—á–µ–Ω–∏–µ with —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
predictor.fit(
 train_data,
 time_limit=3600, # 1 —á–∞—Å
 presets="best_quality" # –õ—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ
)

# –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ
predictions = predictor.predict(train_data)
print(f"predictions shape: {predictions.shape}")

# –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
performance = predictor.evaluate(train_data)
print(f"Model performance: {performance}")
```

## configuration –æ–∫—Ä—É–∂–µ–Ω–∏—è

### –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
```bash
# installation –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö for –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
export OMP_NUM_THREADS=4
export MKL_NUM_THREADS=4
export OPENBLAS_NUM_THREADS=4

# for GPU
export CUDA_VISIBLE_DEVICES=0

# for –æ—Ç–ª–∞–¥–∫–∏
export AUTOGLUON_DEBUG=1
```

#### üîß –î–µ—Ç–∞–ª—å–Ω–æ–µ describe –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è

**OMP_NUM_THREADS - –ö–æ–Ω—Ç—Ä–æ–ª—å OpenMP –ø–æ—Ç–æ–∫–æ–≤**

**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:**
- –ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ for OpenMP –æ–ø–µ—Ä–∞—Ü–∏–π
- –í–ª–∏—è–µ—Ç on –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å numpy, scipy, scikit-learn
- –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CPU —è–¥–µ—Ä

**–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:**
- **2-4 —è–¥—Ä–∞**: `OMP_NUM_THREADS=2`
- **4-8 —è–¥–µ—Ä**: `OMP_NUM_THREADS=4`
- **8+ —è–¥–µ—Ä**: `OMP_NUM_THREADS=6-8`

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ examples:**
```bash
# for —Å–∏—Å—Ç–µ–º—ã with 8 —è–¥—Ä–∞–º–∏
export OMP_NUM_THREADS=6 # –û—Å—Ç–∞–≤–ª—è–µ–º 2 —è–¥—Ä–∞ for —Å–∏—Å—Ç–µ–º—ã

# for —Å–∏—Å—Ç–µ–º—ã with 4 —è–¥—Ä–∞–º–∏
export OMP_NUM_THREADS=3 # –û—Å—Ç–∞–≤–ª—è–µ–º 1 —è–¥—Ä–æ for —Å–∏—Å—Ç–µ–º—ã

# for —Å–∏—Å—Ç–µ–º—ã with 16 —è–¥—Ä–∞–º–∏
export OMP_NUM_THREADS=12 # –û—Å—Ç–∞–≤–ª—è–µ–º 4 —è–¥—Ä–∞ for —Å–∏—Å—Ç–µ–º—ã
```

**–í–ª–∏—è–Ω–∏–µ on –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:**
- **–°–ª–∏—à–∫–æ–º –º–∞–ª–æ –ø–æ—Ç–æ–∫–æ–≤**: –ù–µ–¥–æ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CPU
- **–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–æ–≤**: –ö–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è –∑–∞ —Ä–µ—Å—É—Ä—Å—ã, —Å–Ω–∏–∂–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- **–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ**: 70-80% from –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —è–¥–µ—Ä

**check —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏:**
```python
import numpy as np
import time

# –¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ with —Ä–∞–∑–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–æ—Ç–æ–∫–æ–≤
def test_omp_performance():
 # create –±–æ–ª—å—à–æ–π –º–∞—Ç—Ä–∏—Ü—ã
 size = 5000
 a = np.random.randn(size, size)
 b = np.random.randn(size, size)

 # –ò–∑–º–µ—Ä–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ —É–º–Ω–æ–∂–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü
 start_time = time.time()
 result = np.dot(a, b)
 end_time = time.time()

 print(f"Matrix multiplication time: {end_time - start_time:.2f} seconds")
 print(f"OMP_NUM_THREADS: {np.getenv('OMP_NUM_THREADS', 'default')}")

# Launch —Ç–µ—Å—Ç–∞
test_omp_performance()
```

**MKL_NUM_THREADS - –ö–æ–Ω—Ç—Ä–æ–ª—å Intel MKL –ø–æ—Ç–æ–∫–æ–≤**

**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:**
- –ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ for Intel Math Kernel Library
- –í–ª–∏—è–µ—Ç on –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å numpy, scipy, pandas
- –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏

**–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:**
- **–î–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ä–∞–≤–Ω–æ OMP_NUM_THREADS**: `MKL_NUM_THREADS=4`
- **for –∏–∑–±–µ–∂–∞–Ω–∏—è –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤**: not –¥–æ–ª–∂–Ω–æ –ø—Ä–µ–≤—ã—à–∞—Ç—å OMP_NUM_THREADS
- **for –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏**: –†–∞–≤–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö —è–¥–µ—Ä

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ examples:**
```bash
# Synchronization with OMP_NUM_THREADS
export OMP_NUM_THREADS=4
export MKL_NUM_THREADS=4

# for —Å–∏—Å—Ç–µ–º with Intel CPU
export MKL_NUM_THREADS=4 # Use 4 —è–¥—Ä–∞

# for —Å–∏—Å—Ç–µ–º with AMD CPU
export MKL_NUM_THREADS=2 # –ú–µ–Ω—å—à–µ –ø–æ—Ç–æ–∫–æ–≤ for AMD
```

**–í–ª–∏—è–Ω–∏–µ on –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:**
- **Synchronization with OMP**: –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–µ–≥—Ä—É–∑–∫—É —Å–∏—Å—Ç–µ–º—ã
- **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è MKL**: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
- **–ò–∑–±–µ–∂–∞–Ω–∏–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤**: –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—é –∑–∞ —Ä–µ—Å—É—Ä—Å—ã

**check Settings:**
```python
import numpy as np

# check —Ç–µ–∫—É—â–∏—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫
print(f"OMP_NUM_THREADS: {np.getenv('OMP_NUM_THREADS', 'not set')}")
print(f"MKL_NUM_THREADS: {np.getenv('MKL_NUM_THREADS', 'not set')}")

# –¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
def test_mkl_performance():
 # create –±–æ–ª—å—à–∏—Ö –º–∞—Å—Å–∏–≤–æ–≤
 a = np.random.randn(3000, 3000)
 b = np.random.randn(3000, 3000)

 # –¢–µ—Å—Ç —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
 start = time.time()
 result1 = np.dot(a, b) # –ú–∞—Ç—Ä–∏—á–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ
 time1 = time.time() - start

 start = time.time()
 result2 = np.linalg.svd(a) # SVD —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ
 time2 = time.time() - start

 print(f"Matrix multiplication: {time1:.2f}s")
 print(f"SVD decomposition: {time2:.2f}s")

test_mkl_performance()
```

**OPENBLAS_NUM_THREADS - –ö–æ–Ω—Ç—Ä–æ–ª—å OpenBLAS –ø–æ—Ç–æ–∫–æ–≤**

**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:**
- –ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ for OpenBLAS –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
- –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ Intel MKL for —Å–∏—Å—Ç–µ–º –±–µ–∑ Intel CPU
- –í–ª–∏—è–µ—Ç on –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ª–∏–Ω–µ–π–Ω–æ–π –∞–ª–≥–µ–±—Ä—ã

**–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:**
- **for —Å–∏—Å—Ç–µ–º with Intel MKL**: not –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è (MKL –∏–º–µ–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
- **for —Å–∏—Å—Ç–µ–º –±–µ–∑ MKL**: `OPENBLAS_NUM_THREADS=4`
- **for AMD —Å–∏—Å—Ç–µ–º**: `OPENBLAS_NUM_THREADS=2-4`

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ examples:**
```bash
# for —Å–∏—Å—Ç–µ–º with Intel CPU (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è MKL)
export MKL_NUM_THREADS=4
# OPENBLAS_NUM_THREADS not –Ω—É–∂–µ–Ω

# for —Å–∏—Å—Ç–µ–º with AMD CPU (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è OpenBLAS)
export OPENBLAS_NUM_THREADS=4
export OMP_NUM_THREADS=4

# for —Å–∏—Å—Ç–µ–º –±–µ–∑ MKL
export OPENBLAS_NUM_THREADS=4
export OMP_NUM_THREADS=4
```

**check Use–æ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–∏:**
```python
import numpy as np

# check –∫–∞–∫–æ–π BLAS –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
print(f"NumPy BLAS info: {np.__config__.blas_opt_info}")
print(f"NumPy LAPACK info: {np.__config__.lapack_opt_info}")

# –¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
def test_blas_performance():
 # create –±–æ–ª—å—à–∏—Ö –º–∞—Ç—Ä–∏—Ü
 size = 2000
 a = np.random.randn(size, size)
 b = np.random.randn(size, size)

 # –¢–µ—Å—Ç –º–∞—Ç—Ä–∏—á–Ω–æ–≥–æ —É–º–Ω–æ–∂–µ–Ω–∏—è
 start = time.time()
 result = np.dot(a, b)
 end = time.time()

 print(f"Matrix multiplication time: {end - start:.2f} seconds")
 print(f"BLAS library: {np.__config__.blas_opt_info.get('libraries', ['unknown'])[0]}")

test_blas_performance()
```

**CUDA_VISIBLE_DEVICES - –ö–æ–Ω—Ç—Ä–æ–ª—å GPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤**

**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:**
- –£–∫–∞–∑—ã–≤–∞–µ—Ç What GPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
- –ü–æ–∑–≤–æ–ª—è–µ—Ç –≤—ã–±–∏—Ä–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ GPU
- –ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç –¥–æ—Å—Ç—É–ø –∫ GPU —Ä–µ—Å—É—Ä—Å–∞–º

**–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:**
- **–û–¥–Ω–∞ GPU**: `CUDA_VISIBLE_DEVICES=0`
- **–ù–µ—Å–∫–æ–ª—å–∫–æ GPU**: `CUDA_VISIBLE_DEVICES=0,1`
- **–û—Ç–∫–ª—é—á–∏—Ç—å GPU**: `CUDA_VISIBLE_DEVICES=""`
- **–í—Å–µ GPU**: `CUDA_VISIBLE_DEVICES=0,1,2,3`

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ examples:**
```bash
# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–≤–æ–π GPU
export CUDA_VISIBLE_DEVICES=0

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤—Ç–æ—Ä–æ–π GPU
export CUDA_VISIBLE_DEVICES=1

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–≤—É—Ö GPU
export CUDA_VISIBLE_DEVICES=0,1

# –û—Ç–∫–ª—é—á–µ–Ω–∏–µ GPU (—Ç–æ–ª—å–∫–æ CPU)
export CUDA_VISIBLE_DEVICES=""

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ all –¥–æ—Å—Ç—É–ø–Ω—ã—Ö GPU
export CUDA_VISIBLE_DEVICES=0,1,2,3
```

**check GPU –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏:**
```python
import torch

# check –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ CUDA
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"CUDA device count: {torch.cuda.device_count()}")

# –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ GPU
if torch.cuda.is_available():
 for i in range(torch.cuda.device_count()):
 print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
 print(f"GPU {i} memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.1f} GB")

# –¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ GPU
def test_gpu_performance():
 if torch.cuda.is_available():
 device = torch.device('cuda')

 # create –±–æ–ª—å—à–∏—Ö —Ç–µ–Ω–∑–æ—Ä–æ–≤
 size = 2000
 a = torch.randn(size, size, device=device)
 b = torch.randn(size, size, device=device)

 # –¢–µ—Å—Ç –º–∞—Ç—Ä–∏—á–Ω–æ–≥–æ —É–º–Ω–æ–∂–µ–Ω–∏—è on GPU
 start = time.time()
 result = torch.mm(a, b)
 torch.cuda.synchronize() # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
 end = time.time()

 print(f"GPU matrix multiplication: {end - start:.2f} seconds")
 else:
 print("GPU not available")

test_gpu_performance()
```

**AUTOGLUON_DEBUG - –†–µ–∂–∏–º –æ—Ç–ª–∞–¥–∫–∏**

**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:**
- –í–∫–ª—é—á–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω–æ–µ Logs—Ä–æ–≤–∞–Ω–∏–µ AutoML Gluon
- –ü–æ–º–æ–≥–∞–µ—Ç –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–±–ª–µ–º—ã
- –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã –æ–±—É—á–µ–Ω–∏—è

**–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:**
- **for –æ—Ç–ª–∞–¥–∫–∏**: `AUTOGLUON_DEBUG=1`
- **for –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞**: not —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å (on —É–º–æ–ª—á–∞–Ω–∏—é –≤—ã–∫–ª—é—á–µ–Ω)
- **for development**: `AUTOGLUON_DEBUG=1`

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ examples:**
```bash
# –í–∫–ª—é—á–µ–Ω–∏–µ –æ—Ç–ª–∞–¥–∫–∏
export AUTOGLUON_DEBUG=1

# –û—Ç–∫–ª—é—á–µ–Ω–∏–µ –æ—Ç–ª–∞–¥–∫–∏
unset AUTOGLUON_DEBUG

# –í—Ä–µ–º–µ–Ω–Ω–æ–µ –≤–∫–ª—é—á–µ–Ω–∏–µ for –æ–¥–Ω–æ–≥–æ Launch–∞
AUTOGLUON_DEBUG=1 python train_model.py
```

**–ß—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –æ—Ç–ª–∞–¥–æ—á–Ω—ã–π —Ä–µ–∂–∏–º:**
```python
import os
os.environ['AUTOGLUON_DEBUG'] = '1'

from autogluon.tabular import TabularPredictor
import pandas as pd
import numpy as np

# create tests—ã—Ö –¥–∞–Ω–Ω—ã—Ö
data = pd.dataFrame({
 'feature1': np.random.randn(100),
 'feature2': np.random.randn(100),
 'target': np.random.randint(0, 2, 100)
})

# create –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞ with debugging
predictor = TabularPredictor(label='target')

# –û–±—É—á–µ–Ω–∏–µ with –¥–µ—Ç–∞–ª—å–Ω—ã–º Logs—Ä–æ–≤–∞–Ω–∏–µ–º
predictor.fit(data, time_limit=60)
# –í—ã–≤–µ–¥–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ:
# - –í—ã–±–æ—Ä–µ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
# - –ü—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è
# - –í–∞–ª–∏–¥–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π
# - –°–æ–∑–¥–∞–Ω–∏–∏ –∞–Ω—Å–∞–º–±–ª–µ–π
```

**–ü–æ–ª–Ω–∞—è configuration –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è:**

```bash
#!/bin/bash
# –°–∫—Ä–∏–ø—Ç for –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π Settings AutoML Gluon

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —è–¥–µ—Ä
CPU_CORES=$(nproc)
RECOMMENDED_THREADS=$((CPU_CORES - 2)) # –û—Å—Ç–∞–≤–ª—è–µ–º 2 —è–¥—Ä–∞ for —Å–∏—Å—Ç–µ–º—ã

# configuration –ø–æ—Ç–æ–∫–æ–≤
export OMP_NUM_THREADS=$RECOMMENDED_THREADS
export MKL_NUM_THREADS=$RECOMMENDED_THREADS
export OPENBLAS_NUM_THREADS=$RECOMMENDED_THREADS

# configuration GPU
if command -v nvidia-smi &> /dev/null; then
 export CUDA_VISIBLE_DEVICES=0
 echo "GPU detected, CUDA_VISIBLE_DEVICES=0"
else
 export CUDA_VISIBLE_DEVICES=""
 echo "No GPU detected, Using CPU only"
fi

# –û—Ç–ª–∞–¥–æ—á–Ω—ã–π —Ä–µ–∂–∏–º (–≤–∫–ª—é—á–∏—Ç—å –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
# export AUTOGLUON_DEBUG=1

echo "Environment variables set:"
echo "OMP_NUM_THREADS=$OMP_NUM_THREADS"
echo "MKL_NUM_THREADS=$MKL_NUM_THREADS"
echo "OPENBLAS_NUM_THREADS=$OPENBLAS_NUM_THREADS"
echo "CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES"
```

**check —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –Ω–∞—Å—Ç—Ä–æ–µ–∫:**

```python
import os
import time
import numpy as np
import pandas as pd
from autogluon.tabular import TabularPredictor

def benchmark_environment():
 """–¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ with —Ç–µ–∫—É—â–∏–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏"""

 print("=== Environment Benchmark ===")
 print(f"OMP_NUM_THREADS: {os.getenv('OMP_NUM_THREADS', 'default')}")
 print(f"MKL_NUM_THREADS: {os.getenv('MKL_NUM_THREADS', 'default')}")
 print(f"OPENBLAS_NUM_THREADS: {os.getenv('OPENBLAS_NUM_THREADS', 'default')}")
 print(f"CUDA_VISIBLE_DEVICES: {os.getenv('CUDA_VISIBLE_DEVICES', 'default')}")

 # –¢–µ—Å—Ç NumPy –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
 print("\n=== NumPy Performance Test ===")
 size = 2000
 a = np.random.randn(size, size)
 b = np.random.randn(size, size)

 start = time.time()
 result = np.dot(a, b)
 numpy_time = time.time() - start
 print(f"Matrix multiplication: {numpy_time:.2f} seconds")

 # –¢–µ—Å—Ç AutoML Gluon
 print("\n=== AutoML Gluon Test ===")
 data = pd.dataFrame({
 'feature1': np.random.randn(1000),
 'feature2': np.random.randn(1000),
 'target': np.random.randint(0, 2, 1000)
 })

 predictor = TabularPredictor(label='target')

 start = time.time()
 predictor.fit(data, time_limit=30)
 autogluon_time = time.time() - start
 print(f"AutoML training: {autogluon_time:.2f} seconds")

 return numpy_time, autogluon_time

# Launch —Ç–µ—Å—Ç–∞
benchmark_environment()
```

### üìã –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª
**–ü–æ—á–µ–º—É –Ω—É–∂–µ–Ω –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª?** –ü–æ—Ç–æ–º—É —á—Ç–æ –æ–Ω –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å AutoML Gluon –ø–æ–¥ –≤–∞—à–∏ —Ä–µ—Å—É—Ä—Å—ã and –∑–∞–¥–∞—á–∏ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∫–æ–¥–∞.

–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª `~/.autogluon/config.yaml`:
```yaml
# configuration AutoGluon
default:
 time_limit: 3600 # 1 —á–∞—Å on —É–º–æ–ª—á–∞–Ω–∏—é
 memory_limit: 8 # 8GB RAM
 num_cpus: 4 # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ CPU —è–¥–µ—Ä
 num_gpus: 1 # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU

# Settings for different tasks
```

#### üîß –î–µ—Ç–∞–ª—å–Ω–æ–µ describe –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

**parameter `time_limit`:**

- **–ß—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç**: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è in —Å–µ–∫—É–Ω–¥–∞—Ö
- **–ó–∞—á–µ–º –Ω—É–∂–µ–Ω**: –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ, –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç —Ä–µ—Å—É—Ä—Å—ã
- **–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è**:
 - `3600` (1 —á–∞—Å) - for –±—ã—Å—Ç—Ä—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
 - `7200` (2 —á–∞—Å–∞) - for —Å—Ä–µ–¥–Ω–∏—Ö –∑–∞–¥–∞—á
 - `14400` (4 —á–∞—Å–∞) - for —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á
- **–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏**: –û–±—É—á–µ–Ω–∏–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å
- **–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π example**: –ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å 2 —á–∞—Å–∞ on –∑–∞–¥–∞—á—É, install `time_limit: 7200`
- **–î–µ—Ç–∞–ª—å–Ω–∞—è configuration on —Ç–∏–ø–∞–º –∑–∞–¥–∞—á**:
 - **–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (–º–∞–ª—ã–µ data < 10K —Å—Ç—Ä–æ–∫)**: `1800` (30 minutes)
- **–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (—Å—Ä–µ–¥–Ω–∏–µ data 10K-100K —Å—Ç—Ä–æ–∫)**: `3600` (1 —á–∞—Å)
- **–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (–±–æ–ª—å—à–∏–µ data > 100K —Å—Ç—Ä–æ–∫)**: `7200` (2 —á–∞—Å–∞)
- **–†–µ–≥—Ä–µ—Å—Å–∏—è (–º–∞–ª—ã–µ data < 10K —Å—Ç—Ä–æ–∫)**: `1800` (30 minutes)
- **–†–µ–≥—Ä–µ—Å—Å–∏—è (—Å—Ä–µ–¥–Ω–∏–µ data 10K-100K —Å—Ç—Ä–æ–∫)**: `5400` (1.5 —á–∞—Å–∞)
- **–†–µ–≥—Ä–µ—Å—Å–∏—è (–±–æ–ª—å—à–∏–µ data > 100K —Å—Ç—Ä–æ–∫)**: `10800` (3 —á–∞—Å–∞)
- **–í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã (–∫–æ—Ä–æ—Ç–∫–∏–µ —Å–µ—Ä–∏–∏ < 1K —Ç–æ—á–µ–∫)**: `3600` (1 —á–∞—Å)
- **–í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã (–¥–ª–∏–Ω–Ω—ã–µ —Å–µ—Ä–∏–∏ > 1K —Ç–æ—á–µ–∫)**: `7200` (2 —á–∞—Å–∞)
- **–í–ª–∏—è–Ω–∏–µ on –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏**:
 - **–ö–æ—Ä–æ—Ç–∫–æ–µ –≤—Ä–µ–º—è (30 –º–∏–Ω)**: –ë–∞–∑–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å, –±—ã—Å—Ç—Ä—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
- **–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è (1-2 —á–∞—Å–∞)**: –•–æ—Ä–æ—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å, —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥
- **–î–ª–∏–Ω–Ω–æ–µ –≤—Ä–µ–º—è (4+ —á–∞—Å–æ–≤)**: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å, –ª—É—á—à–∏–µ –º–æ–¥–µ–ª–∏
- **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è on —Ä–µ—Å—É—Ä—Å–∞–º**:
 - **CPU —Ç–æ–ª—å–∫–æ**: –£–≤–µ–ª–∏—á–∏—Ç—å –≤—Ä–µ–º—è in 2-3 —Ä–∞–∑–∞
- **GPU –¥–æ—Å—Ç—É–ø–Ω–∞**: –£–º–µ–Ω—å—à–∏—Ç—å –≤—Ä–µ–º—è in 2-3 —Ä–∞–∑–∞
- **–ú–Ω–æ–≥–æ —è–¥–µ—Ä (8+)**: –£–º–µ–Ω—å—à–∏—Ç—å –≤—Ä–µ–º—è on 30-50%
- **–ú–∞–ª–æ –ø–∞–º—è—Ç–∏ (< 8GB)**: –£–≤–µ–ª–∏—á–∏—Ç—å –≤—Ä–µ–º—è –∏–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π

**parameter `memory_limit`:**

- **–ß—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç**: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ RAM in –≥–∏–≥–∞–±–∞–π—Ç–∞—Ö
- **–ó–∞—á–µ–º –Ω—É–∂–µ–Ω**: –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏, –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç —Ä–µ—Å—É—Ä—Å—ã
- **–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è**:
 - `4` - for —Å–∏—Å—Ç–µ–º with 8GB RAM
 - `8` - for —Å–∏—Å—Ç–µ–º with 16GB RAM
 - `16` - for —Å–∏—Å—Ç–µ–º with 32GB RAM
- **–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏**: –û–±—É—á–µ–Ω–∏–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è with –æ—à–∏–±–∫–æ–π –ø–∞–º—è—Ç–∏
- **–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π example**: –ï—Å–ª–∏ —É –≤–∞—Å 16GB RAM, install `memory_limit: 12` (–æ—Å—Ç–∞–≤–ª—è—è 4GB for —Å–∏—Å—Ç–µ–º—ã)
- **–î–µ—Ç–∞–ª—å–Ω–∞—è configuration on —Ä–∞–∑–º–µ—Ä—É –¥–∞–Ω–Ω—ã—Ö**:
 - **–ú–∞–ª—ã–µ data (< 1MB)**: `2-4` GB
- **–°—Ä–µ–¥–Ω–∏–µ data (1-100MB)**: `4-8` GB
- **–ë–æ–ª—å—à–∏–µ data (100MB-1GB)**: `8-16` GB
- **–û—á–µ–Ω—å –±–æ–ª—å—à–∏–µ data (> 1GB)**: `16-32` GB
- **–í–ª–∏—è–Ω–∏–µ on –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å**:
 - **–ú–∞–ª–æ –ø–∞–º—è—Ç–∏**: –ú–µ–¥–ª–µ–Ω–Ω–∞—è Working, –≤–æ–∑–º–æ–∂–Ω—ã–µ –æ—à–∏–±–∫–∏
- **–î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏**: –ë—ã—Å—Ç—Ä–∞—è Working, —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
- **–ú–Ω–æ–≥–æ –ø–∞–º—è—Ç–∏**: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å, –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö
- **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è on —Ç–∏–ø—É –∑–∞–¥–∞—á**:
 - **–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è**: 2-4x —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
- **–†–µ–≥—Ä–µ—Å—Å–∏—è**: 3-5x —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
- **–í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã**: 4-6x —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
- **–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è**: 6-10x —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
- **Monitoring –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏**:
 - **check**: `import psutil; print(f"RAM usage: {psutil.virtual_memory().percent}%")`
- **–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ**: 70-80% from –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏
- **–ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ**: > 90% from –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏

**parameter `num_cpus`:**

- **–ß—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç**: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ CPU —è–¥–µ—Ä for –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
- **–ó–∞—á–µ–º –Ω—É–∂–µ–Ω**: –£—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —è–¥—Ä–∞
- **–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è**:
 - `2` - for —Å–∏—Å—Ç–µ–º with 4 —è–¥—Ä–∞–º–∏
 - `4` - for —Å–∏—Å—Ç–µ–º with 8 —è–¥—Ä–∞–º–∏
 - `8` - for —Å–∏—Å—Ç–µ–º with 16+ —è–¥—Ä–∞–º–∏
- **–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏**: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–æ—Å—Ç—É–ø–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —è–¥–µ—Ä
- **–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π example**: –ï—Å–ª–∏ —É –≤–∞—Å 8 —è–¥–µ—Ä, install `num_cpus: 6` (–æ—Å—Ç–∞–≤–ª—è—è 2 for —Å–∏—Å—Ç–µ–º—ã)
- **–î–µ—Ç–∞–ª—å–Ω–∞—è configuration on —Ç–∏–ø–∞–º –∑–∞–¥–∞—á**:
 - **–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (–º–∞–ª—ã–µ data)**: `2-4` —è–¥—Ä–∞
- **–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (–±–æ–ª—å—à–∏–µ data)**: `4-8` —è–¥–µ—Ä
- **–†–µ–≥—Ä–µ—Å—Å–∏—è (–º–∞–ª—ã–µ data)**: `2-4` —è–¥—Ä–∞
- **–†–µ–≥—Ä–µ—Å—Å–∏—è (–±–æ–ª—å—à–∏–µ data)**: `6-12` —è–¥–µ—Ä
- **–í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã**: `4-8` —è–¥–µ—Ä
- **–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è**: `8-16` —è–¥–µ—Ä
- **–í–ª–∏—è–Ω–∏–µ on —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è**:
 - **1 —è–¥—Ä–æ**: –ë–∞–∑–æ–≤–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å (100%)
- **2 —è–¥—Ä–∞**: –£—Å–∫–æ—Ä–µ–Ω–∏–µ in 1.5-1.8 —Ä–∞–∑–∞
- **4 —è–¥—Ä–∞**: –£—Å–∫–æ—Ä–µ–Ω–∏–µ in 2.5-3.5 —Ä–∞–∑–∞
- **8 —è–¥–µ—Ä**: –£—Å–∫–æ—Ä–µ–Ω–∏–µ in 4-6 —Ä–∞–∑
- **16+ —è–¥–µ—Ä**: –£—Å–∫–æ—Ä–µ–Ω–∏–µ in 6-10 —Ä–∞–∑
- **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è on –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º**:
 - **XGBoost**: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 4-8 —è–¥–µ—Ä
- **LightGBM**: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 4-12 —è–¥–µ—Ä
- **CatBoost**: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 2-8 —è–¥–µ—Ä
- **Neural networks**: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 8-16 —è–¥–µ—Ä
- **Monitoring –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è CPU**:
 - **check**: `import psutil; print(f"CPU usage: {psutil.cpu_percent()}%")`
- **–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ**: 80-90% from –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —è–¥–µ—Ä
- **–ü–µ—Ä–µ–≥—Ä—É–∑–∫–∞**: > 95% from –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —è–¥–µ—Ä

**parameter `num_gpus`:**

- **–ß—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç**: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU for acceleration –æ–±—É—á–µ–Ω–∏—è
- **–ó–∞—á–µ–º –Ω—É–∂–µ–Ω**: –£—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö networks in 10-100 —Ä–∞–∑
- **–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è**:
 - `0` - –µ—Å–ª–∏ –Ω–µ—Ç GPU or for CPU-only –∑–∞–¥–∞—á
 - `1` - for –æ–¥–Ω–æ–π GPU
 - `2+` - for –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö GPU (—Ç—Ä–µ–±—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π Settings)
- **–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø—Ä–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –∑–Ω–∞—á–µ–Ω–∏–∏**: AutoML Gluon –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã–µ GPU
- **–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π example**: –ï—Å–ª–∏ —É –≤–∞—Å RTX 3070, install `num_gpus: 1`
- **–î–µ—Ç–∞–ª—å–Ω–∞—è configuration on —Ç–∏–ø–∞–º GPU**:
 - **–ù–µ—Ç GPU**: `num_gpus: 0` - –æ–±—É—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ on CPU
- **GTX 1060 6GB**: `num_gpus: 1` - –±–∞–∑–æ–≤–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ GPU
- **RTX 3070 8GB**: `num_gpus: 1` - —Ö–æ—Ä–æ—à–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
- **RTX 4080 16GB**: `num_gpus: 1` - –≤—ã—Å–æ–∫–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
- **A100 40GB**: `num_gpus: 1` - –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è Working
- **–ù–µ—Å–∫–æ–ª—å–∫–æ GPU**: `num_gpus: 2+` - for large models
- **–í–ª–∏—è–Ω–∏–µ on —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è**:
 - **CPU —Ç–æ–ª—å–∫–æ**: –ë–∞–∑–æ–≤–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å (100%)
- **GTX 1060**: –£—Å–∫–æ—Ä–µ–Ω–∏–µ by 3-5 times
- **RTX 3070**: –£—Å–∫–æ—Ä–µ–Ω–∏–µ in 8-15 —Ä–∞–∑
- **RTX 4080**: –£—Å–∫–æ—Ä–µ–Ω–∏–µ in 15-25 —Ä–∞–∑
- **A100**: –£—Å–∫–æ—Ä–µ–Ω–∏–µ in 25-50 —Ä–∞–∑
- **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è on —Ç–∏–ø–∞–º –∑–∞–¥–∞—á**:
 - **–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (—Ç–∞–±–ª–∏—á–Ω—ã–µ data)**: GPU not –∫—Ä–∏—Ç–∏—á–Ω–∞
- **–†–µ–≥—Ä–µ—Å—Å–∏—è (—Ç–∞–±–ª–∏—á–Ω—ã–µ data)**: GPU not –∫—Ä–∏—Ç–∏—á–Ω–∞
- **–í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã**: GPU —É—Å–∫–æ—Ä—è–µ—Ç in 2-5 —Ä–∞–∑
- **–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è**: GPU –∫—Ä–∏—Ç–∏—á–Ω–∞, —É—Å–∫–æ—Ä–µ–Ω–∏–µ in 10-50 —Ä–∞–∑
- **–¢–µ–∫—Å—Ç**: GPU —É—Å–∫–æ—Ä—è–µ—Ç in 5-20 —Ä–∞–∑
- **–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è to memory GPU**:
 - **–ú–∞–ª—ã–µ –º–æ–¥–µ–ª–∏ (< 1M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)**: 2-4 GB VRAM
- **–°—Ä–µ–¥–Ω–∏–µ –º–æ–¥–µ–ª–∏ (1-10M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)**: 4-8 GB VRAM
- **–ë–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ (10-100M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)**: 8-16 GB VRAM
- **–û—á–µ–Ω—å –±–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ (> 100M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)**: 16+ GB VRAM
- **check –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU**:
 - **check CUDA**: `python -c "import torch; print(torch.cuda.is_available())"`
- **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU**: `python -c "import torch; print(torch.cuda.device_count())"`
- **–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ GPU**: `python -c "import torch; print(torch.cuda.get_device_name(0))"`
tabular:
 presets: ['best_quality', 'high_quality', 'good_quality', 'medium_quality', 'optimize_for_deployment']
 hyperparameter_tune_kwargs:
 num_trials: 10
 scheduler: 'local'
 searcher: 'auto'

timeseries:
 Prediction_length: 24
 freq: 'H'
 target_column: 'target'
```

#### üéØ –î–µ—Ç–∞–ª—å–Ω–æ–µ describe –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ for —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

**parameter `presets`:**

- **–ß—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç**: –ü—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏
- **–ó–∞—á–µ–º –Ω—É–∂–µ–Ω**: –£–ø—Ä–æ—â–∞–µ—Ç –≤—ã–±–æ—Ä –º–µ–∂–¥—É —Å–∫–æ—Ä–æ—Å—Ç—å—é and –∫–∞—á–µ—Å—Ç–≤–æ–º
- **–î–µ—Ç–∞–ª—å–Ω–æ–µ describe –∫–∞–∂–¥–æ–≥–æ preset**: **`best_quality`:**
- **–ß—Ç–æ –¥–µ–ª–∞–µ—Ç**: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏
- **–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è**: 4-8 —á–∞—Å–æ–≤
- **–ò—Å–ø–æ–ª—å–∑—É–µ—Ç**: –í—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã, –∞–Ω—Å–∞–º–±–ª–∏, —Ç—é–Ω–∏–Ω–≥ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- **–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å**: for –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞, –∫–æ–≥–¥–∞ –∫–∞—á–µ—Å—Ç–≤–æ –∫—Ä–∏—Ç–∏—á–Ω–æ
- **–†–µ–∑—É–ª—å—Ç–∞—Ç**: –õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å, –Ω–æ –¥–æ–ª–≥–æ–µ –æ–±—É—á–µ–Ω–∏–µ

 **`high_quality`:**
- **–ß—Ç–æ –¥–µ–ª–∞–µ—Ç**: –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ with —Ä–∞–∑—É–º–Ω—ã–º –≤—Ä–µ–º–µ–Ω–µ–º
- **–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è**: 2-4 —á–∞—Å–∞
- **–ò—Å–ø–æ–ª—å–∑—É–µ—Ç**: –û—Å–Ω–æ–≤–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã + –∞–Ω—Å–∞–º–±–ª–∏
- **–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å**: for –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∑–∞–¥–∞—á
- **–†–µ–∑—É–ª—å—Ç–∞—Ç**: –•–æ—Ä–æ—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –∑–∞ —Ä–∞–∑—É–º–Ω–æ–µ –≤—Ä–µ–º—è

 **`good_quality`:**
- **–ß—Ç–æ –¥–µ–ª–∞–µ—Ç**: –•–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∑–∞ –∫–æ—Ä–æ—Ç–∫–æ–µ –≤—Ä–µ–º—è
- **–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è**: 30-60 minutes
- **–ò—Å–ø–æ–ª—å–∑—É–µ—Ç**: –û—Å–Ω–æ–≤–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –±–µ–∑ –∞–Ω—Å–∞–º–±–ª–µ–π
- **–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å**: for –±—ã—Å—Ç—Ä—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
- **–†–µ–∑—É–ª—å—Ç–∞—Ç**: –ü—Ä–∏–µ–º–ª–µ–º–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –±—ã—Å—Ç—Ä–æ

 **`medium_quality`:**
- **–ß—Ç–æ –¥–µ–ª–∞–µ—Ç**: –°—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∑–∞ –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–æ–µ –≤—Ä–µ–º—è
- **–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è**: 10-30 minutes
- **–ò—Å–ø–æ–ª—å–∑—É–µ—Ç**: –¢–æ–ª—å–∫–æ –±—ã—Å—Ç—Ä—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã
- **–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å**: for –ø—Ä–æ—Ç–æ—Ç–∏–ø–∏—Ä–æ–≤–∞–Ω–∏—è
- **–†–µ–∑—É–ª—å—Ç–∞—Ç**: –ë–∞–∑–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ

 **`optimize_for_deployment`:**
- **–ß—Ç–æ –¥–µ–ª–∞–µ—Ç**: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è for –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞
- **–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è**: 1-2 —á–∞—Å–∞
- **–ò—Å–ø–æ–ª—å–∑—É–µ—Ç**: –ë—ã—Å—Ç—Ä—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã with –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
- **–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å**: for –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞ with –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏ —Ä–µ—Å—É—Ä—Å–æ–≤
- **–†–µ–∑—É–ª—å—Ç–∞—Ç**: –ë—ã—Å—Ç—Ä—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è, —Ö–æ—Ä–æ—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å

**parameter `num_trials`:**

- **–ß—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç**: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ —Ç—é–Ω–∏–Ω–≥–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- **–ó–∞—á–µ–º –Ω—É–∂–µ–Ω**: –ë–æ–ª—å—à–µ –ø–æ–ø—ã—Ç–æ–∫ = –ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ, –Ω–æ –¥–æ–ª—å—à–µ –≤—Ä–µ–º—è
- **–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è**:
 - `5` - for –±—ã—Å—Ç—Ä—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
 - `10` - for —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –∑–∞–¥–∞—á
 - `20` - for –≤–∞–∂–Ω—ã—Ö –∑–∞–¥–∞—á
 - `50+` - for –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
- **–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π example**: –ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å 2 —á–∞—Å–∞, install `num_trials: 10`

**parameter `scheduler`:**

- **–ß—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç**: Plan–∏—Ä–æ–≤—â–∏–∫ for —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∑–∞–¥–∞—á
- **–ó–∞—á–µ–º –Ω—É–∂–µ–Ω**: –£–ø—Ä–∞–≤–ª—è–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ–º
- **–î–æ—Å—Ç—É–ø–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è**:
 - `'local'` - –ª–æ–∫–∞–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ (on —É–º–æ–ª—á–∞–Ω–∏—é)
 - `'ray'` - —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ Ray
 - `'dask'` - —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ Dask
- **–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π example**: for –æ–¥–Ω–æ–≥–æ –∫–æ–º–ø—å—é—Ç–µ—Ä–∞ Use `'local'`

#### ‚è∞ –î–µ—Ç–∞–ª—å–Ω–æ–µ describe –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ for –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤

**parameter `Prediction_length`:**

- **–ß—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç**: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±—É–¥—É—â–∏—Ö —Ç–æ—á–µ–∫ for –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è
- **–ó–∞—á–µ–º –Ω—É–∂–µ–Ω**: –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≥–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è
- **–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è**:
 - `24` - for –ø–æ—á–∞—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–ø—Ä–æ–≥–Ω–æ–∑ on —Å—É—Ç–∫–∏)
 - `7` - for –¥–Ω–µ–≤–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–ø—Ä–æ–≥–Ω–æ–∑ on –Ω–µ–¥–µ–ª—é)
 - `30` - for –¥–Ω–µ–≤–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–ø—Ä–æ–≥–Ω–æ–∑ on –º–µ—Å—è—Ü)
- **–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π example**: for –ø—Ä–æ–≥–Ω–æ–∑–∞ –ø—Ä–æ–¥–∞–∂ on –Ω–µ–¥–µ–ª—é install `Prediction_length: 7`

**parameter `freq`:**

- **–ß—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç**: –ß–∞—Å—Ç–æ—Ç–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞
- **–ó–∞—á–µ–º –Ω—É–∂–µ–Ω**: –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É —Ç–æ—á–∫–∞–º–∏
- **–î–æ—Å—Ç—É–ø–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è**:
 - `'H'` - –ø–æ—á–∞—Å–æ–≤—ã–µ data
 - `'D'` - –¥–Ω–µ–≤–Ω—ã–µ data
 - `'W'` - –Ω–µ–¥–µ–ª—å–Ω—ã–µ data
 - `'M'` - –º–µ—Å—è—á–Ω—ã–µ data
- **–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π example**: for –¥–Ω–µ–≤–Ω—ã—Ö –ø—Ä–æ–¥–∞–∂ install `freq: 'D'`

**parameter `target_column`:**

- **–ß—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç**: –ù–∞–∑–≤–∞–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–∞ with —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
- **–ó–∞—á–µ–º –Ω—É–∂–µ–Ω**: –£–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å
- **–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π example**: –ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å —Å—Ç–æ–ª–±–µ—Ü 'sales', install `target_column: 'sales'`
```

## –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º –ø—Ä–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–µ

### Issues with –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏
```bash
# clean cache pip
pip cache purge

# reinstall with –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ–º cache
pip install --no-cache-dir autogluon

# installation –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –≤–µ—Ä—Å–∏–∏
pip install autogluon==0.8.2
```

### Issues with CUDA
```bash
# check –≤–µ—Ä—Å–∏–∏ CUDA
nvidia-smi

# check —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ PyTorch
python -c "import torch; print(torch.cuda.is_available())"

# installation —Å–æ–≤–º–µ—Å—Ç–∏–º–æ–π –≤–µ—Ä—Å–∏–∏ PyTorch
pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 --extra-index-url https://download.pytorch.org/whl/cu117
```

### Issues with –ø–∞–º—è—Ç—å—é
```bash
# installation with –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–∞–º—è—Ç–∏
pip install --no-cache-dir --no-deps autogluon
pip install -r requirements.txt
```

## check —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏

### –ü–æ–ª–Ω—ã–π —Ç–µ—Å—Ç installation
```python
import autogluon as ag
from autogluon.tabular import TabularPredictor
import pandas as pd
import numpy as np

def test_installation():
 """–ü–æ–ª–Ω—ã–π —Ç–µ—Å—Ç installation AutoGluon"""

 # create tests—ã—Ö –¥–∞–Ω–Ω—ã—Ö
 np.random.seed(42)
 n_samples = 1000
 data = pd.dataFrame({
 'feature1': np.random.randn(n_samples),
 'feature2': np.random.randn(n_samples),
 'feature3': np.random.randn(n_samples),
 'target': np.random.randint(0, 2, n_samples)
 })

 # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ on train/test
 train_data = data[:800]
 test_data = data[800:]

 # create and –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
 predictor = TabularPredictor(
 label='target',
 problem_type='binary',
 eval_metric='accuracy'
 )

 # –û–±—É—á–µ–Ω–∏–µ with –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–∏
 predictor.fit(
 train_data,
 time_limit=60, # 1 minutes–∞
 presets='medium_quality'
 )

 # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
 predictions = predictor.predict(test_data)

 # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
 performance = predictor.evaluate(test_data)

 print(f"Model performance: {performance}")
 print("installation test COMPLETED successfully!")

 return True

if __name__ == "__main__":
 test_installation()
```

## üöÄ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞

<img src="images/optimized/production_architecture.png" alt="–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞" style="max-width: 100%; height: auto; display: block; margin: 20px auto;">
*–†–∏—Å—É–Ω–æ–∫ 8: –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ deployment AutoML Gluon in –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ*

**–ü–æ—á–µ–º—É –≤–∞–∂–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞?** –ü–æ—Ç–æ–º—É —á—Ç–æ —ç—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ —ÅPlan–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ:

- **–ú–æ–¥–µ–ª—å**: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å AutoML Gluon
- **API Gateway**: –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ for –∑–∞–ø—Ä–æ—Å–æ–≤
- **Load Balancer**: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–≥—Ä—É–∑–∫–∏ –º–µ–∂–¥—É –∏–Ω—Å—Ç–∞–Ω—Å–∞–º–∏
- **Monitoring**: Monitoring –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ and –∫–∞—á–µ—Å—Ç–≤–∞
- **Scaling**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–¥ –Ω–∞–≥—Ä—É–∑–∫—É
- **data Pipeline**: –ü–æ—Ç–æ–∫ –¥–∞–Ω–Ω—ã—Ö for –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è

### üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–¥–∞–∫—à–µ–Ω —Ä–µ—à–µ–Ω–∏–π

<img src="images/optimized/production_comparison.png" alt="–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–¥–∞–∫—à–µ–Ω —Ä–µ—à–µ–Ω–∏–π" style="max-width: 100%; height: auto; display: block; margin: 20px auto;">
*–†–∏—Å—É–Ω–æ–∫ 9: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –∫ deployment*

**–ü–æ—á–µ–º—É –≤–∞–∂–Ω–æ —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å —Ä–µ—à–µ–Ω–∏—è?** –ü–æ—Ç–æ–º—É —á—Ç–æ —Ä–∞–∑–Ω—ã–µ –∑–∞–¥–∞—á–∏ —Ç—Ä–µ–±—É—é—Ç —Ä–∞–∑–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤:

- **Batch Processing**: –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–∞–∫–µ—Ç–∞–º–∏ (for –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤)
- **Real-time API**: –ú–≥–Ω–æ–≤–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (for –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π)
- **Edge deployment**: –†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ on –ø–µ—Ä–∏—Ñ–µ—Ä–∏–π–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö
- **Cloud deployment**: –†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ in –æ–±–ª–∞–∫–µ (–º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å)
- **Hybrid Approach**: –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ (–≥–∏–±–∫–æ—Å—Ç—å)

## –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏

–ü–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–π installation –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç–µ –∫:
- [–ë–∞–∑–æ–≤–æ–º—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é](./02_basic_usage.md)
- [–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏](./03_advanced_configuration.md)
- [–†–∞–±–æ—Ç–µ with –º–µ—Ç—Ä–∏–∫–∞–º–∏](./04_metrics.md)

## –ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏

- [–û—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è documentation](https://auto.gluon.ai/)
- [GitHub —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π](https://github.com/autogluon/autogluon)
- [examples –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è](https://github.com/autogluon/autogluon/tree/master/examples)
- [–§–æ—Ä—É–º —Å–æ–æ–±—â–µ—Å—Ç–≤–∞](https://discuss.autogluon.ai/)
