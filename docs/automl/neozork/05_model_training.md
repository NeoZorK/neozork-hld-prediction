# 05. ü§ñ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π

**–¶–µ–ª—å:** –ù–∞—É—á–∏—Ç—å—Å—è –æ–±—É—á–∞—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ ML-–º–æ–¥–µ–ª–∏ for —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

## –í—ã–±–æ—Ä –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ for —Ç–æ—Ä–≥–æ–≤–ª–∏

**–¢–µ–æ—Ä–∏—è:** –í—ã–±–æ—Ä –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ for —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–µ–Ω for —É—Å–ø–µ—Ö–∞ ML-—Å–∏—Å—Ç–µ–º. –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–º–µ—é—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —Ç—Ä–µ–±—É—é—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –∫ –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π.

### –ü–æ—á–µ–º—É not –≤—Å–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –ø–æ–¥—Ö–æ–¥—è—Ç?

**–¢–µ–æ—Ä–∏—è:** –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–º–µ—é—Ç —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –¥–µ–ª–∞—é—Ç –º–Ω–æ–≥–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ ML-–∞–ª–≥–æ—Ä–∏—Ç–º—ã –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º–∏ or –¥–∞–∂–µ –æ–ø–∞—Å–Ω—ã–º–∏. –ü–æ–Ω–∏–º–∞–Ω–∏–µ —ç—Ç–∏—Ö –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π –∫—Ä–∏—Ç–∏—á–Ω–æ for –≤—ã–±–æ—Ä–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤.

**–§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–º–µ—é—Ç –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:**

**1. –ù–µ—Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ—Å—Ç—å**
- **–¢–µ–æ—Ä–∏—è:** –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –º–µ–Ω—è—é—Ç—Å—è –≤–æ –≤—Ä–µ–º–µ–Ω–∏ –∏–∑-–∑–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ä—ã–Ω–æ—á–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π
- **–ü–æ—á–µ–º—É –ø—Ä–æ–±–ª–µ–º–∞—Ç–∏—á–Ω–æ:** –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞—é—Ç —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ—Å—Ç—å
- **–ü–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è:** –ú–æ–¥–µ–ª–∏ –±—ã—Å—Ç—Ä–æ —É—Å—Ç–∞—Ä–µ–≤–∞—é—Ç, —Å–Ω–∏–∂–∞–µ—Ç—Å—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
- **–ü–ª—é—Å—ã:** –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º
- **–ú–∏–Ω—É—Å—ã:** –°–ª–æ–∂–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —Ä–µ–≥—É–ª—è—Ä–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è

**2. –í—ã—Å–æ–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å**
- **–¢–µ–æ—Ä–∏—è:** –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –º–Ω–æ–≥–æ —à—É–º–∞ and —Å–ª—É—á–∞–π–Ω—ã—Ö –∫–æ–ª–µ–±–∞–Ω–∏–π
- **–ü–æ—á–µ–º—É –ø—Ä–æ–±–ª–µ–º–∞—Ç–∏—á–Ω–æ:** –®—É–º –º–æ–∂–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å on —Å–ª—É—á–∞–π–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö
- **–ü–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è:** –õ–æ–∂–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã, –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ, –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
- **–ü–ª—é—Å—ã:** –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≤—ã—è–≤–ª–µ–Ω–∏—è —Ä–µ–∞–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
- **–ú–∏–Ω—É—Å—ã:** –°–ª–æ–∂–Ω–æ—Å—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —à—É–º–∞, —Ä–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è

**3. –ù–µ—Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç—å**
- **–¢–µ–æ—Ä–∏—è:** –†–µ–¥–∫–∏–µ, –Ω–æ –≤–∞–∂–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è (–∫—Ä–∏–∑–∏—Å—ã, –∫—Ä–∞—Ö–∏) –∏–º–µ—é—Ç –Ω–µ–ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –±–æ–ª—å—à–æ–µ –≤–ª–∏—è–Ω–∏–µ
- **–ü–æ—á–µ–º—É –ø—Ä–æ–±–ª–µ–º–∞—Ç–∏—á–Ω–æ:** –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –º–æ–≥—É—Ç –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–¥–∫–∏–µ —Å–æ–±—ã—Ç–∏—è
- **–ü–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è:** –ú–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç not —É—á–∏—Ç—ã–≤–∞—Ç—å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–±—ã—Ç–∏—è
- **–ü–ª—é—Å—ã:** –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≤—ã—è–≤–ª–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π
- **–ú–∏–Ω—É—Å—ã:** –°–ª–æ–∂–Ω–æ—Å—Ç—å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤, —Ä–∏—Å–∫ –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∞–∂–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π

**4. –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏**
- **–¢–µ–æ—Ä–∏—è:** –ü—Ä–∏–∑–Ω–∞–∫–∏ —á–∞—Å—Ç–æ —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω—ã, —á—Ç–æ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏
- **–ü–æ—á–µ–º—É –ø—Ä–æ–±–ª–µ–º–∞—Ç–∏—á–Ω–æ:** –ö–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –º–æ–≥—É—Ç –∏—Å–∫–∞–∂–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
- **–ü–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è:** –ù–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏, —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏
- **–ü–ª—é—Å—ã:** –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≤—ã—è–≤–ª–µ–Ω–∏—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
- **–ú–∏–Ω—É—Å—ã:** –°–ª–æ–∂–Ω–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏, —Ä–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è

### –õ—É—á—à–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã for —Ñ–∏–Ω–∞–Ω—Å–æ–≤

**–¢–µ–æ—Ä–∏—è:** –í—ã–±–æ—Ä –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ for —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–æ–ª–∂–µ–Ω –æ—Å–Ω–æ–≤—ã–≤–∞—Ç—å—Å—è on –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç–∞—Ç—å with –Ω–µ—Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω—ã–º–∏, –∑–∞—à—É–º–ª–µ–Ω–Ω—ã–º–∏ and –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏. –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –ø–æ–∫–∞–∑–∞–ª–∏ –æ—Å–æ–±—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å in —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Å—Ñ–µ—Ä–µ.

**1. –ê–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã**
- **–ü–æ—á–µ–º—É —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã:** –ö–æ–º–±–∏–Ω–∏—Ä—É—é—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π, —Å–Ω–∏–∂–∞—è —Ä–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
- **–ü–ª—é—Å—ã:** –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å, —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –≤—ã–±—Ä–æ—Å–∞–º, –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å
- **–ú–∏–Ω—É—Å—ã:** –í—ã—Å–æ–∫–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã, —Å–ª–æ–∂–Ω–æ—Å—Ç—å settings
- **–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ:** Random Forest, XGBoost, LightGBM for –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ and —Ä–µ–≥—Ä–µ—Å—Å–∏–∏

**2. –ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏**
- **–ü–æ—á–µ–º—É —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã:** –ú–æ–≥—É—Ç –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ dependencies
- **–ü–ª—é—Å—ã:** –í—ã—Å–æ–∫–∞—è –≥–∏–±–∫–æ—Å—Ç—å, —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –æ–±—É—á–µ–Ω–∏—é —Å–ª–æ–∂–Ω—ã–º –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
- **–ú–∏–Ω—É—Å—ã:** –¢—Ä–µ–±—É—é—Ç –º–Ω–æ–≥–æ –¥–∞–Ω–Ω—ã—Ö, —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏, —Ä–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
- **–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ:** LSTM, GRU for –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤, Transformer for –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π

**3. SVM (Support Vector Machine)**
- **–ü–æ—á–µ–º—É —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã:** –•–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞—é—Ç with –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–º–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏
- **–ü–ª—é—Å—ã:** –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã on –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —É—Å—Ç–æ–π—á–∏–≤—ã –∫ –≤—ã–±—Ä–æ—Å–∞–º
- **–ú–∏–Ω—É—Å—ã:** –ú–µ–¥–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ on –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö, —Å–ª–æ–∂–Ω–æ—Å—Ç—å settings
- **–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ:** –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω

**4. Logistic Regression**
- **–ü–æ—á–µ–º—É —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã:** –ü—Ä–æ—Å—Ç—ã–µ, –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–µ, –±—ã—Å—Ç—Ä—ã–µ
- **–ü–ª—é—Å—ã:** –õ–µ–≥–∫–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è, –±—ã—Å—Ç—Ä–∞—è —Ä–∞–±–æ—Ç–∞, —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
- **–ú–∏–Ω—É—Å—ã:** –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
- **–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ:** –ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–µ —Å–∏—Å—Ç–µ–º—ã

**–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–æ–æ–±—Ä–∞–∂–µ–Ω–∏—è:**
- **–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è:** –í–∞–∂–Ω–∞ for –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
- **–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è:** –ö—Ä–∏—Ç–∏—á–Ω–∞ for –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
- **–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è:** –ú–æ–∂–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
- **–ê–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ:** –ö–æ–º–±–∏–Ω–∞—Ü–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ —á–∞—Å—Ç–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏

## –ê–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã

**–¢–µ–æ—Ä–∏—è:** –ê–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã –∫–æ–º–±–∏–Ω–∏—Ä—É—é—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π for —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –û–Ω–∏ –æ—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã for —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —Ç–∞–∫ –∫–∞–∫ —Å–Ω–∏–∂–∞—é—Ç —Ä–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è and –ø–æ–≤—ã—à–∞—é—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π.

**–ü–æ—á–µ–º—É –∞–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã for —Ñ–∏–Ω–∞–Ω—Å–æ–≤:**
- **–°–Ω–∏–∂–µ–Ω–∏–µ —Ä–∏—Å–∫–∞:** –ö–æ–º–±–∏–Ω–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π —Å–Ω–∏–∂–∞–µ—Ç —Ä–∏—Å–∫ –æ—à–∏–±–æ–∫
- **–£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –≤—ã–±—Ä–æ—Å–∞–º:** –†–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏ on-—Ä–∞–∑–Ω–æ–º—É —Ä–µ–∞–≥–∏—Ä—É—é—Ç on –≤—ã–±—Ä–æ—Å—ã
- **–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å:** –ê–Ω—Å–∞–º–±–ª–∏ –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã, —á–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
- **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å:** –ú–æ–∂–Ω–æ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

### 1. Random Forest

**–¢–µ–æ—Ä–∏—è:** Random Forest - —ç—Ç–æ –∞–Ω—Å–∞–º–±–ª—å —Ä–µ—à–∞—é—â–∏—Ö –¥–µ—Ä–µ–≤—å–µ–≤, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±—É—Ç—Å—Ç—Ä–∞–ø –∞–≥—Ä–µ–≥–∞—Ü–∏—é (bagging) for —Å–æ–∑–¥–∞–Ω–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π. –ö–∞–∂–¥–æ–µ –¥–µ—Ä–µ–≤–æ –æ–±—É—á–∞–µ—Ç—Å—è on —Å–ª—É—á–∞–π–Ω–æ–π –ø–æ–¥–≤—ã–±–æ—Ä–∫–µ –¥–∞–Ω–Ω—ã—Ö and –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.

**–î–µ—Ç–∞–ª—å–Ω–∞—è —Ç–µ–æ—Ä–∏—è Random Forest:**

**–ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã:**
1. **Bootstrap Sampling:** –ö–∞–∂–¥–æ–µ –¥–µ—Ä–µ–≤–æ –æ–±—É—á–∞–µ—Ç—Å—è on —Å–ª—É—á–∞–π–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ with –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ–º (–æ–±—ã—á–Ω–æ 63% –¥–∞–Ω–Ω—ã—Ö)
2. **Feature Randomness:** on –∫–∞–∂–¥–æ–º —É–∑–ª–µ –¥–µ—Ä–µ–≤–∞ –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è —Å–ª—É—á–∞–π–Ω–æ–µ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
3. **Voting/Averaging:** –§–∏–Ω–∞–ª—å–Ω–æ–µ Prediction - —ç—Ç–æ —Å—Ä–µ–¥–Ω–µ–µ (—Ä–µ–≥—Ä–µ—Å—Å–∏—è) or –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è) –≤—Å–µ—Ö –¥–µ—Ä–µ–≤—å–µ–≤

**–ü–æ—á–µ–º—É Random Forest —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω for —Ñ–∏–Ω–∞–Ω—Å–æ–≤:**
- **–£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—é:** –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤ —Å–Ω–∏–∂–∞—é—Ç —Ä–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è on —à—É–º–µ
- **–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤:** –î–µ—Ä–µ–≤—å—è on-—Ä–∞–∑–Ω–æ–º—É —Ä–µ–∞–≥–∏—Ä—É—é—Ç on –≤—ã–±—Ä–æ—Å—ã, —Å–Ω–∏–∂–∞—è –∏—Ö –≤–ª–∏—è–Ω–∏–µ
- **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å:** –ú–æ–∂–Ω–æ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —á–µ—Ä–µ–∑ feature importance
- **–ë—ã—Å—Ç—Ä–æ—Ç–∞:** parallel training –¥–µ—Ä–µ–≤—å–µ–≤ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –±–æ–ª—å—à–∏–µ –æ–±—ä–µ–º—ã –¥–∞–Ω–Ω—ã—Ö
- **–£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏:** –°–ª—É—á–∞–π–Ω—ã–π –≤—ã–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–Ω–∏–∂–∞–µ—Ç –≤–ª–∏—è–Ω–∏–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π

**–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Å–Ω–æ–≤–∞:**
- **Bootstrap:** for –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞ t, –æ–±—É—á–∞–µ–º on –≤—ã–±–æ—Ä–∫–µ D_t, –ø–æ–ª—É—á–µ–Ω–Ω–æ–π –∏–∑ D with –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ–º
- **Feature Selection:** on –∫–∞–∂–¥–æ–º —É–∑–ª–µ –≤—ã–±–∏—Ä–∞–µ–º ‚àöp –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ p –¥–æ—Å—Ç—É–ø–Ω—ã—Ö
- **Prediction:** ≈∑ = (1/T) * Œ£(t=1 to T) f_t(x), –≥–¥–µ T - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤

**–ü–ª—é—Å—ã Random Forest:**
- –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å on –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–µ –∑–∞–¥–∞—á
- –£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—é
- –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å —á–µ—Ä–µ–∑ feature importance
- –ë—ã—Å—Ç—Ä–æ—Ç–∞ –æ–±—É—á–µ–Ω–∏—è and –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
- –†–∞–±–æ—Ç–∞–µ—Ç with –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
- not —Ç—Ä–µ–±—É–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

**–ú–∏–Ω—É—Å—ã Random Forest:**
- –ú–æ–≥—É—Ç –±—ã—Ç—å –º–µ–Ω–µ–µ —Ç–æ—á–Ω—ã–º–∏ on –æ—á–µ–Ω—å —Å–ª–æ–∂–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- –¢—Ä–µ–±—É—é—Ç settings –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (n_estimators, max_depth, etc.)
- –ú–æ–≥—É—Ç –±—ã—Ç—å –∏–∑–±—ã—Ç–æ—á–Ω—ã–º–∏ for –ø—Ä–æ—Å—Ç—ã—Ö –∑–∞–¥–∞—á
- –ü–ª–æ—Ö–æ —Ä–∞–±–æ—Ç–∞—é—Ç with –æ—á–µ–Ω—å —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
- –ú–æ–≥—É—Ç –ø–µ—Ä–µ–æ–±—É—á–∏—Ç—å—Å—è on –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö
**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Random Forest:**

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç –∫–æ–¥:**
1. **–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:** –°–æ–∑–¥–∞–µ—Ç –æ–±—É—á–∞—é—â—É—é and —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏ with —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø—Ä–æ–ø–æ—Ä—Ü–∏–π –∫–ª–∞—Å—Å–æ–≤
2. **create –º–æ–¥–µ–ª–∏:** –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç parameters Random Forest for —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
3. **–û–±—É—á–µ–Ω–∏–µ:** –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å on –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
4. **–û—Ü–µ–Ω–∫–∞:** –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å on –æ–±—É—á–∞—é—â–µ–π and —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∞—Ö

**–û–±—ä—è—Å–Ω–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:**
- `n_estimators=100`: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤ in –ª–µ—Å—É (–±–æ–ª—å—à–µ = –ª—É—á—à–µ, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ)
- `max_depth=10`: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤–∞ (–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ)
- `min_samples_split=5`: –ú–∏–Ω–∏–º—É–º –æ–±—Ä–∞–∑—Ü–æ–≤ for —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —É–∑–ª–∞
- `min_samples_leaf=2`: –ú–∏–Ω–∏–º—É–º –æ–±—Ä–∞–∑—Ü–æ–≤ in –ª–∏—Å—Ç–µ
- `n_jobs=-1`: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —è–¥—Ä–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞

```python
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

def train_random_forest(X, y, test_size=0.2, random_state=42):
 """
 –û–±—É—á–µ–Ω–∏–µ Random Forest for —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö

 Args:
 X (array-like): –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (samples, features)
 y (array-like): –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (samples,)
 test_size (float): –î–æ–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö (0.0-1.0)
 random_state (int): Seed for –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏

 Returns:
 tuple: (–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å, –º–µ—Ç—Ä–∏–∫–∏, –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
 """

 print("=== –û–±—É—á–µ–Ω–∏–µ Random Forest ===")
 print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {X.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤, {X.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
 print(f"–ö–ª–∞—Å—Å—ã: {np.unique(y, return_counts=True)}")

 # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö with —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø—Ä–æ–ø–æ—Ä—Ü–∏–π –∫–ª–∞—Å—Å–æ–≤
 X_train, X_test, y_train, y_test = train_test_split(
 X, y, test_size=test_size, random_state=random_state, stratify=y
 )

 print(f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
 print(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")

 # create –º–æ–¥–µ–ª–∏ with –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ for —Ñ–∏–Ω–∞–Ω—Å–æ–≤
 rf = RandomForestClassifier(
 n_estimators=100, # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤
 max_depth=10, # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ (–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ)
 min_samples_split=5, # –ú–∏–Ω–∏–º—É–º for —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —É–∑–ª–∞
 min_samples_leaf=2, # –ú–∏–Ω–∏–º—É–º in –ª–∏—Å—Ç–µ
 max_features='sqrt', # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ for —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
 bootstrap=True, # Bootstrap sampling
 oob_score=True, # Out-of-bag –æ—Ü–µ–Ω–∫–∞
 random_state=random_state,
 n_jobs=-1, # parallel training
 verbose=0
 )

 print("\n–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏:")
 print(f"n_estimators: {rf.n_estimators}")
 print(f"max_depth: {rf.max_depth}")
 print(f"max_features: {rf.max_features}")

 # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
 print("\n–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
 rf.fit(X_train, y_train)

 # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
 y_train_pred = rf.predict(X_train)
 y_test_pred = rf.predict(X_test)

 # –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
 train_score = rf.score(X_train, y_train)
 test_score = rf.score(X_test, y_test)
 oob_score = rf.oob_score_

 print(f"\n=== –†–µ–∑—É–ª—å—Ç–∞—Ç—ã ===")
 print(f"Train accuracy: {train_score:.4f}")
 print(f"Test accuracy: {test_score:.4f}")
 print(f"OOB score: {oob_score:.4f}")

 # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
 print(f"\n=== Classification Report (Test) ===")
 print(classification_report(y_test, y_test_pred))

 # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
 feature_importance = rf.feature_importances_
 feature_names = [f'feature_{i}' for i in range(X.shape[1])]

 # create DataFrame with –≤–∞–∂–Ω–æ—Å—Ç—å—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
 importance_df = pd.DataFrame({
 'feature': feature_names,
 'importance': feature_importance
 }).sort_values('importance', ascending=False)

 print(f"\n=== –¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ===")
 print(importance_df.head(10))

 # –ú–µ—Ç—Ä–∏–∫–∏ for –≤–æ–∑–≤—Ä–∞—Ç–∞
 metrics = {
 'train_accuracy': train_score,
 'test_accuracy': test_score,
 'oob_score': oob_score,
 'feature_importance': importance_df,
 'confusion_matrix': confusion_matrix(y_test, y_test_pred)
 }

 return rf, metrics, importance_df

def plot_feature_importance(importance_df, top_n=15):
 """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""

 plt.figure(figsize=(10, 8))
 top_features = importance_df.head(top_n)

 sns.barplot(data=top_features, x='importance', y='feature')
 plt.title(f'–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (Top {top_n})')
 plt.xlabel('–í–∞–∂–Ω–æ—Å—Ç—å')
 plt.ylabel('–ü—Ä–∏–∑–Ω–∞–∫–∏')
 plt.tight_layout()
 plt.show()

# example –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
def example_random_forest_usage():
 """example –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Random Forest"""

 # create —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö for –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
 np.random.seed(42)
 n_samples, n_features = 1000, 20

 # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
 X = np.random.randn(n_samples, n_features)

 # create —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π with –Ω–µ–∫–æ—Ç–æ—Ä–æ–π –ª–æ–≥–∏–∫–æ–π
 y = np.zeros(n_samples)
 for i in range(n_samples):
 if X[i, 0] > 0.5 and X[i, 1] < -0.3:
 y[i] = 1 # –ö–ª–∞—Å—Å 1
 elif X[i, 2] > 1.0 or X[i, 3] < -1.0:
 y[i] = 2 # –ö–ª–∞—Å—Å 2
 else:
 y[i] = 0 # –ö–ª–∞—Å—Å 0

 print("=== example –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Random Forest ===")

 # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
 model, metrics, importance_df = train_random_forest(X, y)

 # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
 plot_feature_importance(importance_df)

 return model, metrics

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–∞ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ for —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
# model, metrics = example_random_forest_usage()
```

### 2. XGBoost

**–¢–µ–æ—Ä–∏—è:** XGBoost (eXtreme Gradient Boosting) - —ç—Ç–æ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –±—É—Å—Ç–∏–Ω–≥–∞, –∫–æ—Ç–æ—Ä–∞—è –æ—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞ for —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –±–ª–∞–≥–æ–¥–∞—Ä—è —Å–≤–æ–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ dependencies and –≤—ã–±—Ä–æ—Å—ã.

**–î–µ—Ç–∞–ª—å–Ω–∞—è —Ç–µ–æ—Ä–∏—è XGBoost:**

**–ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã:**
1. **Gradient Boosting:** –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –¥–æ–±–∞–≤–ª—è–µ—Ç –¥–µ—Ä–µ–≤—å—è, –∫–∞–∂–¥–æ–µ –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç –æ—à–∏–±–∫–∏ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö
2. **Regularization:** –ò—Å–ø–æ–ª—å–∑—É–µ—Ç L1 and L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é for –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
3. **Parallel Processing:** –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω for –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
4. **Missing Value Handling:** –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è

**–ü–æ—á–µ–º—É XGBoost —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω for —Ñ–∏–Ω–∞–Ω—Å–æ–≤:**
- **–í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å:** –ß–∞—Å—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã on —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- **–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤:** –£—Å—Ç–æ–π—á–∏–≤ –∫ –∞–Ω–æ–º–∞–ª—å–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º
- **Feature Importance:** –ü–æ–∑–≤–æ–ª—è–µ—Ç –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
- **–ë—ã—Å—Ç—Ä–æ—Ç–∞:** –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω for —Å–∫–æ—Ä–æ—Å—Ç–∏
- **–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è:** –í—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –∑–∞—â–∏—Ç–∞ from –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è

**–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Å–Ω–æ–≤–∞:**
- **Objective Function:** L(œÜ) = Œ£ l(yi, ≈∑i) + Œ£ Œ©(fk)
- **Gradient Boosting:** F_m(x) = F_{m-1}(x) + Œ≥_m * h_m(x)
- **Regularization:** Œ©(f) = Œ≥T + (1/2)Œª||w||¬≤

**–ö–ª—é—á–µ–≤—ã–µ parameters:**
- `learning_rate`: –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (0.01-0.3)
- `max_depth`: –ì–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤—å–µ–≤ (3-10)
- `n_estimators`: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±—É—Å—Ç–µ—Ä–æ–≤ (50-1000)
- `subsample`: –î–æ–ª—è –æ–±—Ä–∞–∑—Ü–æ–≤ for –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞ (0.6-1.0)
- `colsample_bytree`: –î–æ–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ for –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞ (0.6-1.0)

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è XGBoost:**

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç –∫–æ–¥:**
1. **configuration –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:** –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç parameters for —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
2. **Early Stopping:** –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –≤–∞–ª–∏–¥–∞—Ü–∏—é
3. **–û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:** –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç—Ä–∏–∫–∏, –ø–æ–¥—Ö–æ–¥—è—â–∏–µ for —Ñ–∏–Ω–∞–Ω—Å–æ–≤
4. **Feature Importance:** –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

```python
import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

def train_xgboost(X, y, test_size=0.2, random_state=42, early_stopping_rounds=10):
 """
 –û–±—É—á–µ–Ω–∏–µ XGBoost for —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö

 Args:
 X (array-like): –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (samples, features)
 y (array-like): –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (samples,)
 test_size (float): –î–æ–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö (0.0-1.0)
 random_state (int): Seed for –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
 early_stopping_rounds (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞—É–Ω–¥–æ–≤ for early stopping

 Returns:
 tuple: (–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å, –º–µ—Ç—Ä–∏–∫–∏, –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
 """

 print("=== –û–±—É—á–µ–Ω–∏–µ XGBoost ===")
 print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {X.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤, {X.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
 print(f"–ö–ª–∞—Å—Å—ã: {np.unique(y, return_counts=True)}")

 # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
 X_train, X_test, y_train, y_test = train_test_split(
 X, y, test_size=test_size, random_state=random_state, stratify=y
 )

 print(f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
 print(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")

 # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ parameters for —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
 params = {
 'objective': 'multi:softprob', # –ú–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è with –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏
 'num_class': len(np.unique(y)), # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤
 'max_depth': 6, # –ì–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤—å–µ–≤ (–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ)
 'learning_rate': 0.1, # –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
 'n_estimators': 100, # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±—É—Å—Ç–µ—Ä–æ–≤
 'subsample': 0.8, # –î–æ–ª—è –æ–±—Ä–∞–∑—Ü–æ–≤ for –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞
 'colsample_bytree': 0.8, # –î–æ–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ for –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞
 'reg_alpha': 0.1, # L1 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
 'reg_lambda': 1.0, # L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
 'random_state': random_state,
 'n_jobs': -1, # parallel training
 'verbosity': 0 # –û—Ç–∫–ª—é—á–∏—Ç—å –≤—ã–≤–æ–¥
 }

 print("\n–ü–∞—Ä–∞–º–µ—Ç—Ä—ã XGBoost:")
 for key, value in params.items():
 print(f"{key}: {value}")

 # create –º–æ–¥–µ–ª–∏
 xgb_model = xgb.XGBClassifier(**params)

 # –û–±—É—á–µ–Ω–∏–µ with early stopping
 print("\n–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
 xgb_model.fit(
 X_train, y_train,
 eval_set=[(X_test, y_test)],
 early_stopping_rounds=early_stopping_rounds,
 verbose=False
 )

 # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
 y_train_pred = xgb_model.predict(X_train)
 y_test_pred = xgb_model.predict(X_test)
 y_test_proba = xgb_model.predict_proba(X_test)

 # –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
 train_accuracy = accuracy_score(y_train, y_train_pred)
 test_accuracy = accuracy_score(y_test, y_test_pred)

 print(f"\n=== –†–µ–∑—É–ª—å—Ç–∞—Ç—ã ===")
 print(f"Train accuracy: {train_accuracy:.4f}")
 print(f"Test accuracy: {test_accuracy:.4f}")
 print(f"Best iteration: {xgb_model.best_iteration}")
 print(f"Best score: {xgb_model.best_score:.4f}")

 # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
 print(f"\n=== Classification Report (Test) ===")
 print(classification_report(y_test, y_test_pred))

 # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
 feature_importance = xgb_model.feature_importances_
 feature_names = [f'feature_{i}' for i in range(X.shape[1])]

 # create DataFrame with –≤–∞–∂–Ω–æ—Å—Ç—å—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
 importance_df = pd.DataFrame({
 'feature': feature_names,
 'importance': feature_importance
 }).sort_values('importance', ascending=False)

 print(f"\n=== –¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ===")
 print(importance_df.head(10))

 # –ú–µ—Ç—Ä–∏–∫–∏ for –≤–æ–∑–≤—Ä–∞—Ç–∞
 metrics = {
 'train_accuracy': train_accuracy,
 'test_accuracy': test_accuracy,
 'best_iteration': xgb_model.best_iteration,
 'best_score': xgb_model.best_score,
 'feature_importance': importance_df,
 'confusion_matrix': confusion_matrix(y_test, y_test_pred),
 'Predictions': y_test_pred,
 'probabilities': y_test_proba
 }

 return xgb_model, metrics, importance_df

def plot_xgboost_importance(importance_df, top_n=15):
 """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ XGBoost"""

 plt.figure(figsize=(12, 8))
 top_features = importance_df.head(top_n)

 sns.barplot(data=top_features, x='importance', y='feature')
 plt.title(f'–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ XGBoost (Top {top_n})')
 plt.xlabel('–í–∞–∂–Ω–æ—Å—Ç—å')
 plt.ylabel('–ü—Ä–∏–∑–Ω–∞–∫–∏')
 plt.tight_layout()
 plt.show()

def plot_learning_curve(model, X_train, y_train, X_test, y_test):
 """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫—Ä–∏–≤–æ–π –æ–±—É—á–µ–Ω–∏—è"""

 # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è
 results = model.evals_result()

 plt.figure(figsize=(12, 4))

 # –ì—Ä–∞—Ñ–∏–∫ –æ—à–∏–±–∫–∏
 plt.subplot(1, 2, 1)
 plt.plot(results['validation_0']['mlogloss'], label='Train')
 plt.plot(results['validation_1']['mlogloss'], label='Test')
 plt.title('Learning Curve - Log Loss')
 plt.xlabel('Iterations')
 plt.ylabel('Log Loss')
 plt.legend()
 plt.grid(True)

 # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏
 plt.subplot(1, 2, 2)
 train_acc = [1 - x for x in results['validation_0']['mlogloss']]
 test_acc = [1 - x for x in results['validation_1']['mlogloss']]
 plt.plot(train_acc, label='Train')
 plt.plot(test_acc, label='Test')
 plt.title('Learning Curve - Accuracy')
 plt.xlabel('Iterations')
 plt.ylabel('Accuracy')
 plt.legend()
 plt.grid(True)

 plt.tight_layout()
 plt.show()

# example –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
def example_xgboost_usage():
 """example –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è XGBoost"""

 # create —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö for –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
 np.random.seed(42)
 n_samples, n_features = 1000, 20

 # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ with –Ω–µ–∫–æ—Ç–æ—Ä–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
 X = np.random.randn(n_samples, n_features)

 # create —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π with –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–º–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏
 y = np.zeros(n_samples)
 for i in range(n_samples):
 # –°–ª–æ–∂–Ω–∞—è –Ω–µ–ª–∏–Ω–µ–π–Ω–∞—è dependency
 score = (X[i, 0] ** 2 + X[i, 1] * X[i, 2] +
 np.sin(X[i, 3]) + X[i, 4] * X[i, 5])

 if score > 2.0:
 y[i] = 2 # –ö–ª–∞—Å—Å 2
 elif score > 0.5:
 y[i] = 1 # –ö–ª–∞—Å—Å 1
 else:
 y[i] = 0 # –ö–ª–∞—Å—Å 0

 print("=== example –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è XGBoost ===")

 # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
 model, metrics, importance_df = train_xgboost(X, y)

 # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
 plot_xgboost_importance(importance_df)

 return model, metrics

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–∞ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ for —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
# model, metrics = example_xgboost_usage()
```

### 3. LightGBM

**–¢–µ–æ—Ä–∏—è:** LightGBM (Light Gradient Boosting Machine) - —ç—Ç–æ –±—ã—Å—Ç—Ä–∞—è and —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –±—É—Å—Ç–∏–Ω–≥–∞, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è Microsoft. –û—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞ for –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ and —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –±–ª–∞–≥–æ–¥–∞—Ä—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É –∞–ª–≥–æ—Ä–∏—Ç–º—É –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –¥–µ—Ä–µ–≤—å–µ–≤.

**–î–µ—Ç–∞–ª—å–Ω–∞—è —Ç–µ–æ—Ä–∏—è LightGBM:**

**–ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã:**
1. **Leaf-wise Growth:** –°—Ç—Ä–æ–∏—Ç –¥–µ—Ä–µ–≤—å—è on –ª–∏—Å—Ç—å—è–º, –∞ not on —É—Ä–æ–≤–Ω—è–º (–∫–∞–∫ XGBoost)
2. **Gradient-based one-Side Sampling (GOSS):** –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ –æ–±—Ä–∞–∑—Ü—ã with –±–æ–ª—å—à–∏–º–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏
3. **Exclusive Feature Bundling (EFB):** –ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç –≤–∑–∞–∏–º–Ω–æ –∏—Å–∫–ª—é—á–∞—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
4. **Categorical Feature Support:** –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏

**–ü–æ—á–µ–º—É LightGBM —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω for —Ñ–∏–Ω–∞–Ω—Å–æ–≤:**
- **–°–∫–æ—Ä–æ—Å—Ç—å:** in 10-100 —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ XGBoost on –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö
- **–ü–∞–º—è—Ç—å:** –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏ –±–ª–∞–≥–æ–¥–∞—Ä—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º
- **–¢–æ—á–Ω–æ—Å—Ç—å:** –ß–∞—Å—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
- **–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:** –û—Ç–ª–∏—á–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç with —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏
- **–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è:** –í—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –∑–∞—â–∏—Ç–∞ from –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è

**–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Å–Ω–æ–≤–∞:**
- **Leaf-wise Growth:** –í—ã–±–∏—Ä–∞–µ—Ç –ª–∏—Å—Ç with –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –ø—Ä–∏—Ä–æ—Å—Ç–æ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
- **GOSS:** –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ø-a% –æ–±—Ä–∞–∑—Ü–æ–≤ with –±–æ–ª—å—à–∏–º–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏ + —Å–ª—É—á–∞–π–Ω—ã–µ b% –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
- **EFB:** –ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ with –Ω–∏–∑–∫–æ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π

**–ö–ª—é—á–µ–≤—ã–µ parameters:**
- `num_leaves`: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏—Å—Ç—å–µ–≤ (31-255)
- `learning_rate`: –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (0.01-0.3)
- `feature_fraction`: –î–æ–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (0.6-1.0)
- `bagging_fraction`: –î–æ–ª—è –æ–±—Ä–∞–∑—Ü–æ–≤ (0.6-1.0)
- `min_data_in_leaf`: –ú–∏–Ω–∏–º—É–º –¥–∞–Ω–Ω—ã—Ö in –ª–∏—Å—Ç–µ (20-100)

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è LightGBM:**

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç –∫–æ–¥:**
1. **–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ parameters:** configuration for —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
2. **Early Stopping:** –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
3. **–í–∞–ª–∏–¥–∞—Ü–∏—è:** –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
4. **Feature Importance:** –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

```python
import numpy as np
import pandas as pd
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

def train_lightgbm(X, y, test_size=0.2, random_state=42, early_stopping_rounds=10):
 """
 –û–±—É—á–µ–Ω–∏–µ LightGBM for —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö

 Args:
 X (array-like): –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (samples, features)
 y (array-like): –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (samples,)
 test_size (float): –î–æ–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö (0.0-1.0)
 random_state (int): Seed for –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
 early_stopping_rounds (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞—É–Ω–¥–æ–≤ for early stopping

 Returns:
 tuple: (–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å, –º–µ—Ç—Ä–∏–∫–∏, –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
 """

 print("=== –û–±—É—á–µ–Ω–∏–µ LightGBM ===")
 print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {X.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤, {X.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
 print(f"–ö–ª–∞—Å—Å—ã: {np.unique(y, return_counts=True)}")

 # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
 X_train, X_test, y_train, y_test = train_test_split(
 X, y, test_size=test_size, random_state=random_state, stratify=y
 )

 print(f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
 print(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")

 # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ parameters for —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
 params = {
 'objective': 'multiclass', # –ú–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
 'num_class': len(np.unique(y)), # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤
 'boosting_type': 'gbdt', # –¢–∏–ø –±—É—Å—Ç–∏–Ω–≥–∞ (Gradient Boosting Decision Tree)
 'num_leaves': 31, # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏—Å—Ç—å–µ–≤ (2^max_depth - 1)
 'learning_rate': 0.05, # –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
 'feature_fraction': 0.9, # –î–æ–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ for –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞
 'bagging_fraction': 0.8, # –î–æ–ª—è –æ–±—Ä–∞–∑—Ü–æ–≤ for –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞
 'bagging_freq': 5, # –ß–∞—Å—Ç–æ—Ç–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è bagging
 'min_data_in_leaf': 20, # –ú–∏–Ω–∏–º—É–º –¥–∞–Ω–Ω—ã—Ö in –ª–∏—Å—Ç–µ
 'min_sum_hessian_in_leaf': 1e-3, # –ú–∏–Ω–∏–º—É–º —Å—É–º–º—ã –≥–µ—Å—Å–∏–∞–Ω–æ–≤ in –ª–∏—Å—Ç–µ
 'lambda_l1': 0.1, # L1 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
 'lambda_l2': 1.0, # L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
 'min_gain_to_split': 0.0, # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–∏—Ä–æ—Å—Ç for —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
 'max_depth': -1, # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ (-1 = –Ω–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ)
 'save_binary': True, # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –±–∏–Ω–∞—Ä–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
 'seed': random_state, # Seed for –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
 'feature_fraction_seed': random_state,
 'bagging_seed': random_state,
 'drop_seed': random_state,
 'data_random_seed': random_state,
 'verbose': -1, # –û—Ç–∫–ª—é—á–∏—Ç—å –≤—ã–≤–æ–¥
 'n_jobs': -1 # parallel training
 }

 print("\n–ü–∞—Ä–∞–º–µ—Ç—Ä—ã LightGBM:")
 for key, value in params.items():
 print(f"{key}: {value}")

 # create –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ LightGBM
 train_data = lgb.Dataset(X_train, label=y_train)
 test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)

 # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
 print("\n–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
 model = lgb.train(
 params,
 train_data,
 valid_sets=[test_data],
 num_boost_round=100,
 callbacks=[
 lgb.early_stopping(early_stopping_rounds),
 lgb.log_evaluation(0) # –û—Ç–∫–ª—é—á–∏—Ç—å –≤—ã–≤–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
 ]
 )

 # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
 y_train_pred = model.predict(X_train, num_iteration=model.best_iteration)
 y_test_pred = model.predict(X_test, num_iteration=model.best_iteration)

 # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π in –∫–ª–∞—Å—Å—ã
 y_train_pred_class = np.argmax(y_train_pred, axis=1)
 y_test_pred_class = np.argmax(y_test_pred, axis=1)

 # –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
 train_accuracy = accuracy_score(y_train, y_train_pred_class)
 test_accuracy = accuracy_score(y_test, y_test_pred_class)

 print(f"\n=== –†–µ–∑—É–ª—å—Ç–∞—Ç—ã ===")
 print(f"Train accuracy: {train_accuracy:.4f}")
 print(f"Test accuracy: {test_accuracy:.4f}")
 print(f"Best iteration: {model.best_iteration}")

 # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
 print(f"\n=== Classification Report (Test) ===")
 print(classification_report(y_test, y_test_pred_class))

 # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
 feature_importance = model.feature_importance(importance_type='gain')
 feature_names = [f'feature_{i}' for i in range(X.shape[1])]

 # create DataFrame with –≤–∞–∂–Ω–æ—Å—Ç—å—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
 importance_df = pd.DataFrame({
 'feature': feature_names,
 'importance': feature_importance
 }).sort_values('importance', ascending=False)

 print(f"\n=== –¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ===")
 print(importance_df.head(10))

 # –ú–µ—Ç—Ä–∏–∫–∏ for –≤–æ–∑–≤—Ä–∞—Ç–∞
 metrics = {
 'train_accuracy': train_accuracy,
 'test_accuracy': test_accuracy,
 'best_iteration': model.best_iteration,
 'feature_importance': importance_df,
 'confusion_matrix': confusion_matrix(y_test, y_test_pred_class),
 'Predictions': y_test_pred_class,
 'probabilities': y_test_pred
 }

 return model, metrics, importance_df

def plot_lightgbm_importance(importance_df, top_n=15):
 """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ LightGBM"""

 plt.figure(figsize=(12, 8))
 top_features = importance_df.head(top_n)

 sns.barplot(data=top_features, x='importance', y='feature')
 plt.title(f'–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ LightGBM (Top {top_n})')
 plt.xlabel('–í–∞–∂–Ω–æ—Å—Ç—å')
 plt.ylabel('–ü—Ä–∏–∑–Ω–∞–∫–∏')
 plt.tight_layout()
 plt.show()

def plot_lightgbm_learning_curve(model):
 """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫—Ä–∏–≤–æ–π –æ–±—É—á–µ–Ω–∏—è LightGBM"""

 # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è
 history = model.evals_result_

 plt.figure(figsize=(12, 4))

 # –ì—Ä–∞—Ñ–∏–∫ –æ—à–∏–±–∫–∏
 plt.subplot(1, 2, 1)
 train_loss = history['training']['multi_logloss']
 valid_loss = history['valid_0']['multi_logloss']

 plt.plot(train_loss, label='Train')
 plt.plot(valid_loss, label='Validation')
 plt.title('Learning Curve - Multi Log Loss')
 plt.xlabel('Iterations')
 plt.ylabel('Multi Log Loss')
 plt.legend()
 plt.grid(True)

 # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏
 plt.subplot(1, 2, 2)
 train_acc = [1 - x for x in train_loss]
 valid_acc = [1 - x for x in valid_loss]

 plt.plot(train_acc, label='Train')
 plt.plot(valid_acc, label='Validation')
 plt.title('Learning Curve - Accuracy')
 plt.xlabel('Iterations')
 plt.ylabel('Accuracy')
 plt.legend()
 plt.grid(True)

 plt.tight_layout()
 plt.show()

# example –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
def example_lightgbm_usage():
 """example –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è LightGBM"""

 # create —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö for –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
 np.random.seed(42)
 n_samples, n_features = 1000, 20

 # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ with –Ω–µ–∫–æ—Ç–æ—Ä–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
 X = np.random.randn(n_samples, n_features)

 # create —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π with –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–º–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏
 y = np.zeros(n_samples)
 for i in range(n_samples):
 # –°–ª–æ–∂–Ω–∞—è –Ω–µ–ª–∏–Ω–µ–π–Ω–∞—è dependency
 score = (X[i, 0] ** 2 + X[i, 1] * X[i, 2] +
 np.sin(X[i, 3]) + X[i, 4] * X[i, 5])

 if score > 2.0:
 y[i] = 2 # –ö–ª–∞—Å—Å 2
 elif score > 0.5:
 y[i] = 1 # –ö–ª–∞—Å—Å 1
 else:
 y[i] = 0 # –ö–ª–∞—Å—Å 0

 print("=== example –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è LightGBM ===")

 # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
 model, metrics, importance_df = train_lightgbm(X, y)

 # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
 plot_lightgbm_importance(importance_df)

 # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫—Ä–∏–≤–æ–π –æ–±—É—á–µ–Ω–∏—è
 plot_lightgbm_learning_curve(model)

 return model, metrics

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–∞ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ for —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
# model, metrics = example_lightgbm_usage()
```

## –ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏

**–¢–µ–æ—Ä–∏—è:** –ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ - —ç—Ç–æ –º–æ—â–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç for –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –Ω–µ–ª–∏–Ω–µ–π–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π in —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –û–Ω–∏ –æ—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã for –≤—ã—è–≤–ª–µ–Ω–∏—è —Å–∫—Ä—ã—Ç—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ and –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –º–µ–∂–¥—É –ø—Ä–∏sign–º–∏.

**–ü–æ—á–µ–º—É –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã for —Ñ–∏–Ω–∞–Ω—Å–æ–≤:**
- **–ù–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å:** –ú–æ–≥—É—Ç –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ dependencies
- **–í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è:** –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã—è–≤–ª—è—é—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–µ–∂–¥—É –ø—Ä–∏sign–º–∏
- **–ê–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å:** –ú–æ–≥—É—Ç –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ –∏–∑–º–µ–Ω—è—é—â–∏–º—Å—è —Ä—ã–Ω–æ—á–Ω—ã–º —É—Å–ª–æ–≤–∏—è–º
- **–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å:** –•–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞—é—Ç with –±–æ–ª—å—à–∏–º–∏ –æ–±—ä–µ–º–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö

### 1. –ü—Ä–æ—Å—Ç–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å

**–¢–µ–æ—Ä–∏—è:** –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å (Multi-Layer Perceptron) —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–ª–æ–µ–≤ –Ω–µ–π—Ä–æ–Ω–æ–≤, —Å–æ–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –≤–µ—Å–∞–º–∏. –ö–∞–∂–¥—ã–π –Ω–µ–π—Ä–æ–Ω –ø—Ä–∏–º–µ–Ω—è–µ—Ç –Ω–µ–ª–∏–Ω–µ–π–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –∫ –≤–∑–≤–µ—à–µ–Ω–Ω–æ–π —Å—É–º–º–µ –≤—Ö–æ–¥–æ–≤.

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–µ—Ç–∏:**
- **–í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π:** –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ–Ω–æ–≤ = –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
- **–°–∫—Ä—ã—Ç—ã–µ —Å–ª–æ–∏:** 2-3 —Å–ª–æ—è with 64-256 –Ω–µ–π—Ä–æ–Ω–∞–º–∏ –∫–∞–∂–¥—ã–π
- **–í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π:** –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ–Ω–æ–≤ = –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤
- **Dropout:** –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è for –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
- **–ê–∫—Ç–∏–≤–∞—Ü–∏—è:** ReLU for —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ–µ–≤, Softmax for –≤—ã—Ö–æ–¥–Ω–æ–≥–æ

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:**

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç –∫–æ–¥:**
1. **create –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã:** –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏
2. **–û–±—É—á–µ–Ω–∏–µ:** –ò—Å–ø–æ–ª—å–∑—É–µ—Ç backpropagation for –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤–µ—Å–æ–≤
3. **–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è:** –ü—Ä–∏–º–µ–Ω—è–µ—Ç dropout for –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
4. **–í–∞–ª–∏–¥–∞—Ü–∏—è:** –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å in –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split

class TradingNN(nn.Module):
 """
 –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å for —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π

 –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
 - –í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π: input_size –Ω–µ–π—Ä–æ–Ω–æ–≤
 - 3 —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—è: hidden_size –Ω–µ–π—Ä–æ–Ω–æ–≤ –∫–∞–∂–¥—ã–π
 - –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π: num_classes –Ω–µ–π—Ä–æ–Ω–æ–≤
 - Dropout: 0.2 for —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
 - –ê–∫—Ç–∏–≤–∞—Ü–∏—è: ReLU for —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ–µ–≤
 """

 def __init__(self, input_size, hidden_size=128, num_classes=3, dropout_rate=0.2):
 super(TradingNN, self).__init__()

 # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ª–æ–µ–≤
 self.fc1 = nn.Linear(input_size, hidden_size)
 self.fc2 = nn.Linear(hidden_size, hidden_size)
 self.fc3 = nn.Linear(hidden_size, hidden_size)
 self.fc4 = nn.Linear(hidden_size, num_classes)

 # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
 self.dropout = nn.Dropout(dropout_rate)

 # function –∞–∫—Ç–∏–≤–∞—Ü–∏–∏
 self.relu = nn.ReLU()

 # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
 self._initialize_weights()

 def _initialize_weights(self):
 """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ for –ª—É—á—à–µ–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏"""
 for m in self.modules():
 if isinstance(m, nn.Linear):
 nn.init.xavier_uniform_(m.weight)
 nn.init.constant_(m.bias, 0)

 def forward(self, x):
 """–ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ —Å–µ—Ç—å"""
 # –ü–µ—Ä–≤—ã–π —Å–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π
 x = self.relu(self.fc1(x))
 x = self.dropout(x)

 # –í—Ç–æ—Ä–æ–π —Å–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π
 x = self.relu(self.fc2(x))
 x = self.dropout(x)

 # –¢—Ä–µ—Ç–∏–π —Å–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π
 x = self.relu(self.fc3(x))
 x = self.dropout(x)

 # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π (–±–µ–∑ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ - –±—É–¥–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω in loss function)
 x = self.fc4(x)

 return x

def train_neural_network(X, y, epochs=100, batch_size=32, learning_rate=0.001,
 test_size=0.2, random_state=42):
 """
 –û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ for —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π

 Args:
 X (array-like): –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
 y (array-like): –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
 epochs (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è
 batch_size (int): –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
 learning_rate (float): –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
 test_size (float): –î–æ–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
 random_state (int): Seed for –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏

 Returns:
 tuple: (–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å, –º–µ—Ç—Ä–∏–∫–∏, –∏—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è)
 """

 print("=== –û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ ===")
 print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {X.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤, {X.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
 print(f"–ö–ª–∞—Å—Å—ã: {np.unique(y, return_counts=True)}")

 # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
 X_train, X_test, y_train, y_test = train_test_split(
 X, y, test_size=test_size, random_state=random_state, stratify=y
 )

 print(f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
 print(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")

 # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ in —Ç–µ–Ω–∑–æ—Ä—ã PyTorch
 X_train_tensor = torch.FloatTensor(X_train)
 y_train_tensor = torch.LongTensor(y_train)
 X_test_tensor = torch.FloatTensor(X_test)
 y_test_tensor = torch.LongTensor(y_test)

 # create –¥–∞—Ç–∞—Å–µ—Ç–∞ and DataLoader
 train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
 train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

 # create –º–æ–¥–µ–ª–∏
 model = TradingNN(X.shape[1], num_classes=len(np.unique(y)))

 # function –ø–æ—Ç–µ—Ä—å and –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
 criterion = nn.CrossEntropyLoss()
 optimizer = optim.Adam(model.parameters(), lr=learning_rate)

 # –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
 train_losses = []
 train_accuracies = []
 test_accuracies = []

 print(f"\n–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:")
 print(f"Epochs: {epochs}")
 print(f"Batch size: {batch_size}")
 print(f"Learning rate: {learning_rate}")
 print(f"Model parameters: {sum(p.numel() for p in model.parameters())}")

 # –û–±—É—á–µ–Ω–∏–µ
 print("\n–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
 model.train()

 for epoch in range(epochs):
 epoch_loss = 0.0
 correct = 0
 total = 0

 for batch_X, batch_y in train_dataloader:
 # –û–±–Ω—É–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
 optimizer.zero_grad()

 # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
 outputs = model(batch_X)
 loss = criterion(outputs, batch_y)

 # –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥
 loss.backward()
 optimizer.step()

 # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
 epoch_loss += loss.item()
 _, predicted = torch.max(outputs.data, 1)
 total += batch_y.size(0)
 correct += (predicted == batch_y).sum().item()

 # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
 avg_loss = epoch_loss / len(train_dataloader)
 train_accuracy = correct / total

 train_losses.append(avg_loss)
 train_accuracies.append(train_accuracy)

 # –û—Ü–µ–Ω–∫–∞ on —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
 model.eval()
 with torch.no_grad():
 test_outputs = model(X_test_tensor)
 _, test_predicted = torch.max(test_outputs.data, 1)
 test_accuracy = accuracy_score(y_test, test_predicted.numpy())
 test_accuracies.append(test_accuracy)
 model.train()

 # –í—ã–≤–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
 if epoch % 10 == 0 or epoch == epochs - 1:
 print(f'Epoch {epoch:3d}/{epochs}: '
 f'Loss: {avg_loss:.4f}, '
 f'Train Acc: {train_accuracy:.4f}, '
 f'Test Acc: {test_accuracy:.4f}')

 # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
 model.eval()
 with torch.no_grad():
 test_outputs = model(X_test_tensor)
 _, test_predicted = torch.max(test_outputs.data, 1)
 test_Predictions = test_predicted.numpy()
 test_probabilities = torch.softmax(test_outputs, dim=1).numpy()

 # –ú–µ—Ç—Ä–∏–∫–∏
 final_accuracy = accuracy_score(y_test, test_Predictions)

 print(f"\n=== –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã ===")
 print(f"Test accuracy: {final_accuracy:.4f}")

 # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
 print(f"\n=== Classification Report ===")
 print(classification_report(y_test, test_Predictions))

 # –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
 history = {
 'train_losses': train_losses,
 'train_accuracies': train_accuracies,
 'test_accuracies': test_accuracies
 }

 # –ú–µ—Ç—Ä–∏–∫–∏ for –≤–æ–∑–≤—Ä–∞—Ç–∞
 metrics = {
 'test_accuracy': final_accuracy,
 'confusion_matrix': confusion_matrix(y_test, test_Predictions),
 'Predictions': test_Predictions,
 'probabilities': test_probabilities,
 'history': history
 }

 return model, metrics, history

def plot_training_history(history):
 """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è"""

 fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

 # –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å
 ax1.plot(history['train_losses'], label='Train Loss')
 ax1.set_title('Training Loss')
 ax1.set_xlabel('Epoch')
 ax1.set_ylabel('Loss')
 ax1.legend()
 ax1.grid(True)

 # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏
 ax2.plot(history['train_accuracies'], label='Train Accuracy')
 ax2.plot(history['test_accuracies'], label='Test Accuracy')
 ax2.set_title('Training and Test Accuracy')
 ax2.set_xlabel('Epoch')
 ax2.set_ylabel('Accuracy')
 ax2.legend()
 ax2.grid(True)

 plt.tight_layout()
 plt.show()

# example –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
def example_neural_network_usage():
 """example –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏"""

 # create —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
 np.random.seed(42)
 n_samples, n_features = 1000, 20

 # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
 X = np.random.randn(n_samples, n_features)

 # create —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π with –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–º–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏
 y = np.zeros(n_samples)
 for i in range(n_samples):
 # –°–ª–æ–∂–Ω–∞—è –Ω–µ–ª–∏–Ω–µ–π–Ω–∞—è dependency
 score = (X[i, 0] ** 2 + X[i, 1] * X[i, 2] +
 np.sin(X[i, 3]) + X[i, 4] * X[i, 5])

 if score > 2.0:
 y[i] = 2 # –ö–ª–∞—Å—Å 2
 elif score > 0.5:
 y[i] = 1 # –ö–ª–∞—Å—Å 1
 else:
 y[i] = 0 # –ö–ª–∞—Å—Å 0

 print("=== example –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ ===")

 # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
 model, metrics, history = train_neural_network(X, y, epochs=50)

 # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è
 plot_training_history(history)

 return model, metrics

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–∞ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ for —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
# model, metrics = example_neural_network_usage()
```

### 2. LSTM for –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤

**–¢–µ–æ—Ä–∏—è:** LSTM (Long Short-Term Memory) - —ç—Ç–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ç–∏–ø —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π for —Ä–∞–±–æ—Ç—ã with –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º–∏. LSTM –æ—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω for —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —Ç–∞–∫ –∫–∞–∫ –º–æ–∂–µ—Ç –∑–∞–ø–æ–º–∏–Ω–∞—Ç—å –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ dependencies and –ø–∞—Ç—Ç–µ—Ä–Ω—ã.

**–î–µ—Ç–∞–ª—å–Ω–∞—è —Ç–µ–æ—Ä–∏—è LSTM:**

**–ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã:**
1. **–ó–∞–±—ã–≤–∞—é—â–∏–π –≥–µ–π—Ç (Forget Gate):** –†–µ—à–∞–µ—Ç, –∫–∞–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∑–∞–±—ã—Ç—å –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
2. **–í—Ö–æ–¥–Ω–æ–π –≥–µ–π—Ç (Input Gate):** –†–µ—à–∞–µ—Ç, –∫–∞–∫—É—é –Ω–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å
3. **–ì–µ–π—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è (Update Gate):** –û–±–Ω–æ–≤–ª—è–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ —è—á–µ–π–∫–∏
4. **–í—ã—Ö–æ–¥–Ω–æ–π –≥–µ–π—Ç (Output Gate):** –†–µ—à–∞–µ—Ç, –∫–∞–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤—ã–≤–µ—Å—Ç–∏

**–ü–æ—á–µ–º—É LSTM —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω for —Ñ–∏–Ω–∞–Ω—Å–æ–≤:**
- **–í—Ä–µ–º–µ–Ω–Ω—ã–µ dependencies:** –ú–æ–∂–µ—Ç –∑–∞–ø–æ–º–∏–Ω–∞—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã on –¥–ª–∏–Ω–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞—Ö
- **–£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–º—É –∏—Å—á–µ–∑–Ω–æ–≤–µ–Ω–∏—é:** –†–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É RNN
- **–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π:** –ò–¥–µ–∞–ª—å–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç for –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
- **–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:** –£—á–∏—Ç—ã–≤–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é for –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π

**–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Å–Ω–æ–≤–∞:**
- **Forget Gate:** f_t = œÉ(W_f ¬∑ [h_{t-1}, x_t] + b_f)
- **Input Gate:** i_t = œÉ(W_i ¬∑ [h_{t-1}, x_t] + b_i)
- **Cell State:** CÃÉ_t = tanh(W_C ¬∑ [h_{t-1}, x_t] + b_C)
- **Update:** C_t = f_t * C_{t-1} + i_t * CÃÉ_t
- **Output Gate:** o_t = œÉ(W_o ¬∑ [h_{t-1}, x_t] + b_o)
- **Hidden State:** h_t = o_t * tanh(C_t)

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è LSTM:**

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç –∫–æ–¥:**
1. **create –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π:** –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ in —Ñ–æ—Ä–º–∞—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
2. **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ LSTM:** –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ–π —Å–µ—Ç–∏
3. **–û–±—É—á–µ–Ω–∏–µ:** –ò—Å–ø–æ–ª—å–∑—É–µ—Ç backpropagation through time (BPTT)
4. **–í–∞–ª–∏–¥–∞—Ü–∏—è:** –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å on –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split

class LSTMTradingModel(nn.Module):
 """
 LSTM –º–æ–¥–µ–ª—å for —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π on –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–∞—Ö

 –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
 - LSTM —Å–ª–æ–∏: for –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
 - Dropout: for —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
 - –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π: for —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
 """

 def __init__(self, input_size, hidden_size=64, num_layers=2, num_classes=3, dropout_rate=0.2):
 super(LSTMTradingModel, self).__init__()

 self.hidden_size = hidden_size
 self.num_layers = num_layers

 # LSTM —Å–ª–æ–∏
 self.lstm = nn.LSTM(
 input_size=input_size,
 hidden_size=hidden_size,
 num_layers=num_layers,
 batch_first=True,
 dropout=dropout_rate if num_layers > 1 else 0,
 bidirectional=False
 )

 # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π for –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
 self.fc = nn.Linear(hidden_size, num_classes)

 # Dropout for —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
 self.dropout = nn.Dropout(dropout_rate)

 # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
 self._initialize_weights()

 def _initialize_weights(self):
 """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ LSTM"""
 for name, param in self.named_parameters():
 if 'weight_ih' in name:
 nn.init.xavier_uniform_(param.data)
 elif 'weight_hh' in name:
 nn.init.orthogonal_(param.data)
 elif 'bias' in name:
 param.data.fill_(0)
 # installation forget gate bias in 1 for –ª—É—á—à–µ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
 n = param.size(0)
 param.data[(n//4):(n//2)].fill_(1)

 def forward(self, x):
 """
 –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ LSTM

 Args:
 x: –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ñ–æ—Ä–º—ã (batch_size, sequence_length, input_size)

 Returns:
 –í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ñ–æ—Ä–º—ã (batch_size, num_classes)
 """
 batch_size = x.size(0)

 # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
 h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=x.device)
 c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=x.device)

 # LSTM forward pass
 lstm_out, (hn, cn) = self.lstm(x, (h0, c0))

 # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—ã—Ö–æ–¥ –∏–∑ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
 last_output = lstm_out[:, -1, :] # (batch_size, hidden_size)

 # –ü—Ä–∏–º–µ–Ω—è–µ–º dropout
 last_output = self.dropout(last_output)

 # –§–∏–Ω–∞–ª—å–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
 output = self.fc(last_output)

 return output

def create_sequences(X, y, sequence_length):
 """
 create –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π for LSTM

 Args:
 X: –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (samples, features)
 y: –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (samples,)
 sequence_length: –î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

 Returns:
 X_seq: –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (samples-seq_len+1, seq_len, features)
 y_seq: –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ for –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π (samples-seq_len+1,)
 """
 X_seq, y_seq = [], []

 for i in range(sequence_length, len(X)):
 X_seq.append(X[i-sequence_length:i])
 y_seq.append(y[i])

 return np.array(X_seq), np.array(y_seq)

def train_lstm_model(X, y, sequence_length=10, epochs=100, batch_size=32,
 learning_rate=0.001, test_size=0.2, random_state=42):
 """
 –û–±—É—á–µ–Ω–∏–µ LSTM –º–æ–¥–µ–ª–∏ for —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π

 Args:
 X: –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
 y: –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
 sequence_length: –î–ª–∏–Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
 epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è
 batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
 learning_rate: –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
 test_size: –î–æ–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
 random_state: Seed for –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏

 Returns:
 tuple: (–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å, –º–µ—Ç—Ä–∏–∫–∏, –∏—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è)
 """

 print("=== –û–±—É—á–µ–Ω–∏–µ LSTM –º–æ–¥–µ–ª–∏ ===")
 print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {X.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤, {X.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
 print(f"–ö–ª–∞—Å—Å—ã: {np.unique(y, return_counts=True)}")
 print(f"–î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {sequence_length}")

 # create –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
 X_seq, y_seq = create_sequences(X, y, sequence_length)
 print(f"–†–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π: {X_seq.shape}")

 # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
 X_train, X_test, y_train, y_test = train_test_split(
 X_seq, y_seq, test_size=test_size, random_state=random_state, stratify=y_seq
 )

 print(f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train.shape[0]} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
 print(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test.shape[0]} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")

 # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ in —Ç–µ–Ω–∑–æ—Ä—ã PyTorch
 X_train_tensor = torch.FloatTensor(X_train)
 y_train_tensor = torch.LongTensor(y_train)
 X_test_tensor = torch.FloatTensor(X_test)
 y_test_tensor = torch.LongTensor(y_test)

 # create –¥–∞—Ç–∞—Å–µ—Ç–∞ and DataLoader
 train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
 train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

 # create –º–æ–¥–µ–ª–∏
 model = LSTMTradingModel(
 input_size=X.shape[1],
 num_classes=len(np.unique(y)),
 hidden_size=64,
 num_layers=2
 )

 # function –ø–æ—Ç–µ—Ä—å and –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
 criterion = nn.CrossEntropyLoss()
 optimizer = optim.Adam(model.parameters(), lr=learning_rate)

 # –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
 train_losses = []
 train_accuracies = []
 test_accuracies = []

 print(f"\n–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:")
 print(f"Epochs: {epochs}")
 print(f"Batch size: {batch_size}")
 print(f"Learning rate: {learning_rate}")
 print(f"Sequence length: {sequence_length}")
 print(f"Model parameters: {sum(p.numel() for p in model.parameters())}")

 # –û–±—É—á–µ–Ω–∏–µ
 print("\n–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
 model.train()

 for epoch in range(epochs):
 epoch_loss = 0.0
 correct = 0
 total = 0

 for batch_X, batch_y in train_dataloader:
 # –û–±–Ω—É–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
 optimizer.zero_grad()

 # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
 outputs = model(batch_X)
 loss = criterion(outputs, batch_y)

 # –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥
 loss.backward()

 # –û–±—Ä–µ–∑–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ for —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
 torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

 optimizer.step()

 # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
 epoch_loss += loss.item()
 _, predicted = torch.max(outputs.data, 1)
 total += batch_y.size(0)
 correct += (predicted == batch_y).sum().item()

 # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
 avg_loss = epoch_loss / len(train_dataloader)
 train_accuracy = correct / total

 train_losses.append(avg_loss)
 train_accuracies.append(train_accuracy)

 # –û—Ü–µ–Ω–∫–∞ on —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
 model.eval()
 with torch.no_grad():
 test_outputs = model(X_test_tensor)
 _, test_predicted = torch.max(test_outputs.data, 1)
 test_accuracy = accuracy_score(y_test, test_predicted.numpy())
 test_accuracies.append(test_accuracy)
 model.train()

 # –í—ã–≤–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
 if epoch % 10 == 0 or epoch == epochs - 1:
 print(f'Epoch {epoch:3d}/{epochs}: '
 f'Loss: {avg_loss:.4f}, '
 f'Train Acc: {train_accuracy:.4f}, '
 f'Test Acc: {test_accuracy:.4f}')

 # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
 model.eval()
 with torch.no_grad():
 test_outputs = model(X_test_tensor)
 _, test_predicted = torch.max(test_outputs.data, 1)
 test_Predictions = test_predicted.numpy()
 test_probabilities = torch.softmax(test_outputs, dim=1).numpy()

 # –ú–µ—Ç—Ä–∏–∫–∏
 final_accuracy = accuracy_score(y_test, test_Predictions)

 print(f"\n=== –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã ===")
 print(f"Test accuracy: {final_accuracy:.4f}")

 # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
 print(f"\n=== Classification Report ===")
 print(classification_report(y_test, test_Predictions))

 # –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
 history = {
 'train_losses': train_losses,
 'train_accuracies': train_accuracies,
 'test_accuracies': test_accuracies
 }

 # –ú–µ—Ç—Ä–∏–∫–∏ for –≤–æ–∑–≤—Ä–∞—Ç–∞
 metrics = {
 'test_accuracy': final_accuracy,
 'confusion_matrix': confusion_matrix(y_test, test_Predictions),
 'Predictions': test_Predictions,
 'probabilities': test_probabilities,
 'history': history
 }

 return model, metrics, history

def plot_lstm_training_history(history):
 """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è LSTM"""

 fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

 # –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å
 ax1.plot(history['train_losses'], label='Train Loss')
 ax1.set_title('LSTM Training Loss')
 ax1.set_xlabel('Epoch')
 ax1.set_ylabel('Loss')
 ax1.legend()
 ax1.grid(True)

 # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏
 ax2.plot(history['train_accuracies'], label='Train Accuracy')
 ax2.plot(history['test_accuracies'], label='Test Accuracy')
 ax2.set_title('LSTM Training and Test Accuracy')
 ax2.set_xlabel('Epoch')
 ax2.set_ylabel('Accuracy')
 ax2.legend()
 ax2.grid(True)

 plt.tight_layout()
 plt.show()

# example –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
def example_lstm_usage():
 """example –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è LSTM"""

 # create —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
 np.random.seed(42)
 n_samples, n_features = 1000, 10
 sequence_length = 10

 # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ with –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
 X = np.random.randn(n_samples, n_features)

 # create —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π with –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏
 y = np.zeros(n_samples)
 for i in range(n_samples):
 # –í—Ä–µ–º–µ–Ω–Ω–∞—è dependency: —Ç–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏—Ç from –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö
 if i < sequence_length:
 y[i] = 0 # –ù–∞—á–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
 else:
 # –°–ª–æ–∂–Ω–∞—è –≤—Ä–µ–º–µ–Ω–Ω–∞—è dependency
 recent_sum = np.sum(X[i-sequence_length:i, 0])
 recent_volatility = np.std(X[i-sequence_length:i, 1])

 if recent_sum > 2.0 and recent_volatility < 1.0:
 y[i] = 2 # –ö–ª–∞—Å—Å 2
 elif recent_sum > 0.5 or recent_volatility > 1.5:
 y[i] = 1 # –ö–ª–∞—Å—Å 1
 else:
 y[i] = 0 # –ö–ª–∞—Å—Å 0

 print("=== example –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è LSTM ===")

 # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
 model, metrics, history = train_lstm_model(
 X, y,
 sequence_length=sequence_length,
 epochs=50
 )

 # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è
 plot_lstm_training_history(history)

 return model, metrics

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–∞ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ for —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
# model, metrics = example_lstm_usage()
```

## –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π

**–¢–µ–æ—Ä–∏—è:** –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π for —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–∞, —Ç–∞–∫ –∫–∞–∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏ –º–æ–≥—É—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ data leakage (—É—Ç–µ—á–∫–µ –¥–∞–Ω–Ω—ã—Ö) –∏–∑-–∑–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø—Ä–∏—Ä–æ–¥—ã —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

**–ü–æ—á–µ–º—É —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è not –ø–æ–¥—Ö–æ–¥–∏—Ç:**
- **Data Leakage:** –ë—É–¥—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ –º–æ–≥—É—Ç "–ø—Ä–æ—Ç–µ–∫–∞—Ç—å" in –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É
- **–í—Ä–µ–º–µ–Ω–Ω–∞—è dependency:** –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–º–µ—é—Ç –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
- **–ù–µ—Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ—Å—Ç—å:** –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–µ–Ω—è—é—Ç—Å—è –≤–æ –≤—Ä–µ–º–µ–Ω–∏
- **–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç—å:** –ù—É–∂–Ω–æ –∏–º–∏—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ —Ç–æ—Ä–≥–æ–≤—ã–µ —É—Å–ª–æ–≤–∏—è

### 1. Time Series Cross Validation

**–¢–µ–æ—Ä–∏—è:** Time Series Cross Validation (TSCV) - —ç—Ç–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ for –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç data leakage, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –ø—Ä–æ—à–ª—ã–µ –¥–∞–Ω–Ω—ã–µ for –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±—É–¥—É—â–∏—Ö.

**–ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã TSCV:**
1. **–í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ:** –î–∞–Ω–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—è—é—Ç—Å—è on –≤—Ä–µ–º–µ–Ω–∏, –∞ not —Å–ª—É—á–∞–π–Ω–æ
2. **–°—Ç—Ä–æ–≥–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å:** –ö–∞–∂–¥–∞—è —Å–ª–µ–¥—É—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞ –≤–∫–ª—é—á–∞–µ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ
3. **–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç—å:** –ò–º–∏—Ç–∏—Ä—É–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ —É—Å–ª–æ–≤–∏—è —Ç–æ—Ä–≥–æ–≤–ª–∏
4. **–ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ leakage:** –ë—É–¥—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ –Ω–∏–∫–æ–≥–¥–∞ not –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è for –æ–±—É—á–µ–Ω–∏—è

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è TSCV:**

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç –∫–æ–¥:**
1. **–í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ:** –°–æ–∑–¥–∞–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–æ–ª–¥—ã –±–µ–∑ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π
2. **–û–±—É—á–µ–Ω–∏–µ on –∏—Å—Ç–æ—Ä–∏–∏:** –ö–∞–∂–¥–∞—è –º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ on –ø—Ä–æ—à–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö
3. **–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ on –±—É–¥—É—â–µ–º:** –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–µ–ª–∞—é—Ç—Å—è on –±—É–¥—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
4. **–ú–µ—Ç—Ä–∏–∫–∏:** –í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ for –∫–∞–∂–¥–æ–≥–æ —Ñ–æ–ª–¥–∞

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
 f1_score, confusion_matrix, classification_report)
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Any, Tuple

def time_series_cv(model, X, y, n_splits=5, test_size=None, random_state=42):
 """
 –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è for –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤

 Args:
 model: –ú–æ–¥–µ–ª—å for –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–¥–æ–ª–∂–Ω–∞ –∏–º–µ—Ç—å –º–µ—Ç–æ–¥—ã fit and predict)
 X: –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (samples, features)
 y: –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (samples,)
 n_splits: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ–ª–¥–æ–≤
 test_size: –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —Ñ–æ–ª–¥–∞ (–µ—Å–ª–∏ None, –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
 random_state: Seed for –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏

 Returns:
 dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏ with –º–µ—Ç—Ä–∏–∫–∞–º–∏ for –∫–∞–∂–¥–æ–≥–æ —Ñ–æ–ª–¥–∞
 """

 print("=== Time Series Cross Validation ===")
 print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {X.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤, {X.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
 print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ–ª–¥–æ–≤: {n_splits}")
 print(f"–ö–ª–∞—Å—Å—ã: {np.unique(y, return_counts=True)}")

 # create TimeSeriesSplit
 tscv = TimeSeriesSplit(n_splits=n_splits, test_size=test_size)

 # –°–ø–∏—Å–∫–∏ for —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
 fold_scores = []
 fold_Predictions = []
 fold_confusion_matrices = []

 print(f"\n–ù–∞—á–∞–ª–æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏...")

 for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):
 print(f"\n--- Fold {fold + 1}/{n_splits} ---")

 # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
 X_train, X_test = X[train_idx], X[test_idx]
 y_train, y_test = y[train_idx], y[test_idx]

 print(f"Train size: {len(train_idx)} ({len(train_idx)/len(X)*100:.1f}%)")
 print(f"Test size: {len(test_idx)} ({len(test_idx)/len(X)*100:.1f}%)")
 print(f"Train period: {train_idx[0]} - {train_idx[-1]}")
 print(f"Test period: {test_idx[0]} - {test_idx[-1]}")

 # create –∫–æ–ø–∏–∏ –º–æ–¥–µ–ª–∏ for –∫–∞–∂–¥–æ–≥–æ —Ñ–æ–ª–¥–∞
 fold_model = type(model)(**model.get_params())

 # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
 print("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
 fold_model.fit(X_train, y_train)

 # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
 y_pred = fold_model.predict(X_test)
 y_pred_proba = fold_model.predict_proba(X_test) if hasattr(fold_model, 'predict_proba') else None

 # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
 accuracy = accuracy_score(y_test, y_pred)
 precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
 recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
 f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)

 # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
 cm = confusion_matrix(y_test, y_pred)

 # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
 fold_score = {
 'fold': fold + 1,
 'accuracy': accuracy,
 'precision': precision,
 'recall': recall,
 'f1': f1,
 'train_size': len(train_idx),
 'test_size': len(test_idx)
 }

 fold_scores.append(fold_score)
 fold_Predictions.append({
 'y_true': y_test,
 'y_pred': y_pred,
 'y_pred_proba': y_pred_proba
 })
 fold_confusion_matrices.append(cm)

 print(f"Accuracy: {accuracy:.4f}")
 print(f"Precision: {precision:.4f}")
 print(f"Recall: {recall:.4f}")
 print(f"F1: {f1:.4f}")

 # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
 results = {
 'fold_scores': fold_scores,
 'fold_Predictions': fold_Predictions,
 'fold_confusion_matrices': fold_confusion_matrices,
 'mean_accuracy': np.mean([s['accuracy'] for s in fold_scores]),
 'std_accuracy': np.std([s['accuracy'] for s in fold_scores]),
 'mean_precision': np.mean([s['precision'] for s in fold_scores]),
 'mean_recall': np.mean([s['recall'] for s in fold_scores]),
 'mean_f1': np.mean([s['f1'] for s in fold_scores])
 }

 # –í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
 print(f"\n=== –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã TSCV ===")
 print(f"Mean Accuracy: {results['mean_accuracy']:.4f} ¬± {results['std_accuracy']:.4f}")
 print(f"Mean Precision: {results['mean_precision']:.4f}")
 print(f"Mean Recall: {results['mean_recall']:.4f}")
 print(f"Mean F1: {results['mean_f1']:.4f}")

 return results

def plot_tscv_results(results, figsize=(15, 10)):
 """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Time Series Cross Validation"""

 fig, axes = plt.subplots(2, 2, figsize=figsize)

 # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
 fold_scores = results['fold_scores']
 folds = [s['fold'] for s in fold_scores]
 accuracies = [s['accuracy'] for s in fold_scores]
 precisions = [s['precision'] for s in fold_scores]
 recalls = [s['recall'] for s in fold_scores]
 f1_scores = [s['f1'] for s in fold_scores]

 # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏ on —Ñ–æ–ª–¥–∞–º
 axes[0, 0].plot(folds, accuracies, 'o-', label='Accuracy')
 axes[0, 0].axhline(y=results['mean_accuracy'], color='r', linestyle='--',
 label=f'Mean: {results["mean_accuracy"]:.3f}')
 axes[0, 0].set_title('Accuracy by Fold')
 axes[0, 0].set_xlabel('Fold')
 axes[0, 0].set_ylabel('Accuracy')
 axes[0, 0].legend()
 axes[0, 0].grid(True)

 # –ì—Ä–∞—Ñ–∏–∫ –º–µ—Ç—Ä–∏–∫ on —Ñ–æ–ª–¥–∞–º
 axes[0, 1].plot(folds, accuracies, 'o-', label='Accuracy')
 axes[0, 1].plot(folds, precisions, 's-', label='Precision')
 axes[0, 1].plot(folds, recalls, '^-', label='Recall')
 axes[0, 1].plot(folds, f1_scores, 'd-', label='F1')
 axes[0, 1].set_title('Metrics by Fold')
 axes[0, 1].set_xlabel('Fold')
 axes[0, 1].set_ylabel('Score')
 axes[0, 1].legend()
 axes[0, 1].grid(True)

 # Box plot –º–µ—Ç—Ä–∏–∫
 metrics_data = [accuracies, precisions, recalls, f1_scores]
 metrics_labels = ['Accuracy', 'Precision', 'Recall', 'F1']
 axes[1, 0].boxplot(metrics_data, labels=metrics_labels)
 axes[1, 0].set_title('Distribution of Metrics')
 axes[1, 0].set_ylabel('Score')
 axes[1, 0].grid(True)

 # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
 if results['fold_confusion_matrices']:
 # –°—É–º–º–∏—Ä—É–µ–º –≤—Å–µ –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫
 total_cm = np.sum(results['fold_confusion_matrices'], axis=0)

 # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º for –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤
 total_cm_norm = total_cm.astype('float') / total_cm.sum(axis=1)[:, np.newaxis]

 sns.heatmap(total_cm_norm, annot=True, fmt='.2f', cmap='Blues', ax=axes[1, 1])
 axes[1, 1].set_title('Aggregated Confusion Matrix')
 axes[1, 1].set_xlabel('Predicted')
 axes[1, 1].set_ylabel('True')

 plt.tight_layout()
 plt.show()

def analyze_tscv_stability(results):
 """–ê–Ω–∞–ª–∏–∑ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ TSCV"""

 fold_scores = results['fold_scores']
 accuracies = [s['accuracy'] for s in fold_scores]

 print("=== –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ TSCV ===")
 print(f"Accuracy - Min: {min(accuracies):.4f}, Max: {max(accuracies):.4f}")
 print(f"Accuracy - Range: {max(accuracies) - min(accuracies):.4f}")
 print(f"Accuracy - Std: {np.std(accuracies):.4f}")
 print(f"Accuracy - CV: {np.std(accuracies) / np.mean(accuracies):.4f}")

 # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–∞
 if len(accuracies) >= 3:
 # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ç—Ä–µ–Ω–¥ (improve/—É—Ö—É–¥—à–µ–Ω–∏–µ —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º)
 from scipy import stats
 slope, intercept, r_value, p_value, std_err = stats.linregress(range(len(accuracies)), accuracies)

 print(f"\n–¢—Ä–µ–Ω–¥ —Ç–æ—á–Ω–æ—Å—Ç–∏:")
 print(f"Slope: {slope:.6f} (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π = improve)")
 print(f"R-squared: {r_value**2:.4f}")
 print(f"P-value: {p_value:.4f}")

 if p_value < 0.05:
 if slope > 0:
 print("–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º–æ–µ improve —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º")
 else:
 print("–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º–æ–µ —É—Ö—É–¥—à–µ–Ω–∏–µ —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º")
 else:
 print("–ù–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º–æ–≥–æ —Ç—Ä–µ–Ω–¥–∞")

# example –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
def example_tscv_usage():
 """example –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Time Series Cross Validation"""

 from sklearn.ensemble import RandomForestClassifier

 # create —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
 np.random.seed(42)
 n_samples, n_features = 1000, 20

 # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ with –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
 X = np.random.randn(n_samples, n_features)

 # create —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π with –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏
 y = np.zeros(n_samples)
 for i in range(n_samples):
 # –í—Ä–µ–º–µ–Ω–Ω–∞—è dependency
 if i < 100:
 y[i] = 0
 elif i < 500:
 y[i] = 1 if X[i, 0] > 0 else 0
 else:
 y[i] = 2 if X[i, 0] > 0.5 else (1 if X[i, 0] > -0.5 else 0)

 print("=== example Time Series Cross Validation ===")

 # create –º–æ–¥–µ–ª–∏
 model = RandomForestClassifier(n_estimators=50, random_state=42)

 # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ TSCV
 results = time_series_cv(model, X, y, n_splits=5)

 # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
 plot_tscv_results(results)

 # –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
 analyze_tscv_stability(results)

 return results

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–∞ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ for —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
# results = example_tscv_usage()
```

### 2. Walk-Forward Validation

**–¢–µ–æ—Ä–∏—è:** Walk-Forward Validation (WFV) - —ç—Ç–æ –º–µ—Ç–æ–¥ –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –∏–º–∏—Ç–∏—Ä—É–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ —Ç–æ—Ä–≥–æ–≤—ã–µ —É—Å–ª–æ–≤–∏—è, –≥–¥–µ –º–æ–¥–µ–ª—å –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –ø–µ—Ä–µ–æ–±—É—á–∞–µ—Ç—Å—è on –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö and –¥–µ–ª–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è on —Å–ª–µ–¥—É—é—â–∏–π –ø–µ—Ä–∏–æ–¥.

**–ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã WFV:**
1. **–°–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ:** –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞ –∏–º–µ–µ—Ç —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä
2. **–ü–æ—à–∞–≥–æ–≤–æ–µ –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏–µ:** –û–∫–Ω–æ —Å–¥–≤–∏–≥–∞–µ—Ç—Å—è on —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —à–∞–≥
3. **–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç—å:** –ò–º–∏—Ç–∏—Ä—É–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ —Ç–æ—Ä–≥–æ–≤—ã–µ —É—Å–ª–æ–≤–∏—è
4. **–ê–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å:** –ú–æ–¥–µ–ª—å –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –∫ –Ω–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º

**–ü–æ—á–µ–º—É WFV —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω for —Ñ–∏–Ω–∞–Ω—Å–æ–≤:**
- **–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç—å:** –¢–æ—á–Ω–æ –∏–º–∏—Ç–∏—Ä—É–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ —Ç–æ—Ä–≥–æ–≤—ã–µ —É—Å–ª–æ–≤–∏—è
- **–ê–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å:** –ú–æ–¥–µ–ª—å –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è
- **–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å:** –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –∫–∞–∫ –º–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç in –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–µ
- **–î—Ä–µ–π—Ñ –¥–∞–Ω–Ω—ã—Ö:** –ü–æ–º–æ–≥–∞–µ—Ç –≤—ã—è–≤–∏—Ç—å, –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å —É—Å—Ç–∞—Ä–µ–≤–∞–µ—Ç

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è WFV:**

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç –∫–æ–¥:**
1. **–°–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ:** –°–æ–∑–¥–∞–µ—Ç –æ–±—É—á–∞—é—â–∏–µ –≤—ã–±–æ—Ä–∫–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
2. **–ü–æ—à–∞–≥–æ–≤–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ:** –¢–µ—Å—Ç–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å on —Å–ª–µ–¥—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
3. **–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ:** –ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–æ–±—É—á–∞–µ—Ç—Å—è on –∫–∞–∂–¥–æ–º —à–∞–≥–µ
4. **–ú–µ—Ç—Ä–∏–∫–∏:** –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤–æ –≤—Ä–µ–º–µ–Ω–∏

```python
import numpy as np
import pandas as pd
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
 f1_score, confusion_matrix)
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Any, Tuple

def walk_forward_validation(model, X, y, train_size=1000, step_size=100,
 min_test_size=50, random_state=42):
 """
 Walk-Forward –≤–∞–ª–∏–¥–∞—Ü–∏—è for –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤

 Args:
 model: –ú–æ–¥–µ–ª—å for –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–¥–æ–ª–∂–Ω–∞ –∏–º–µ—Ç—å –º–µ—Ç–æ–¥—ã fit and predict)
 X: –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (samples, features)
 y: –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (samples,)
 train_size: –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–≥–æ –æ–∫–Ω–∞
 step_size: –†–∞–∑–º–µ—Ä —à–∞–≥–∞ for –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏—è –æ–∫–Ω–∞
 min_test_size: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏
 random_state: Seed for –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏

 Returns:
 dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã walk-forward –≤–∞–ª–∏–¥–∞—Ü–∏–∏
 """

 print("=== Walk-Forward Validation ===")
 print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {X.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤, {X.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
 print(f"–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–≥–æ –æ–∫–Ω–∞: {train_size}")
 print(f"–†–∞–∑–º–µ—Ä —à–∞–≥–∞: {step_size}")
 print(f"–ö–ª–∞—Å—Å—ã: {np.unique(y, return_counts=True)}")

 # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π
 n_iterations = (len(X) - train_size) // step_size
 print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π: {n_iterations}")

 # –°–ø–∏—Å–∫–∏ for —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
 iteration_results = []
 all_Predictions = []
 all_true_labels = []

 print(f"\n–ù–∞—á–∞–ª–æ walk-forward –≤–∞–ª–∏–¥–∞—Ü–∏–∏...")

 for i in range(n_iterations):
 start_idx = i * step_size
 train_end_idx = start_idx + train_size
 test_start_idx = train_end_idx
 test_end_idx = min(test_start_idx + step_size, len(X))

 # check –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏
 if test_end_idx - test_start_idx < min_test_size:
 print(f"–ò—Ç–µ—Ä–∞—Ü–∏—è {i+1}: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö for —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
 continue

 print(f"\n--- –ò—Ç–µ—Ä–∞—Ü–∏—è {i+1}/{n_iterations} ---")
 print(f"–û–±—É—á–∞—é—â–∏–π –ø–µ—Ä–∏–æ–¥: {start_idx} - {train_end_idx-1}")
 print(f"–¢–µ—Å—Ç–æ–≤—ã–π –ø–µ—Ä–∏–æ–¥: {test_start_idx} - {test_end_idx-1}")

 # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
 X_train = X[start_idx:train_end_idx]
 y_train = y[start_idx:train_end_idx]
 X_test = X[test_start_idx:test_end_idx]
 y_test = y[test_start_idx:test_end_idx]

 print(f"Train size: {len(X_train)}")
 print(f"Test size: {len(X_test)}")

 # create –∫–æ–ø–∏–∏ –º–æ–¥–µ–ª–∏ for –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏
 fold_model = type(model)(**model.get_params())

 # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
 print("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
 fold_model.fit(X_train, y_train)

 # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
 y_pred = fold_model.predict(X_test)
 y_pred_proba = fold_model.predict_proba(X_test) if hasattr(fold_model, 'predict_proba') else None

 # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
 accuracy = accuracy_score(y_test, y_pred)
 precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
 recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
 f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)

 # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
 cm = confusion_matrix(y_test, y_pred)

 # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
 iteration_result = {
 'iteration': i + 1,
 'train_start': start_idx,
 'train_end': train_end_idx - 1,
 'test_start': test_start_idx,
 'test_end': test_end_idx - 1,
 'accuracy': accuracy,
 'precision': precision,
 'recall': recall,
 'f1': f1,
 'train_size': len(X_train),
 'test_size': len(X_test),
 'confusion_matrix': cm
 }

 iteration_results.append(iteration_result)
 all_Predictions.extend(y_pred)
 all_true_labels.extend(y_test)

 print(f"Accuracy: {accuracy:.4f}")
 print(f"Precision: {precision:.4f}")
 print(f"Recall: {recall:.4f}")
 print(f"F1: {f1:.4f}")

 # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
 if iteration_results:
 accuracies = [r['accuracy'] for r in iteration_results]
 precisions = [r['precision'] for r in iteration_results]
 recalls = [r['recall'] for r in iteration_results]
 f1_scores = [r['f1'] for r in iteration_results]

 results = {
 'iteration_results': iteration_results,
 'all_Predictions': np.array(all_Predictions),
 'all_true_labels': np.array(all_true_labels),
 'mean_accuracy': np.mean(accuracies),
 'std_accuracy': np.std(accuracies),
 'mean_precision': np.mean(precisions),
 'mean_recall': np.mean(recalls),
 'mean_f1': np.mean(f1_scores),
 'n_iterations': len(iteration_results)
 }

 # –í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
 print(f"\n=== –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã Walk-Forward ===")
 print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π: {results['n_iterations']}")
 print(f"Mean Accuracy: {results['mean_accuracy']:.4f} ¬± {results['std_accuracy']:.4f}")
 print(f"Mean Precision: {results['mean_precision']:.4f}")
 print(f"Mean Recall: {results['mean_recall']:.4f}")
 print(f"Mean F1: {results['mean_f1']:.4f}")

 # –û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å on –≤—Å–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º
 overall_accuracy = accuracy_score(all_true_labels, all_Predictions)
 print(f"Overall Accuracy: {overall_accuracy:.4f}")

 else:
 print("–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ for analysis")
 results = None

 return results

def plot_walk_forward_results(results, figsize=(15, 12)):
 """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Walk-Forward –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""

 if results is None:
 print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö for –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")
 return

 fig, axes = plt.subplots(2, 2, figsize=figsize)

 # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
 iteration_results = results['iteration_results']
 iterations = [r['iteration'] for r in iteration_results]
 accuracies = [r['accuracy'] for r in iteration_results]
 precisions = [r['precision'] for r in iteration_results]
 recalls = [r['recall'] for r in iteration_results]
 f1_scores = [r['f1'] for r in iteration_results]

 # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏ on –∏—Ç–µ—Ä–∞—Ü–∏—è–º
 axes[0, 0].plot(iterations, accuracies, 'o-', label='Accuracy')
 axes[0, 0].axhline(y=results['mean_accuracy'], color='r', linestyle='--',
 label=f'Mean: {results["mean_accuracy"]:.3f}')
 axes[0, 0].set_title('Accuracy by Iteration')
 axes[0, 0].set_xlabel('Iteration')
 axes[0, 0].set_ylabel('Accuracy')
 axes[0, 0].legend()
 axes[0, 0].grid(True)

 # –ì—Ä–∞—Ñ–∏–∫ –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫ on –∏—Ç–µ—Ä–∞—Ü–∏—è–º
 axes[0, 1].plot(iterations, accuracies, 'o-', label='Accuracy')
 axes[0, 1].plot(iterations, precisions, 's-', label='Precision')
 axes[0, 1].plot(iterations, recalls, '^-', label='Recall')
 axes[0, 1].plot(iterations, f1_scores, 'd-', label='F1')
 axes[0, 1].set_title('Metrics by Iteration')
 axes[0, 1].set_xlabel('Iteration')
 axes[0, 1].set_ylabel('Score')
 axes[0, 1].legend()
 axes[0, 1].grid(True)

 # –°–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ —Ç–æ—á–Ω–æ—Å—Ç–∏
 window_size = max(1, len(accuracies) // 5) # 20% from –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π
 if window_size > 1:
 rolling_accuracy = pd.Series(accuracies).rolling(window=window_size).mean()
 axes[1, 0].plot(iterations, accuracies, 'o-', alpha=0.3, label='Raw Accuracy')
 axes[1, 0].plot(iterations, rolling_accuracy, 'r-', linewidth=2,
 label=f'Rolling Mean (window={window_size})')
 axes[1, 0].set_title('Accuracy with Rolling Mean')
 axes[1, 0].set_xlabel('Iteration')
 axes[1, 0].set_ylabel('Accuracy')
 axes[1, 0].legend()
 axes[1, 0].grid(True)
 else:
 axes[1, 0].plot(iterations, accuracies, 'o-')
 axes[1, 0].set_title('Accuracy by Iteration')
 axes[1, 0].set_xlabel('Iteration')
 axes[1, 0].set_ylabel('Accuracy')
 axes[1, 0].grid(True)

 # –û–±—â–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
 if len(results['all_true_labels']) > 0:
 overall_cm = confusion_matrix(results['all_true_labels'], results['all_Predictions'])
 overall_cm_norm = overall_cm.astype('float') / overall_cm.sum(axis=1)[:, np.newaxis]

 sns.heatmap(overall_cm_norm, annot=True, fmt='.2f', cmap='Blues', ax=axes[1, 1])
 axes[1, 1].set_title('Overall Confusion Matrix')
 axes[1, 1].set_xlabel('Predicted')
 axes[1, 1].set_ylabel('True')

 plt.tight_layout()
 plt.show()

def analyze_walk_forward_stability(results):
 """–ê–Ω–∞–ª–∏–∑ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Walk-Forward –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""

 if results is None:
 print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö for analysis")
 return

 iteration_results = results['iteration_results']
 accuracies = [r['accuracy'] for r in iteration_results]

 print("=== –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ Walk-Forward ===")
 print(f"Accuracy - Min: {min(accuracies):.4f}, Max: {max(accuracies):.4f}")
 print(f"Accuracy - Range: {max(accuracies) - min(accuracies):.4f}")
 print(f"Accuracy - Std: {np.std(accuracies):.4f}")
 print(f"Accuracy - CV: {np.std(accuracies) / np.mean(accuracies):.4f}")

 # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–∞
 if len(accuracies) >= 3:
 from scipy import stats
 slope, intercept, r_value, p_value, std_err = stats.linregress(range(len(accuracies)), accuracies)

 print(f"\n–¢—Ä–µ–Ω–¥ —Ç–æ—á–Ω–æ—Å—Ç–∏:")
 print(f"Slope: {slope:.6f} (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π = improve)")
 print(f"R-squared: {r_value**2:.4f}")
 print(f"P-value: {p_value:.4f}")

 if p_value < 0.05:
 if slope > 0:
 print("–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º–æ–µ improve —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º")
 else:
 print("–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º–æ–µ —É—Ö—É–¥—à–µ–Ω–∏–µ —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º")
 else:
 print("–ù–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º–æ–≥–æ —Ç—Ä–µ–Ω–¥–∞")

 # –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ (—Å–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ)
 if len(accuracies) >= 10:
 window_size = max(3, len(accuracies) // 5)
 rolling_std = pd.Series(accuracies).rolling(window=window_size).std()

 print(f"\n–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å (—Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ, –æ–∫–Ω–æ={window_size}):")
 print(f"Mean Rolling Std: {rolling_std.mean():.4f}")
 print(f"Max Rolling Std: {rolling_std.max():.4f}")

 # check on –¥–µ–≥—Ä–∞–¥–∞—Ü–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
 recent_acc = np.mean(accuracies[-window_size:])
 early_acc = np.mean(accuracies[:window_size])
 degradation = early_acc - recent_acc

 print(f"\n–î–µ–≥—Ä–∞–¥–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:")
 print(f"Early accuracy: {early_acc:.4f}")
 print(f"Recent accuracy: {recent_acc:.4f}")
 print(f"Degradation: {degradation:.4f}")

 if degradation > 0.05: # 5% –¥–µ–≥—Ä–∞–¥–∞—Ü–∏—è
 print("‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–∞—è –¥–µ–≥—Ä–∞–¥–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏!")
 elif degradation > 0.02: # 2% –¥–µ–≥—Ä–∞–¥–∞—Ü–∏—è
 print("‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –£–º–µ—Ä–µ–Ω–Ω–∞—è –¥–µ–≥—Ä–∞–¥–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
 else:
 print("‚úÖ –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å—Ç–∞–±–∏–ª—å–Ω–∞")

# example –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
def example_walk_forward_usage():
 """example –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Walk-Forward –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""

 from sklearn.ensemble import RandomForestClassifier

 # create —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
 np.random.seed(42)
 n_samples, n_features = 2000, 20

 # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ with –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
 X = np.random.randn(n_samples, n_features)

 # create —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π with –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏ and –¥—Ä–µ–π—Ñ–æ–º
 y = np.zeros(n_samples)
 for i in range(n_samples):
 # –î—Ä–µ–π—Ñ: –ø–∞—Ç—Ç–µ—Ä–Ω—ã –º–µ–Ω—è—é—Ç—Å—è —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º
 if i < 500:
 # –†–∞–Ω–Ω–∏–π –ø–µ—Ä–∏–æ–¥: –ø—Ä–æ—Å—Ç—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
 y[i] = 1 if X[i, 0] > 0 else 0
 elif i < 1000:
 # –°—Ä–µ–¥–Ω–∏–π –ø–µ—Ä–∏–æ–¥: –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
 y[i] = 2 if X[i, 0] > 0.5 else (1 if X[i, 0] > -0.5 else 0)
 else:
 # –ü–æ–∑–¥–Ω–∏–π –ø–µ—Ä–∏–æ–¥: –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å–Ω–æ–≤–∞ –º–µ–Ω—è—é—Ç—Å—è
 y[i] = 1 if X[i, 0] > -0.2 else (2 if X[i, 0] > 0.8 else 0)

 print("=== example Walk-Forward –≤–∞–ª–∏–¥–∞—Ü–∏–∏ ===")

 # create –º–æ–¥–µ–ª–∏
 model = RandomForestClassifier(n_estimators=50, random_state=42)

 # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ Walk-Forward –≤–∞–ª–∏–¥–∞—Ü–∏–∏
 results = walk_forward_validation(
 model, X, y,
 train_size=500,
 step_size=100
 )

 # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
 plot_walk_forward_results(results)

 # –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
 analyze_walk_forward_stability(results)

 return results

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–∞ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ for —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
# results = example_walk_forward_usage()
```

## –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

**–¢–µ–æ—Ä–∏—è:** –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è - —ç—Ç–æ –ø—Ä–æ—Ü–µ—Å—Å –ø–æ–∏—Å–∫–∞ –Ω–∞–∏–ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏ for –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. for —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö —ç—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ, —Ç–∞–∫ –∫–∞–∫ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ parameters –º–æ–≥—É—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—é or –Ω–µ–¥–æ–æ–±—É—á–µ–Ω–∏—é.

**–ü–æ—á–µ–º—É –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–∞ for —Ñ–∏–Ω–∞–Ω—Å–æ–≤:**
- **–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ:** –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–∫–ª–æ–Ω–Ω—ã –∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—é
- **–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å:** –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ parameters –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
- **–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:** –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ parameters —É–ª—É—á—à–∞—é—Ç —Ç–æ—á–Ω–æ—Å—Ç—å
- **–†–∏—Å–∫-–¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å:** –ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é and —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å—é

**–ú–µ—Ç–æ–¥—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:**
1. **Grid Search:** –ü–æ–ª–Ω—ã–π –ø–µ—Ä–µ–±–æ—Ä –≤—Å–µ—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
2. **Random Search:** –°–ª—É—á–∞–π–Ω—ã–π –ø–æ–∏—Å–∫ in –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
3. **Bayesian Optimization:** –£–º–Ω—ã–π –ø–æ–∏—Å–∫ with –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
4. **Optuna:** –°–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ for –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

### 1. Grid Search

**–¢–µ–æ—Ä–∏—è:** Grid Search - —ç—Ç–æ –º–µ—Ç–æ–¥ –ø–æ–ª–Ω–æ–≥–æ –ø–µ—Ä–µ–±–æ—Ä–∞, –∫–æ—Ç–æ—Ä—ã–π —Ç–µ—Å—Ç–∏—Ä—É–µ—Ç –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–∑ –∑–∞–¥–∞–Ω–Ω–æ–π —Å–µ—Ç–∫–∏. –•–æ—Ç—è –æ–Ω –º–æ–∂–µ—Ç –±—ã—Ç—å –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ –¥–æ—Ä–æ–≥–∏–º, –æ–Ω –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ in –∑–∞–¥–∞–Ω–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ.

**–ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã Grid Search:**
1. **–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–µ—Ç–∫–∏:** –ó–∞–¥–∞–µ—Ç—Å—è –¥–∏–∞–ø–∞–∑–æ–Ω –∑–Ω–∞—á–µ–Ω–∏–π for –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞
2. **–ü–æ–ª–Ω—ã–π –ø–µ—Ä–µ–±–æ—Ä:** –¢–µ—Å—Ç–∏—Ä—É—é—Ç—Å—è –≤—Å–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
3. **–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è:** –ö–∞–∂–¥–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è with –ø–æ–º–æ—â—å—é CV
4. **–í—ã–±–æ—Ä –ª—É—á—à–∏—Ö:** –í—ã–±–∏—Ä–∞–µ—Ç—Å—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è with –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é

**–ü–ª—é—Å—ã Grid Search:**
- –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ in —Å–µ—Ç–∫–µ
- –ü—Ä–æ—Å—Ç–æ–π in –ø–æ–Ω–∏–º–∞–Ω–∏–∏ and —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏
- –•–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç with –Ω–µ–±–æ–ª—å—à–∏–º–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

**–ú–∏–Ω—É—Å—ã Grid Search:**
- –í—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ –¥–æ—Ä–æ–≥–æ–π
- not –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è on –±–æ–ª—å—à–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- –ú–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º for –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Grid Search:**

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç –∫–æ–¥:**
1. **–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–µ—Ç–∫–∏:** –°–æ–∑–¥–∞–µ—Ç —Å–µ—Ç–∫—É –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ for Random Forest
2. **–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è:** –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Time Series CV for —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
3. **–ü–æ–∏—Å–∫:** –¢–µ—Å—Ç–∏—Ä—É–µ—Ç –≤—Å–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
4. **–û—Ü–µ–Ω–∫–∞:** –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ª—É—á—à—É—é –º–æ–¥–µ–ª—å and parameters

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, Any, Tuple

def optimize_random_forest(X, y, param_grid=None, cv_folds=5,
 scoring='accuracy', n_jobs=-1, random_state=42):
 """
 –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è Random Forest with –ø–æ–º–æ—â—å—é Grid Search

 Args:
 X: –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (samples, features)
 y: –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (samples,)
 param_grid: –°–µ—Ç–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ for –ø–æ–∏—Å–∫–∞
 cv_folds: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ–ª–¥–æ–≤ for –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏
 scoring: –ú–µ—Ç—Ä–∏–∫–∞ for –æ—Ü–µ–Ω–∫–∏
 n_jobs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤
 random_state: Seed for –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏

 Returns:
 tuple: (–ª—É—á—à–∞—è –º–æ–¥–µ–ª—å, –ª—É—á—à–∏–µ parameters, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞)
 """

 print("=== Grid Search for Random Forest ===")
 print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {X.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤, {X.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
 print(f"–ö–ª–∞—Å—Å—ã: {np.unique(y, return_counts=True)}")

 # –°–µ—Ç–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ on —É–º–æ–ª—á–∞–Ω–∏—é
 if param_grid is None:
 param_grid = {
 'n_estimators': [50, 100, 200],
 'max_depth': [5, 10, 15, None],
 'min_samples_split': [2, 5, 10],
 'min_samples_leaf': [1, 2, 4],
 'max_features': ['sqrt', 'log2', None]
 }

 print(f"\n–°–µ—Ç–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:")
 for param, values in param_grid.items():
 print(f"{param}: {values}")

 # –ü–æ–¥—Å—á–µ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ–º–±–∏–Ω–∞—Ü–∏–π
 total_combinations = 1
 for values in param_grid.values():
 total_combinations *= len(values)
 print(f"–í—Å–µ–≥–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏–π: {total_combinations}")
 print(f"–í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤: {total_combinations * cv_folds}")

 # create –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
 rf = RandomForestClassifier(random_state=random_state, n_jobs=1)

 # create Time Series CV for —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
 tscv = TimeSeriesSplit(n_splits=cv_folds)

 # Grid Search
 grid_search = GridSearchCV(
 estimator=rf,
 param_grid=param_grid,
 cv=tscv,
 scoring=scoring,
 n_jobs=n_jobs,
 verbose=1,
 return_train_score=True
 )

 print(f"\n–ù–∞—á–∞–ª–æ Grid Search...")
 grid_search.fit(X, y)

 # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
 best_model = grid_search.best_estimator_
 best_params = grid_search.best_params_
 best_score = grid_search.best_score_

 print(f"\n=== –†–µ–∑—É–ª—å—Ç–∞—Ç—ã Grid Search ===")
 print(f"–õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_score:.4f}")
 print(f"–õ—É—á—à–∏–µ parameters:")
 for param, value in best_params.items():
 print(f" {param}: {value}")

 # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
 results_df = pd.DataFrame(grid_search.cv_results_)

 # –¢–æ–ø-5 –∫–æ–º–±–∏–Ω–∞—Ü–∏–π
 top_results = results_df.nlargest(5, 'mean_test_score')[
 ['params', 'mean_test_score', 'std_test_score']
 ]

 print(f"\n–¢–æ–ø-5 –∫–æ–º–±–∏–Ω–∞—Ü–∏–π:")
 for i, (_, row) in enumerate(top_results.iterrows(), 1):
 print(f"{i}. Score: {row['mean_test_score']:.4f} ¬± {row['std_test_score']:.4f}")
 print(f" Params: {row['params']}")

 return best_model, best_params, grid_search

def plot_grid_search_results(grid_search, param_name, figsize=(12, 8)):
 """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Grid Search for –æ–¥–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞"""

 results_df = pd.DataFrame(grid_search.cv_results_)

 # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è on –ø–∞—Ä–∞–º–µ—Ç—Ä—É
 param_results = results_df[results_df['param_' + param_name].notna()]

 if param_results.empty:
 print(f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö for –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ {param_name}")
 return

 # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ on –∑–Ω–∞—á–µ–Ω–∏—è–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞
 param_values = param_results['param_' + param_name].unique()
 mean_scores = []
 std_scores = []

 for value in param_values:
 value_results = param_results[param_results['param_' + param_name] == value]
 mean_scores.append(value_results['mean_test_score'].mean())
 std_scores.append(value_results['std_test_score'].mean())

 # create –≥—Ä–∞—Ñ–∏–∫–∞
 fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)

 # –ì—Ä–∞—Ñ–∏–∫ —Å—Ä–µ–¥–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π
 ax1.errorbar(param_values, mean_scores, yerr=std_scores,
 marker='o', capsize=5, capthick=2)
 ax1.set_title(f'Grid Search Results: {param_name}')
 ax1.set_xlabel(param_name)
 ax1.set_ylabel('Mean Test Score')
 ax1.grid(True, alpha=0.3)

 # –ì—Ä–∞—Ñ–∏–∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π
 ax2.bar(param_values, std_scores, alpha=0.7)
 ax2.set_title(f'Score Variability: {param_name}')
 ax2.set_xlabel(param_name)
 ax2.set_ylabel('Std Test Score')
 ax2.grid(True, alpha=0.3)

 plt.tight_layout()
 plt.show()

def analyze_grid_search_stability(grid_search):
 """–ê–Ω–∞–ª–∏–∑ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Grid Search"""

 results_df = pd.DataFrame(grid_search.cv_results_)

 print("=== –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ Grid Search ===")

 # –õ—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
 best_score = grid_search.best_score_
 best_std = results_df.loc[grid_search.best_index_, 'std_test_score']

 print(f"–õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:")
 print(f" Score: {best_score:.4f} ¬± {best_std:.4f}")
 print(f" CV: {best_std / best_score:.4f}")

 # –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
 all_scores = results_df['mean_test_score']
 all_stds = results_df['std_test_score']

 print(f"\n–û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
 print(f" Score range: {all_scores.min():.4f} - {all_scores.max():.4f}")
 print(f" Mean std: {all_stds.mean():.4f}")
 print(f" Max std: {all_stds.max():.4f}")

 # –¢–æ–ø-10 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
 top_10 = results_df.nlargest(10, 'mean_test_score')

 print(f"\n–¢–æ–ø-10 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
 for i, (_, row) in enumerate(top_10.iterrows(), 1):
 print(f"{i:2d}. {row['mean_test_score']:.4f} ¬± {row['std_test_score']:.4f} - {row['params']}")

# example –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
def example_grid_search_usage():
 """example –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Grid Search"""

 # create —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
 np.random.seed(42)
 n_samples, n_features = 1000, 20

 # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
 X = np.random.randn(n_samples, n_features)

 # create —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
 y = np.zeros(n_samples)
 for i in range(n_samples):
 if X[i, 0] > 0.5 and X[i, 1] < -0.3:
 y[i] = 1
 elif X[i, 2] > 1.0 or X[i, 3] < -1.0:
 y[i] = 2
 else:
 y[i] = 0

 print("=== example Grid Search ===")

 # –ü—Ä–æ—Å—Ç–∞—è —Å–µ—Ç–∫–∞ for –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
 param_grid = {
 'n_estimators': [50, 100],
 'max_depth': [5, 10],
 'min_samples_split': [2, 5]
 }

 # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ Grid Search
 best_model, best_params, grid_search = optimize_random_forest(
 X, y, param_grid=param_grid, cv_folds=3
 )

 # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
 analyze_grid_search_stability(grid_search)

 # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è (–µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ)
 if 'n_estimators' in best_params:
 plot_grid_search_results(grid_search, 'n_estimators')

 return best_model, best_params, grid_search

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–∞ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ for —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
# best_model, best_params, grid_search = example_grid_search_usage()
```

### 2. Optuna –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

**–¢–µ–æ—Ä–∏—è:** Optuna - —ç—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ for –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Bayesian Optimization and –¥—Ä—É–≥–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–µ—Ç–æ–¥—ã for —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.

**–ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã Optuna:**
1. **Bayesian Optimization:** –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã for –≤—ã–±–æ—Ä–∞ —Å–ª–µ–¥—É—é—â–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
2. **Tree-structured Parzen Estimator (TPE):** –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º for –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
3. **Pruning:** –ü—Ä–µ–∫—Ä–∞—â–∞–µ—Ç –Ω–µ–ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–µ –∏—Å–ø—ã—Ç–∞–Ω–∏—è —Ä–∞–Ω—å—à–µ
4. **–ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—è:** –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∏—Å–ø—ã—Ç–∞–Ω–∏–π

**–ü–æ—á–µ–º—É Optuna —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω for —Ñ–∏–Ω–∞–Ω—Å–æ–≤:**
- **–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å:** –ù–∞—Ö–æ–¥–∏—Ç —Ö–æ—Ä–æ—à–∏–µ parameters –±—ã—Å—Ç—Ä–µ–µ Grid Search
- **–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å:** –†–∞–±–æ—Ç–∞–µ—Ç with –±–æ–ª—å—à–∏–º–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- **Pruning:** –≠–∫–æ–Ω–æ–º–∏—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã
- **–ì–∏–±–∫–æ—Å—Ç—å:** –õ–µ–≥–∫–æ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∑–∞–¥–∞—á–∏

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Optuna:**

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç –∫–æ–¥:**
1. **–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞:** –°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
2. **Objective function:** –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é for –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
3. **–ò—Å–ø—ã—Ç–∞–Ω–∏—è:** –í—ã–ø–æ–ª–Ω—è–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∏—Å–ø—ã—Ç–∞–Ω–∏–π with —Ä–∞–∑–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
4. **Pruning:** –ü—Ä–µ–∫—Ä–∞—â–∞–µ—Ç –Ω–µ–ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–µ –∏—Å–ø—ã—Ç–∞–Ω–∏—è

```python
import numpy as np
import pandas as pd
import optuna
import xgboost as xgb
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, Any, Tuple
import warnings
warnings.filterwarnings('ignore')

def optimize_xgboost_optuna(X, y, n_trials=100, cv_folds=5,
 timeout=None, n_jobs=1, random_state=42):
 """
 –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è XGBoost with –ø–æ–º–æ—â—å—é Optuna

 Args:
 X: –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (samples, features)
 y: –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (samples,)
 n_trials: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å–ø—ã—Ç–∞–Ω–∏–π
 cv_folds: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ–ª–¥–æ–≤ for –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏
 timeout: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ in —Å–µ–∫—É–Ω–¥–∞—Ö
 n_jobs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤
 random_state: Seed for –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏

 Returns:
 tuple: (–ª—É—á—à–∏–µ parameters, –æ–±—ä–µ–∫—Ç study, –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å)
 """

 print("=== Optuna –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è XGBoost ===")
 print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {X.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤, {X.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
 print(f"–ö–ª–∞—Å—Å—ã: {np.unique(y, return_counts=True)}")
 print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å–ø—ã—Ç–∞–Ω–∏–π: {n_trials}")

 # create Time Series CV for —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
 tscv = TimeSeriesSplit(n_splits=cv_folds)

 def objective(trial):
 """Objective function for Optuna"""

 # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
 params = {
 'objective': 'multi:softprob',
 'num_class': len(np.unique(y)),
 'max_depth': trial.suggest_int('max_depth', 3, 12),
 'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
 'n_estimators': trial.suggest_int('n_estimators', 50, 500),
 'subsample': trial.suggest_float('subsample', 0.6, 1.0),
 'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
 'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),
 'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0),
 'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
 'gamma': trial.suggest_float('gamma', 0.0, 5.0),
 'random_state': random_state,
 'n_jobs': 1,
 'verbosity': 0
 }

 # create –º–æ–¥–µ–ª–∏
 model = xgb.XGBClassifier(**params)

 # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
 try:
 scores = cross_val_score(
 model, X, y,
 cv=tscv,
 scoring='accuracy',
 n_jobs=1
 )
 return scores.mean()
 except Exception as e:
 # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–ª–æ—Ö–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ
 return 0.0

 # create study
 study = optuna.create_study(
 direction='maximize',
 sampler=optuna.samplers.TPESampler(seed=random_state),
 pruner=optuna.pruners.MedianPruner(
 n_startup_trials=10,
 n_warmup_steps=5,
 interval_steps=1
 )
 )

 # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
 print(f"\n–ù–∞—á–∞–ª–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏...")
 study.optimize(
 objective,
 n_trials=n_trials,
 timeout=timeout,
 n_jobs=n_jobs,
 show_progress_bar=True
 )

 # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
 best_params = study.best_params_
 best_score = study.best_value

 print(f"\n=== –†–µ–∑—É–ª—å—Ç–∞—Ç—ã Optuna ===")
 print(f"–õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_score:.4f}")
 print(f"–õ—É—á—à–∏–µ parameters:")
 for param, value in best_params.items():
 print(f" {param}: {value}")

 # create –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
 best_model = xgb.XGBClassifier(**best_params)
 best_model.fit(X, y)

 # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
 print(f"\n=== –ê–Ω–∞–ª–∏–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ ===")
 print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –∏—Å–ø—ã—Ç–∞–Ω–∏–π: {len(study.trials)}")
 print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ—Ä–≤–∞–Ω–Ω—ã—Ö –∏—Å–ø—ã—Ç–∞–Ω–∏–π: {len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])}")

 return best_params, study, best_model

def plot_optuna_results(study, figsize=(15, 10)):
 """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Optuna"""

 fig, axes = plt.subplots(2, 2, figsize=figsize)

 # –ì—Ä–∞—Ñ–∏–∫ –∏—Å—Ç–æ—Ä–∏–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
 trials = study.trials
 trial_numbers = [t.number for t in trials if t.state == optuna.trial.TrialState.COMPLETE]
 values = [t.value for t in trials if t.state == optuna.trial.TrialState.COMPLETE]

 axes[0, 0].plot(trial_numbers, values, 'o-', alpha=0.7)
 axes[0, 0].set_title('Optimization History')
 axes[0, 0].set_xlabel('Trial Number')
 axes[0, 0].set_ylabel('Objective Value')
 axes[0, 0].grid(True, alpha=0.3)

 # –ì—Ä–∞—Ñ–∏–∫ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
 try:
 importance = optuna.importance.get_param_importances(study)
 params = list(importance.keys())
 importances = list(importance.values())

 axes[0, 1].barh(params, importances)
 axes[0, 1].set_title('Parameter Importance')
 axes[0, 1].set_xlabel('Importance')
 axes[0, 1].grid(True, alpha=0.3)
 except Exception as e:
 axes[0, 1].text(0.5, 0.5, f'Importance not available:\n{str(e)}',
 ha='center', va='center', transform=axes[0, 1].transAxes)
 axes[0, 1].set_title('Parameter Importance')

 # –ì—Ä–∞—Ñ–∏–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∑–Ω–∞—á–µ–Ω–∏–π
 if len(values) > 0:
 axes[1, 0].hist(values, bins=20, alpha=0.7, edgecolor='black')
 axes[1, 0].axvline(np.mean(values), color='red', linestyle='--',
 label=f'Mean: {np.mean(values):.4f}')
 axes[1, 0].axvline(np.max(values), color='green', linestyle='--',
 label=f'Best: {np.max(values):.4f}')
 axes[1, 0].set_title('Value Distribution')
 axes[1, 0].set_xlabel('Objective Value')
 axes[1, 0].set_ylabel('Frequency')
 axes[1, 0].legend()
 axes[1, 0].grid(True, alpha=0.3)

 # –ì—Ä–∞—Ñ–∏–∫ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏
 if len(values) > 1:
 best_values = np.maximum.accumulate(values)
 axes[1, 1].plot(trial_numbers, best_values, 'g-', linewidth=2, label='Best Value')
 axes[1, 1].plot(trial_numbers, values, 'o-', alpha=0.3, label='All Values')
 axes[1, 1].set_title('Convergence')
 axes[1, 1].set_xlabel('Trial Number')
 axes[1, 1].set_ylabel('Best Objective Value')
 axes[1, 1].legend()
 axes[1, 1].grid(True, alpha=0.3)

 plt.tight_layout()
 plt.show()

def analyze_optuna_study(study):
 """–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Optuna study"""

 print("=== –ê–Ω–∞–ª–∏–∑ Optuna Study ===")

 # –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
 trials = study.trials
 completed_trials = [t for t in trials if t.state == optuna.trial.TrialState.COMPLETE]
 pruned_trials = [t for t in trials if t.state == optuna.trial.TrialState.PRUNED]

 print(f"–í—Å–µ–≥–æ –∏—Å–ø—ã—Ç–∞–Ω–∏–π: {len(trials)}")
 print(f"–ó–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö: {len(completed_trials)}")
 print(f"–ü—Ä–µ—Ä–≤–∞–Ω–Ω—ã—Ö: {len(pruned_trials)}")

 if completed_trials:
 values = [t.value for t in completed_trials]
 print(f"\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–Ω–∞—á–µ–Ω–∏–π:")
 print(f" –õ—É—á—à–µ–µ: {max(values):.4f}")
 print(f" –•—É–¥—à–µ–µ: {min(values):.4f}")
 print(f" –°—Ä–µ–¥–Ω–µ–µ: {np.mean(values):.4f}")
 print(f" –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {np.std(values):.4f}")

 # –ê–Ω–∞–ª–∏–∑ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏
 best_values = np.maximum.accumulate(values)
 improvement = best_values[-1] - best_values[0]
 print(f"\n–£–ª—É—á—à–µ–Ω–∏–µ: {improvement:.4f}")

 # –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
 recent_trials = min(10, len(values))
 recent_values = values[-recent_trials:]
 recent_std = np.std(recent_values)
 print(f"–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å (–ø–æ—Å–ª–µ–¥–Ω–∏–µ {recent_trials} –∏—Å–ø—ã—Ç–∞–Ω–∏–π): {recent_std:.4f}")

 # –ê–Ω–∞–ª–∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
 if completed_trials:
 print(f"\n–ê–Ω–∞–ª–∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:")
 param_names = list(completed_trials[0].params.keys())

 for param_name in param_names:
 param_values = [t.params[param_name] for t in completed_trials]
 if isinstance(param_values[0], (int, float)):
 print(f" {param_name}: {min(param_values):.4f} - {max(param_values):.4f}")
 else:
 unique_values = list(set(param_values))
 print(f" {param_name}: {unique_values}")

# example –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
def example_optuna_usage():
 """example –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Optuna"""

 # create —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
 np.random.seed(42)
 n_samples, n_features = 1000, 20

 # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
 X = np.random.randn(n_samples, n_features)

 # create —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
 y = np.zeros(n_samples)
 for i in range(n_samples):
 if X[i, 0] > 0.5 and X[i, 1] < -0.3:
 y[i] = 1
 elif X[i, 2] > 1.0 or X[i, 3] < -1.0:
 y[i] = 2
 else:
 y[i] = 0

 print("=== example Optuna –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ ===")

 # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
 best_params, study, best_model = optimize_xgboost_optuna(
 X, y, n_trials=50, cv_folds=3
 )

 # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
 plot_optuna_results(study)

 # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
 analyze_optuna_study(study)

 return best_params, study, best_model

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–∞ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ for —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
# best_params, study, best_model = example_optuna_usage()
```

## –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω—Å–∞–º–±–ª–µ–≤—ã—Ö –º–µ—Ç–æ–¥–æ–≤

**–¢–µ–æ—Ä–∏—è:** –ê–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã –∫–æ–º–±–∏–Ω–∏—Ä—É—é—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π for —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. in —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Å—Ñ–µ—Ä–µ —ç—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ –≤–∞–∂–Ω–æ, —Ç–∞–∫ –∫–∞–∫ —Ä–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –≤—ã—è–≤–ª—è—Ç—å —Ä–∞–∑–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã in –¥–∞–Ω–Ω—ã—Ö.

**–ü–æ—á–µ–º—É –∞–Ω—Å–∞–º–±–ª–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã for —Ñ–∏–Ω–∞–Ω—Å–æ–≤:**
- **–°–Ω–∏–∂–µ–Ω–∏–µ —Ä–∏—Å–∫–∞:** –ö–æ–º–±–∏–Ω–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π —Å–Ω–∏–∂–∞–µ—Ç —Ä–∏—Å–∫ –æ—à–∏–±–æ–∫
- **–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ:** –†–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤—ã—è–≤–ª—è—é—Ç —Ä–∞–∑–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
- **–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å:** –ê–Ω—Å–∞–º–±–ª–∏ –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã, —á–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
- **–†–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å:** –£—Å—Ç–æ–π—á–∏–≤—ã –∫ –≤—ã–±—Ä–æ—Å–∞–º and —à—É–º—É

**–¢–∏–ø—ã –∞–Ω—Å–∞–º–±–ª–µ–π:**
1. **Voting:** –ü—Ä–æ—Å—Ç–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
2. **Stacking:** –ú–µ—Ç–∞-–º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è on –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
3. **Blending:** –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
4. **Bagging:** –û–±—É—á–µ–Ω–∏–µ on —Ä–∞–∑–Ω—ã—Ö –ø–æ–¥–≤—ã–±–æ—Ä–∫–∞—Ö –¥–∞–Ω–Ω—ã—Ö

### 1. Voting Classifier

**–¢–µ–æ—Ä–∏—è:** Voting Classifier - —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ–π –º–µ—Ç–æ–¥ –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ. –ú–æ–∂–µ—Ç –±—ã—Ç—å hard voting (–≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ on –∫–ª–∞—Å—Å–∞–º) or soft voting (–≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ on –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º).

**–ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã Voting:**
1. **Hard Voting:** –ö–∞–∂–¥–∞—è –º–æ–¥–µ–ª—å –≥–æ–ª–æ—Å—É–µ—Ç –∑–∞ –∫–ª–∞—Å—Å, –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –∫–ª–∞—Å—Å with –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ–º –≥–æ–ª–æ—Å–æ–≤
2. **Soft Voting:** –ö–∞–∂–¥–∞—è –º–æ–¥–µ–ª—å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏, –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è —Å—Ä–µ–¥–Ω–µ–µ and –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –∫–ª–∞—Å—Å with –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é

**–ü–ª—é—Å—ã Voting:**
- –ü—Ä–æ—Å—Ç–æ—Ç–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏
- –•–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç with —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
- –õ–µ–≥–∫–æ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å

**–ú–∏–Ω—É—Å—ã Voting:**
- not —É—á–∏—Ç—ã–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
- –ú–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º –ø—Ä–∏ –ø–ª–æ—Ö–∏—Ö –º–æ–¥–µ–ª—è—Ö

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Voting Classifier:**

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç –∫–æ–¥:**
1. **create –º–æ–¥–µ–ª–µ–π:** –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ for –∞–Ω—Å–∞–º–±–ª—è
2. **Voting:** –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —Ç–∏–ø –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è (hard/soft)
3. **–û–±—É—á–µ–Ω–∏–µ:** –û–±—É—á–∞–µ—Ç –≤–µ—Å—å –∞–Ω—Å–∞–º–±–ª—å
4. **–û—Ü–µ–Ω–∫–∞:** –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞–Ω—Å–∞–º–±–ª—è

```python
import numpy as np
import pandas as pd
from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
import lightgbm as lgb
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Any, Tuple

def create_ensemble_model(X, y, voting='soft', test_size=0.2, random_state=42):
 """
 create –∞–Ω—Å–∞–º–±–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏ with Voting Classifier

 Args:
 X: –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (samples, features)
 y: –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (samples,)
 voting: –¢–∏–ø –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è ('hard' or 'soft')
 test_size: –î–æ–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
 random_state: Seed for –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏

 Returns:
 tuple: (–∞–Ω—Å–∞–º–±–ª–µ–≤–∞—è –º–æ–¥–µ–ª—å, –º–µ—Ç—Ä–∏–∫–∏, –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏)
 """

 print("=== create Voting Ensemble ===")
 print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {X.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤, {X.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
 print(f"–ö–ª–∞—Å—Å—ã: {np.unique(y, return_counts=True)}")
 print(f"–¢–∏–ø –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è: {voting}")

 # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
 X_train, X_test, y_train, y_test = train_test_split(
 X, y, test_size=test_size, random_state=random_state, stratify=y
 )

 print(f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
 print(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")

 # create –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
 models = {
 'rf': RandomForestClassifier(
 n_estimators=100,
 max_depth=10,
 random_state=random_state,
 n_jobs=-1
 ),
 'xgb': xgb.XGBClassifier(
 n_estimators=100,
 max_depth=6,
 learning_rate=0.1,
 random_state=random_state,
 n_jobs=-1,
 verbosity=0
 ),
 'lgb': lgb.LGBMClassifier(
 n_estimators=100,
 max_depth=6,
 learning_rate=0.1,
 random_state=random_state,
 n_jobs=-1,
 verbose=-1
 )
 }

 print(f"\n–ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏:")
 for name, model in models.items():
 print(f" {name}: {type(model).__name__}")

 # create Voting Classifier
 ensemble = VotingClassifier(
 estimators=list(models.items()),
 voting=voting,
 n_jobs=-1
 )

 # –û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è
 print(f"\n–û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è...")
 ensemble.fit(X_train, y_train)

 # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∞–Ω—Å–∞–º–±–ª—è
 y_pred_ensemble = ensemble.predict(X_test)
 y_pred_proba_ensemble = ensemble.predict_proba(X_test)

 # –û—Ü–µ–Ω–∫–∞ –∞–Ω—Å–∞–º–±–ª—è
 ensemble_accuracy = accuracy_score(y_test, y_pred_ensemble)

 print(f"\n=== –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω—Å–∞–º–±–ª—è ===")
 print(f"Ensemble accuracy: {ensemble_accuracy:.4f}")

 # –û—Ü–µ–Ω–∫–∞ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
 individual_scores = {}
 individual_Predictions = {}

 print(f"\n=== –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π ===")
 for name, model in models.items():
 # –û–±—É—á–µ–Ω–∏–µ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
 model.fit(X_train, y_train)

 # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
 y_pred = model.predict(X_test)
 y_pred_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None

 # –û—Ü–µ–Ω–∫–∞
 accuracy = accuracy_score(y_test, y_pred)
 individual_scores[name] = accuracy
 individual_Predictions[name] = y_pred

 print(f"{name}: {accuracy:.4f}")

 # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
 print(f"\n=== –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ===")
 best_individual = max(individual_scores, key=individual_scores.get)
 best_individual_score = individual_scores[best_individual]

 print(f"–õ—É—á—à–∞—è –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å: {best_individual} ({best_individual_score:.4f})")
 print(f"–ê–Ω—Å–∞–º–±–ª—å: {ensemble_accuracy:.4f}")
 print(f"improve: {ensemble_accuracy - best_individual_score:.4f}")

 # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
 print(f"\n=== Classification Report (Ensemble) ===")
 print(classification_report(y_test, y_pred_ensemble))

 # –ú–µ—Ç—Ä–∏–∫–∏ for –≤–æ–∑–≤—Ä–∞—Ç–∞
 metrics = {
 'ensemble_accuracy': ensemble_accuracy,
 'individual_scores': individual_scores,
 'best_individual': best_individual,
 'improvement': ensemble_accuracy - best_individual_score,
 'confusion_matrix': confusion_matrix(y_test, y_pred_ensemble),
 'Predictions': y_pred_ensemble,
 'probabilities': y_pred_proba_ensemble
 }

 return ensemble, metrics, models

def plot_ensemble_comparison(metrics, figsize=(12, 8)):
 """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∞–Ω—Å–∞–º–±–ª—è and –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""

 fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)

 # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏
 models = list(metrics['individual_scores'].keys()) + ['Ensemble']
 scores = list(metrics['individual_scores'].values()) + [metrics['ensemble_accuracy']]
 colors = ['lightblue'] * len(metrics['individual_scores']) + ['red']

 bars = ax1.bar(models, scores, color=colors, alpha=0.7)
 ax1.set_title('Model Performance Comparison')
 ax1.set_ylabel('Accuracy')
 ax1.set_ylim(0, 1)

 # add –∑–Ω–∞—á–µ–Ω–∏–π on —Å—Ç–æ–ª–±—Ü—ã
 for bar, score in zip(bars, scores):
 height = bar.get_height()
 ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
 f'{score:.3f}', ha='center', va='bottom')

 ax1.grid(True, alpha=0.3)

 # –ì—Ä–∞—Ñ–∏–∫ —É–ª—É—á—à–µ–Ω–∏—è
 individual_scores = list(metrics['individual_scores'].values())
 ensemble_score = metrics['ensemble_accuracy']
 improvements = [ensemble_score - score for score in individual_scores]

 ax2.bar(metrics['individual_scores'].keys(), improvements,
 color='green', alpha=0.7)
 ax2.set_title('Improvement over Individual Models')
 ax2.set_ylabel('Improvement')
 ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)
 ax2.grid(True, alpha=0.3)

 plt.tight_layout()
 plt.show()

def analyze_ensemble_diversity(individual_Predictions, y_test):
 """–ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –∞–Ω—Å–∞–º–±–ª—è"""

 print("=== –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –∞–Ω—Å–∞–º–±–ª—è ===")

 # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏
 model_names = list(individual_Predictions.keys())
 n_models = len(model_names)

 # –ú–∞—Ç—Ä–∏—Ü–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
 agreement_matrix = np.zeros((n_models, n_models))

 for i, model1 in enumerate(model_names):
 for j, model2 in enumerate(model_names):
 if i != j:
 agreement = np.mean(individual_Predictions[model1] == individual_Predictions[model2])
 agreement_matrix[i, j] = agreement

 print(f"–ú–∞—Ç—Ä–∏—Ü–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏:")
 print(f"–°—Ä–µ–¥–Ω—è—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å: {np.mean(agreement_matrix):.4f}")
 print(f"–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å: {np.min(agreement_matrix):.4f}")
 print(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å: {np.max(agreement_matrix):.4f}")

 # –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫
 correct_Predictions = {}
 for name, pred in individual_Predictions.items():
 correct_Predictions[name] = (pred == y_test)

 # –°–ª—É—á–∞–∏, –≥–¥–µ –≤—Å–µ –º–æ–¥–µ–ª–∏ –æ—à–∏–±–ª–∏—Å—å
 all_wrong = np.all([~correct_Predictions[name] for name in model_names], axis=0)
 all_wrong_count = np.sum(all_wrong)

 # –°–ª—É—á–∞–∏, –≥–¥–µ –≤—Å–µ –º–æ–¥–µ–ª–∏ –±—ã–ª–∏ –ø—Ä–∞–≤—ã
 all_correct = np.all([correct_Predictions[name] for name in model_names], axis=0)
 all_correct_count = np.sum(all_correct)

 print(f"\n–ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫:")
 print(f"–í—Å–µ –º–æ–¥–µ–ª–∏ –ø—Ä–∞–≤—ã: {all_correct_count} ({all_correct_count/len(y_test)*100:.1f}%)")
 print(f"–í—Å–µ –º–æ–¥–µ–ª–∏ –æ—à–∏–±–ª–∏—Å—å: {all_wrong_count} ({all_wrong_count/len(y_test)*100:.1f}%)")

 # –°–ª—É—á–∞–∏, –≥–¥–µ –º–Ω–µ–Ω–∏—è —Ä–∞–∑–¥–µ–ª–∏–ª–∏—Å—å
 mixed_cases = len(y_test) - all_correct_count - all_wrong_count
 print(f"–°–º–µ—à–∞–Ω–Ω—ã–µ —Å–ª—É—á–∞–∏: {mixed_cases} ({mixed_cases/len(y_test)*100:.1f}%)")

# example –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
def example_voting_ensemble_usage():
 """example –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Voting Ensemble"""

 # create —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
 np.random.seed(42)
 n_samples, n_features = 1000, 20

 # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
 X = np.random.randn(n_samples, n_features)

 # create —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
 y = np.zeros(n_samples)
 for i in range(n_samples):
 if X[i, 0] > 0.5 and X[i, 1] < -0.3:
 y[i] = 1
 elif X[i, 2] > 1.0 or X[i, 3] < -1.0:
 y[i] = 2
 else:
 y[i] = 0

 print("=== example Voting Ensemble ===")

 # create –∞–Ω—Å–∞–º–±–ª—è
 ensemble, metrics, models = create_ensemble_model(X, y, voting='soft')

 # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
 plot_ensemble_comparison(metrics)

 # –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
 individual_Predictions = {}
 for name, model in models.items():
 individual_Predictions[name] = model.predict(X)

 analyze_ensemble_diversity(individual_Predictions, y)

 return ensemble, metrics, models

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–∞ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ for —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
# ensemble, metrics, models = example_voting_ensemble_usage()
```

### 2. Stacking

**–¢–µ–æ—Ä–∏—è:** Stacking (Stacked Generalization) - —ç—Ç–æ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –º–µ—Ç–æ–¥ –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–∞-–º–æ–¥–µ–ª—å for –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ú–µ—Ç–∞-–º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è on –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –µ–π –Ω–∞—Ö–æ–¥–∏—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã –∏—Ö –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è.

**–ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã Stacking:**
1. **–ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏:** –û–±—É—á–∞—é—Ç—Å—è on –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
2. **–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è:** –ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–µ–ª–∞—é—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è on –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
3. **–ú–µ—Ç–∞-–º–æ–¥–µ–ª—å:** –û–±—É—á–∞–µ—Ç—Å—è on –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
4. **–§–∏–Ω–∞–ª—å–Ω–æ–µ Prediction:** –ú–µ—Ç–∞-–º–æ–¥–µ–ª—å –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π

**–ü–ª—é—Å—ã Stacking:**
- –ë–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π
- –ú–µ—Ç–∞-–º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –≤—ã—É—á–∏—Ç—å –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ dependencies
- –ß–∞—Å—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —á–µ–º Voting

**–ú–∏–Ω—É—Å—ã Stacking:**
- –ë–æ–ª–µ–µ —Å–ª–æ–∂–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
- –¢—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤
- –ú–æ–∂–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–∏—Ç—å—Å—è –ø—Ä–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–µ

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Stacking:**

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç –∫–æ–¥:**
1. **–ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏:** –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–∞–±–æ—Ä —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
2. **–ú–µ—Ç–∞-–º–æ–¥–µ–ª—å:** –í—ã–±–∏—Ä–∞–µ—Ç –º–æ–¥–µ–ª—å for –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
3. **–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è:** –ò—Å–ø–æ–ª—å–∑—É–µ—Ç CV for –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
4. **–û–±—É—á–µ–Ω–∏–µ:** –û–±—É—á–∞–µ—Ç –≤–µ—Å—å —Å—Ç–µ–∫ –º–æ–¥–µ–ª–µ–π

```python
import numpy as np
import pandas as pd
from sklearn.ensemble import StackingClassifier, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
import xgboost as xgb
import lightgbm as lgb
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Any, Tuple

def create_stacking_model(X, y, test_size=0.2, cv_folds=5, random_state=42):
 """
 create Stacking –º–æ–¥–µ–ª–∏

 Args:
 X: –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (samples, features)
 y: –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (samples,)
 test_size: –î–æ–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
 cv_folds: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ–ª–¥–æ–≤ for –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏
 random_state: Seed for –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏

 Returns:
 tuple: (stacking –º–æ–¥–µ–ª—å, –º–µ—Ç—Ä–∏–∫–∏, –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏)
 """

 print("=== create Stacking –º–æ–¥–µ–ª–∏ ===")
 print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {X.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤, {X.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
 print(f"–ö–ª–∞—Å—Å—ã: {np.unique(y, return_counts=True)}")
 print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ–ª–¥–æ–≤ CV: {cv_folds}")

 # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
 X_train, X_test, y_train, y_test = train_test_split(
 X, y, test_size=test_size, random_state=random_state, stratify=y
 )

 print(f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
 print(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")

 # create –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
 base_models = [
 ('rf', RandomForestClassifier(
 n_estimators=100,
 max_depth=10,
 random_state=random_state,
 n_jobs=-1
 )),
 ('xgb', xgb.XGBClassifier(
 n_estimators=100,
 max_depth=6,
 learning_rate=0.1,
 random_state=random_state,
 n_jobs=-1,
 verbosity=0
 )),
 ('lgb', lgb.LGBMClassifier(
 n_estimators=100,
 max_depth=6,
 learning_rate=0.1,
 random_state=random_state,
 n_jobs=-1,
 verbose=-1
 )),
 ('svm', SVC(
 probability=True,
 random_state=random_state
 )),
 ('mlp', MLPClassifier(
 hidden_layer_sizes=(100, 50),
 max_iter=500,
 random_state=random_state
 ))
 ]

 print(f"\n–ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏:")
 for name, model in base_models:
 print(f" {name}: {type(model).__name__}")

 # create –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏
 meta_models = {
 'logistic': LogisticRegression(random_state=random_state, max_iter=1000),
 'rf_meta': RandomForestClassifier(n_estimators=50, random_state=random_state),
 'xgb_meta': xgb.XGBClassifier(n_estimators=50, random_state=random_state, verbosity=0)
 }

 print(f"\n–ú–µ—Ç–∞-–º–æ–¥–µ–ª–∏:")
 for name, model in meta_models.items():
 print(f" {name}: {type(model).__name__}")

 # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –º–µ—Ç–∞-–º–æ–¥–µ–ª–µ–π
 best_meta_model = None
 best_score = 0
 meta_scores = {}

 print(f"\n–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–∞-–º–æ–¥–µ–ª–µ–π...")

 for meta_name, meta_model in meta_models.items():
 # create Stacking –º–æ–¥–µ–ª–∏
 stacking_model = StackingClassifier(
 estimators=base_models,
 final_estimator=meta_model,
 cv=cv_folds,
 n_jobs=-1
 )

 # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
 scores = cross_val_score(
 stacking_model, X_train, y_train,
 cv=cv_folds, scoring='accuracy'
 )

 mean_score = scores.mean()
 meta_scores[meta_name] = mean_score

 print(f" {meta_name}: {mean_score:.4f} ¬± {scores.std():.4f}")

 if mean_score > best_score:
 best_score = mean_score
 best_meta_model = meta_model

 print(f"\n–õ—É—á—à–∞—è –º–µ—Ç–∞-–º–æ–¥–µ–ª—å: {max(meta_scores, key=meta_scores.get)}")

 # create —Ñ–∏–Ω–∞–ª—å–Ω–æ–π Stacking –º–æ–¥–µ–ª–∏
 final_stacking_model = StackingClassifier(
 estimators=base_models,
 final_estimator=best_meta_model,
 cv=cv_folds,
 n_jobs=-1
 )

 # –û–±—É—á–µ–Ω–∏–µ
 print(f"\n–û–±—É—á–µ–Ω–∏–µ Stacking –º–æ–¥–µ–ª–∏...")
 final_stacking_model.fit(X_train, y_train)

 # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
 y_pred_stacking = final_stacking_model.predict(X_test)
 y_pred_proba_stacking = final_stacking_model.predict_proba(X_test)

 # –û—Ü–µ–Ω–∫–∞ Stacking –º–æ–¥–µ–ª–∏
 stacking_accuracy = accuracy_score(y_test, y_pred_stacking)

 print(f"\n=== –†–µ–∑—É–ª—å—Ç–∞—Ç—ã Stacking –º–æ–¥–µ–ª–∏ ===")
 print(f"Stacking accuracy: {stacking_accuracy:.4f}")

 # –û—Ü–µ–Ω–∫–∞ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
 base_scores = {}
 base_Predictions = {}

 print(f"\n=== –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π ===")
 for name, model in base_models:
 # –û–±—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
 model.fit(X_train, y_train)

 # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
 y_pred = model.predict(X_test)
 y_pred_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None

 # –û—Ü–µ–Ω–∫–∞
 accuracy = accuracy_score(y_test, y_pred)
 base_scores[name] = accuracy
 base_Predictions[name] = y_pred

 print(f"{name}: {accuracy:.4f}")

 # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
 print(f"\n=== –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ===")
 best_base = max(base_scores, key=base_scores.get)
 best_base_score = base_scores[best_base]

 print(f"–õ—É—á—à–∞—è –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: {best_base} ({best_base_score:.4f})")
 print(f"Stacking –º–æ–¥–µ–ª—å: {stacking_accuracy:.4f}")
 print(f"improve: {stacking_accuracy - best_base_score:.4f}")

 # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
 print(f"\n=== Classification Report (Stacking) ===")
 print(classification_report(y_test, y_pred_stacking))

 # –ú–µ—Ç—Ä–∏–∫–∏ for –≤–æ–∑–≤—Ä–∞—Ç–∞
 metrics = {
 'stacking_accuracy': stacking_accuracy,
 'base_scores': base_scores,
 'meta_scores': meta_scores,
 'best_base': best_base,
 'best_meta': max(meta_scores, key=meta_scores.get),
 'improvement': stacking_accuracy - best_base_score,
 'confusion_matrix': confusion_matrix(y_test, y_pred_stacking),
 'Predictions': y_pred_stacking,
 'probabilities': y_pred_proba_stacking
 }

 return final_stacking_model, metrics, base_models

def plot_stacking_results(metrics, figsize=(15, 10)):
 """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Stacking"""

 fig, axes = plt.subplots(2, 2, figsize=figsize)

 # –ì—Ä–∞—Ñ–∏–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π and Stacking
 models = list(metrics['base_scores'].keys()) + ['Stacking']
 scores = list(metrics['base_scores'].values()) + [metrics['stacking_accuracy']]
 colors = ['lightblue'] * len(metrics['base_scores']) + ['red']

 bars = axes[0, 0].bar(models, scores, color=colors, alpha=0.7)
 axes[0, 0].set_title('Base Models vs Stacking')
 axes[0, 0].set_ylabel('Accuracy')
 axes[0, 0].set_ylim(0, 1)

 # add –∑–Ω–∞—á–µ–Ω–∏–π on —Å—Ç–æ–ª–±—Ü—ã
 for bar, score in zip(bars, scores):
 height = bar.get_height()
 axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + 0.01,
 f'{score:.3f}', ha='center', va='bottom')

 axes[0, 0].grid(True, alpha=0.3)

 # –ì—Ä–∞—Ñ–∏–∫ –º–µ—Ç–∞-–º–æ–¥–µ–ª–µ–π
 meta_models = list(metrics['meta_scores'].keys())
 meta_scores = list(metrics['meta_scores'].values())

 bars = axes[0, 1].bar(meta_models, meta_scores, color='green', alpha=0.7)
 axes[0, 1].set_title('Meta-Model Performance')
 axes[0, 1].set_ylabel('Accuracy')
 axes[0, 1].set_ylim(0, 1)

 # add –∑–Ω–∞—á–µ–Ω–∏–π on —Å—Ç–æ–ª–±—Ü—ã
 for bar, score in zip(bars, meta_scores):
 height = bar.get_height()
 axes[0, 1].text(bar.get_x() + bar.get_width()/2., height + 0.01,
 f'{score:.3f}', ha='center', va='bottom')

 axes[0, 1].grid(True, alpha=0.3)

 # –ì—Ä–∞—Ñ–∏–∫ —É–ª—É—á—à–µ–Ω–∏—è
 base_scores = list(metrics['base_scores'].values())
 stacking_score = metrics['stacking_accuracy']
 improvements = [stacking_score - score for score in base_scores]

 axes[1, 0].bar(metrics['base_scores'].keys(), improvements,
 color='orange', alpha=0.7)
 axes[1, 0].set_title('Stacking Improvement over Base Models')
 axes[1, 0].set_ylabel('Improvement')
 axes[1, 0].axhline(y=0, color='black', linestyle='-', alpha=0.3)
 axes[1, 0].grid(True, alpha=0.3)

 # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
 cm = metrics['confusion_matrix']
 sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 1])
 axes[1, 1].set_title('Confusion Matrix (Stacking)')
 axes[1, 1].set_xlabel('Predicted')
 axes[1, 1].set_ylabel('True')

 plt.tight_layout()
 plt.show()

def analyze_stacking_contribution(stacking_model, X_test, y_test):
 """–ê–Ω–∞–ª–∏–∑ –≤–∫–ª–∞–¥–∞ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π in Stacking"""

 print("=== –ê–Ω–∞–ª–∏–∑ –≤–∫–ª–∞–¥–∞ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π ===")

 # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
 base_Predictions = stacking_model.transform(X_test)

 # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏
 if hasattr(stacking_model.final_estimator_, 'coef_'):
 # for –ª–∏–Ω–µ–π–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
 weights = stacking_model.final_estimator_.coef_[0]
 print(f"–í–µ—Å–∞ –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏: {weights}")

 # –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
 base_names = [name for name, _ in stacking_model.estimators]
 for name, weight in zip(base_names, weights):
 print(f" {name}: {weight:.4f}")

 # –ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –º–µ–∂–¥—É –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
 base_Predictions_df = pd.DataFrame(
 base_Predictions,
 columns=[name for name, _ in stacking_model.estimators]
 )

 correlation_matrix = base_Predictions_df.corr()

 print(f"\n–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π:")
 print(correlation_matrix.round(3))

 # –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
 mean_correlation = correlation_matrix.values[np.triu_indices_from(correlation_matrix.values, k=1)].mean()
 print(f"\n–°—Ä–µ–¥–Ω—è—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è: {mean_correlation:.4f}")

 if mean_correlation < 0.5:
 print("‚úÖ –•–æ—Ä–æ—à–µ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π")
 elif mean_correlation < 0.7:
 print("‚ö†Ô∏è –£–º–µ—Ä–µ–Ω–Ω–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π")
 else:
 print("‚ùå –ù–∏–∑–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π")

# example –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
def example_stacking_usage():
 """example –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Stacking"""

 # create —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
 np.random.seed(42)
 n_samples, n_features = 1000, 20

 # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
 X = np.random.randn(n_samples, n_features)

 # create —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
 y = np.zeros(n_samples)
 for i in range(n_samples):
 if X[i, 0] > 0.5 and X[i, 1] < -0.3:
 y[i] = 1
 elif X[i, 2] > 1.0 or X[i, 3] < -1.0:
 y[i] = 2
 else:
 y[i] = 0

 print("=== example Stacking ===")

 # create Stacking –º–æ–¥–µ–ª–∏
 stacking_model, metrics, base_models = create_stacking_model(X, y)

 # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
 plot_stacking_results(metrics)

 # –ê–Ω–∞–ª–∏–∑ –≤–∫–ª–∞–¥–∞ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
 analyze_stacking_contribution(stacking_model, X, y)

 return stacking_model, metrics, base_models

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–∞ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ for —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
# stacking_model, metrics, base_models = example_stacking_usage()
```

## –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

**–¢–µ–æ—Ä–∏—è:** –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π for —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Ç—Ä–µ–±—É–µ—Ç –æ—Å–æ–±–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞, —Ç–∞–∫ –∫–∞–∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –º–æ–≥—É—Ç not –æ—Ç—Ä–∞–∂–∞—Ç—å —Ä–µ–∞–ª—å–Ω—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ç–æ—Ä–≥–æ–≤–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏.

**–ü–æ—á–µ–º—É —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã:**
- **–¢–æ—á–Ω–æ—Å—Ç—å not —Ä–∞–≤–Ω–∞ –ø—Ä–∏–±—ã–ª—å–Ω–æ—Å—Ç–∏:** –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–∂–µ—Ç not –æ–∑–Ω–∞—á–∞—Ç—å –ø—Ä–∏–±—ã–ª—å–Ω–æ—Å—Ç—å
- **–ö–ª–∞—Å—Å–æ–≤—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å:** –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —á–∞—Å—Ç–æ –∏–º–µ—é—Ç –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
- **–í—Ä–µ–º–µ–Ω–Ω–∞—è dependency:** –í–∞–∂–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
- **–†–∏—Å–∫-–¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å:** –ù—É–∂–Ω–æ —É—á–∏—Ç—ã–≤–∞—Ç—å —Ä–∏—Å–∫, –∞ not —Ç–æ–ª—å–∫–æ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å

**–¢–∏–ø—ã –º–µ—Ç—Ä–∏–∫:**
1. **–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:** Accuracy, Precision, Recall, F1
2. **–¢–æ—Ä–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏:** Sharpe Ratio, Maximum Drawdown, Win Rate
3. **–í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:** –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –≤–æ –≤—Ä–µ–º–µ–Ω–∏
4. **–†–∏—Å–∫–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏:** VaR, CVaR, Volatility

### 1. –ú–µ—Ç—Ä–∏–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏

**–¢–µ–æ—Ä–∏—è:** –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏–∑–º–µ—Ä—è—é—Ç –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–∏ on basis –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –æ–±—Ä–∞–∑—Ü–æ–≤ on –∫–ª–∞—Å—Å–∞–º.

**–û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:**
- **Accuracy:** –î–æ–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
- **Precision:** –î–æ–ª—è –∏—Å—Ç–∏–Ω–Ω–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö —Å—Ä–µ–¥–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö
- **Recall:** –î–æ–ª—è –∏—Å—Ç–∏–Ω–Ω–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö
- **F1-Score:** –ì–∞—Ä–º–æ–Ω–∏—á–µ—Å–∫–æ–µ —Å—Ä–µ–¥–Ω–µ–µ Precision and Recall

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:**

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç –∫–æ–¥:**
1. **–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è:** –ü–æ–ª—É—á–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
2. **–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫:** –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
3. **–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è:** –°–æ–∑–¥–∞–µ—Ç –≥—Ä–∞—Ñ–∏–∫–∏ for analysis
4. **–ê–Ω–∞–ª–∏–∑:** –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

```python
import numpy as np
import pandas as pd
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
 f1_score, confusion_matrix, classification_report,
 roc_auc_score, roc_curve, precision_recall_curve)
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, Any, Tuple, List

def evaluate_model(model, X_test, y_test, model_name="Model"):
 """
 –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏

 Args:
 model: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
 X_test: –¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
 y_test: –¢–µ—Å—Ç–æ–≤—ã–µ –º–µ—Ç–∫–∏
 model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ for –æ—Ç—á–µ—Ç–æ–≤

 Returns:
 dict: –°–ª–æ–≤–∞—Ä—å with –º–µ—Ç—Ä–∏–∫–∞–º–∏ and —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
 """

 print(f"=== –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏: {model_name} ===")
 print(f"–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: {len(y_test)} –æ–±—Ä–∞–∑—Ü–æ–≤")
 print(f"–ö–ª–∞—Å—Å—ã: {np.unique(y_test, return_counts=True)}")

 # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
 y_pred = model.predict(X_test)
 y_pred_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None

 # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
 accuracy = accuracy_score(y_test, y_pred)
 precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
 recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
 f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)

 # –ú–µ—Ç—Ä–∏–∫–∏ on –∫–ª–∞—Å—Å–∞–º
 precision_per_class = precision_score(y_test, y_pred, average=None, zero_division=0)
 recall_per_class = recall_score(y_test, y_pred, average=None, zero_division=0)
 f1_per_class = f1_score(y_test, y_pred, average=None, zero_division=0)

 # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
 cm = confusion_matrix(y_test, y_pred)

 # ROC AUC (for –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏)
 roc_auc = None
 if len(np.unique(y_test)) == 2 and y_pred_proba is not None:
 roc_auc = roc_auc_score(y_test, y_pred_proba[:, 1])

 # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
 results = {
 'model_name': model_name,
 'accuracy': accuracy,
 'precision': precision,
 'recall': recall,
 'f1': f1,
 'precision_per_class': precision_per_class,
 'recall_per_class': recall_per_class,
 'f1_per_class': f1_per_class,
 'confusion_matrix': cm,
 'roc_auc': roc_auc,
 'Predictions': y_pred,
 'probabilities': y_pred_proba
 }

 # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
 print(f"\n=== –†–µ–∑—É–ª—å—Ç–∞—Ç—ã ===")
 print(f"Accuracy: {accuracy:.4f}")
 print(f"Precision: {precision:.4f}")
 print(f"Recall: {recall:.4f}")
 print(f"F1-Score: {f1:.4f}")

 if roc_auc is not None:
 print(f"ROC AUC: {roc_auc:.4f}")

 # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
 print(f"\n=== Classification Report ===")
 print(classification_report(y_test, y_pred))

 return results

def plot_classification_metrics(results, figsize=(15, 10)):
 """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""

 fig, axes = plt.subplots(2, 2, figsize=figsize)

 # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
 cm = results['confusion_matrix']
 sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0])
 axes[0, 0].set_title('Confusion Matrix')
 axes[0, 0].set_xlabel('Predicted')
 axes[0, 0].set_ylabel('True')

 # –ú–µ—Ç—Ä–∏–∫–∏ on –∫–ª–∞—Å—Å–∞–º
 classes = range(len(results['precision_per_class']))
 x = np.arange(len(classes))
 width = 0.25

 axes[0, 1].bar(x - width, results['precision_per_class'], width, label='Precision', alpha=0.8)
 axes[0, 1].bar(x, results['recall_per_class'], width, label='Recall', alpha=0.8)
 axes[0, 1].bar(x + width, results['f1_per_class'], width, label='F1-Score', alpha=0.8)

 axes[0, 1].set_title('Metrics per Class')
 axes[0, 1].set_xlabel('Class')
 axes[0, 1].set_ylabel('Score')
 axes[0, 1].set_xticks(x)
 axes[0, 1].set_xticklabels(classes)
 axes[0, 1].legend()
 axes[0, 1].grid(True, alpha=0.3)

 # –û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
 metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
 values = [results['accuracy'], results['precision'], results['recall'], results['f1']]

 bars = axes[1, 0].bar(metrics, values, color=['skyblue', 'lightgreen', 'lightcoral', 'gold'], alpha=0.8)
 axes[1, 0].set_title('Overall Metrics')
 axes[1, 0].set_ylabel('Score')
 axes[1, 0].set_ylim(0, 1)

 # add –∑–Ω–∞—á–µ–Ω–∏–π on —Å—Ç–æ–ª–±—Ü—ã
 for bar, value in zip(bars, values):
 height = bar.get_height()
 axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + 0.01,
 f'{value:.3f}', ha='center', va='bottom')

 axes[1, 0].grid(True, alpha=0.3)

 # ROC –∫—Ä–∏–≤–∞—è (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞)
 if results['roc_auc'] is not None:
 fpr, tpr, _ = roc_curve(results['y_test'], results['probabilities'][:, 1])
 axes[1, 1].plot(fpr, tpr, color='darkorange', lw=2,
 label=f'ROC curve (AUC = {results["roc_auc"]:.2f})')
 axes[1, 1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
 axes[1, 1].set_xlim([0.0, 1.0])
 axes[1, 1].set_ylim([0.0, 1.05])
 axes[1, 1].set_xlabel('False Positive Rate')
 axes[1, 1].set_ylabel('True Positive Rate')
 axes[1, 1].set_title('ROC Curve')
 axes[1, 1].legend(loc="lower right")
 axes[1, 1].grid(True, alpha=0.3)
 else:
 axes[1, 1].text(0.5, 0.5, 'ROC Curve not available\nfor multiclass problem',
 ha='center', va='center', transform=axes[1, 1].transAxes)
 axes[1, 1].set_title('ROC Curve')

 plt.tight_layout()
 plt.show()

def analyze_class_balance(y_test, y_pred):
 """–ê–Ω–∞–ª–∏–∑ –±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤"""

 print("=== –ê–Ω–∞–ª–∏–∑ –±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤ ===")

 # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
 unique_classes, counts = np.unique(y_test, return_counts=True)
 total_samples = len(y_test)

 print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ in —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ:")
 for class_label, count in zip(unique_classes, counts):
 percentage = count / total_samples * 100
 print(f" –ö–ª–∞—Å—Å {class_label}: {count} ({percentage:.1f}%)")

 # –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
 pred_unique, pred_counts = np.unique(y_pred, return_counts=True)

 print(f"\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
 for class_label, count in zip(pred_unique, pred_counts):
 percentage = count / total_samples * 100
 print(f" –ö–ª–∞—Å—Å {class_label}: {count} ({percentage:.1f}%)")

 # –ê–Ω–∞–ª–∏–∑ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞
 max_count = max(counts)
 min_count = min(counts)
 imbalance_ratio = max_count / min_count

 print(f"\n–ê–Ω–∞–ª–∏–∑ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞:")
 print(f" –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {imbalance_ratio:.2f}:1")

 if imbalance_ratio > 10:
 print(" ‚ö†Ô∏è –°–∏–ª—å–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤")
 elif imbalance_ratio > 3:
 print(" ‚ö†Ô∏è –£–º–µ—Ä–µ–Ω–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤")
 else:
 print(" ‚úÖ –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã")

# example –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
def example_classification_metrics_usage():
 """example –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""

 from sklearn.ensemble import RandomForestClassifier
 from sklearn.datasets import make_classification

 # create —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
 X, y = make_classification(
 n_samples=1000, n_features=20, n_classes=3,
 n_informative=15, n_redundant=5, random_state=42
 )

 # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
 from sklearn.model_selection import train_test_split
 X_train, X_test, y_train, y_test = train_test_split(
 X, y, test_size=0.2, random_state=42, stratify=y
 )

 # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
 model = RandomForestClassifier(n_estimators=100, random_state=42)
 model.fit(X_train, y_train)

 print("=== example –º–µ—Ç—Ä–∏–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ ===")

 # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
 results = evaluate_model(model, X_test, y_test, "Random Forest")

 # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
 plot_classification_metrics(results)

 # –ê–Ω–∞–ª–∏–∑ –±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤
 analyze_class_balance(y_test, results['Predictions'])

 return results

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–∞ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ for —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
# results = example_classification_metrics_usage()
```

### 2. –¢–æ—Ä–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏

**–¢–µ–æ—Ä–∏—è:** –¢–æ—Ä–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏–∑–º–µ—Ä—è—é—Ç —Ä–µ–∞–ª—å–Ω—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ç–æ—Ä–≥–æ–≤–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏, —É—á–∏—Ç—ã–≤–∞—è not —Ç–æ–ª—å–∫–æ —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π, –Ω–æ and —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.

**–û—Å–Ω–æ–≤–Ω—ã–µ —Ç–æ—Ä–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏:**
- **Sharpe Ratio:** –û—Ç–Ω–æ—à–µ–Ω–∏–µ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –∫ —Ä–∏—Å–∫—É
- **Maximum Drawdown:** –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø–æ—Ç–µ—Ä—è from –ø–∏–∫–∞
- **Win Rate:** –î–æ–ª—è –ø—Ä–∏–±—ã–ª—å–Ω—ã—Ö —Å–¥–µ–ª–æ–∫
- **Profit Factor:** –û—Ç–Ω–æ—à–µ–Ω–∏–µ –ø—Ä–∏–±—ã–ª–∏ –∫ —É–±—ã—Ç–∫–∞–º
- **Calmar Ratio:** –û—Ç–Ω–æ—à–µ–Ω–∏–µ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –∫ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ—Å–∞–¥–∫–µ

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ—Ä–≥–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫:**

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç –∫–æ–¥:**
1. **–†–∞—Å—á–µ—Ç –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏:** –í—ã—á–∏—Å–ª—è–µ—Ç –ø—Ä–∏–±—ã–ª—å–Ω–æ—Å—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
2. **–†–∏—Å–∫–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏:** –û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–∏—Å–∫ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
3. **–¢–æ—Ä–≥–æ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏:** –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Ç–æ—Ä–≥–æ–≤–ª–∏
4. **–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è:** –°–æ–∑–¥–∞–µ—Ç –≥—Ä–∞—Ñ–∏–∫–∏ for analysis

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, Any, Tuple, List
from sklearn.metrics import accuracy_score

def calculate_trading_metrics(y_true, y_pred, returns, transaction_costs=0.001):
 """
 –†–∞—Å—á–µ—Ç —Ç–æ—Ä–≥–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫ for —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏

 Args:
 y_true: –ò—Å—Ç–∏–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã (0: –ø—Ä–æ–¥–∞–∂–∞, 1: —É–¥–µ—Ä–∂–∞–Ω–∏–µ, 2: –ø–æ–∫—É–ø–∫–∞)
 y_pred: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã
 returns: –î–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –∞–∫—Ç–∏–≤–æ–≤
 transaction_costs: –¢—Ä–∞–Ω–∑–∞–∫—Ü–∏–æ–Ω–Ω—ã–µ –∏–∑–¥–µ—Ä–∂–∫–∏ (–¥–æ–ª—è from —Å–¥–µ–ª–∫–∏)

 Returns:
 dict: –°–ª–æ–≤–∞—Ä—å with —Ç–æ—Ä–≥–æ–≤—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
 """

 print("=== –†–∞—Å—á–µ—Ç —Ç–æ—Ä–≥–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫ ===")
 print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–¥–µ–ª–æ–∫: {len(y_true)}")
 print(f"–¢—Ä–∞–Ω–∑–∞–∫—Ü–∏–æ–Ω–Ω—ã–µ –∏–∑–¥–µ—Ä–∂–∫–∏: {transaction_costs*100:.2f}%")

 # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
 accuracy = accuracy_score(y_true, y_pred)

 # create —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤
 # 0: –ø—Ä–æ–¥–∞–∂–∞ (-1), 1: —É–¥–µ—Ä–∂–∞–Ω–∏–µ (0), 2: –ø–æ–∫—É–ø–∫–∞ (1)
 signal_mapping = {0: -1, 1: 0, 2: 1}
 y_true_signals = np.array([signal_mapping[label] for label in y_true])
 y_pred_signals = np.array([signal_mapping[label] for label in y_pred])

 # –†–∞—Å—á–µ—Ç –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
 strategy_returns = returns * y_pred_signals

 # –£—á–µ—Ç —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–æ–Ω–Ω—ã—Ö –∏–∑–¥–µ—Ä–∂–µ–∫
 position_changes = np.diff(y_pred_signals, prepend=y_pred_signals[0])
 transaction_costs_total = np.abs(position_changes) * transaction_costs
 strategy_returns_net = strategy_returns - transaction_costs_total

 # –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–æ—Ä–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
 total_return = np.sum(strategy_returns_net)
 annualized_return = np.mean(strategy_returns_net) * 252

 # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
 volatility = np.std(strategy_returns_net) * np.sqrt(252)

 # Sharpe Ratio
 if volatility > 0:
 sharpe_ratio = annualized_return / volatility
 else:
 sharpe_ratio = 0

 # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ—Å–∞–¥–∫–∞
 cumulative_returns = np.cumprod(1 + strategy_returns_net)
 running_max = np.maximum.accumulate(cumulative_returns)
 drawdown = (cumulative_returns - running_max) / running_max
 max_drawdown = np.min(drawdown)

 # Calmar Ratio
 if abs(max_drawdown) > 0:
 calmar_ratio = annualized_return / abs(max_drawdown)
 else:
 calmar_ratio = np.inf

 # Win Rate
 profitable_trades = strategy_returns_net > 0
 win_rate = np.mean(profitable_trades) if len(profitable_trades) > 0 else 0

 # Profit Factor
 gross_profit = np.sum(strategy_returns_net[strategy_returns_net > 0])
 gross_loss = abs(np.sum(strategy_returns_net[strategy_returns_net < 0]))
 profit_factor = gross_profit / gross_loss if gross_loss > 0 else np.inf

 # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–¥–µ–ª–æ–∫
 num_trades = np.sum(np.abs(position_changes))

 # –°—Ä–µ–¥–Ω—è—è –ø—Ä–∏–±—ã–ª—å/—É–±—ã—Ç–æ–∫
 avg_profit = np.mean(strategy_returns_net[strategy_returns_net > 0]) if np.any(strategy_returns_net > 0) else 0
 avg_loss = np.mean(strategy_returns_net[strategy_returns_net < 0]) if np.any(strategy_returns_net < 0) else 0

 # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
 metrics = {
 'accuracy': accuracy,
 'total_return': total_return,
 'annualized_return': annualized_return,
 'volatility': volatility,
 'sharpe_ratio': sharpe_ratio,
 'max_drawdown': max_drawdown,
 'calmar_ratio': calmar_ratio,
 'win_rate': win_rate,
 'profit_factor': profit_factor,
 'num_trades': num_trades,
 'avg_profit': avg_profit,
 'avg_loss': avg_loss,
 'strategy_returns': strategy_returns_net,
 'cumulative_returns': cumulative_returns,
 'drawdown': drawdown
 }

 # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
 print(f"\n=== –¢–æ—Ä–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ ===")
 print(f"Accuracy: {accuracy:.4f}")
 print(f"Total Return: {total_return:.4f}")
 print(f"Annualized Return: {annualized_return:.4f}")
 print(f"Volatility: {volatility:.4f}")
 print(f"Sharpe Ratio: {sharpe_ratio:.4f}")
 print(f"Max Drawdown: {max_drawdown:.4f}")
 print(f"Calmar Ratio: {calmar_ratio:.4f}")
 print(f"Win Rate: {win_rate:.4f}")
 print(f"Profit Factor: {profit_factor:.4f}")
 print(f"Number of Trades: {num_trades}")

 return metrics

def plot_trading_metrics(metrics, figsize=(15, 12)):
 """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ—Ä–≥–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫"""

 fig, axes = plt.subplots(2, 2, figsize=figsize)

 # –ö—É–º—É–ª—è—Ç–∏–≤–Ω–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å
 cumulative_returns = metrics['cumulative_returns']
 axes[0, 0].plot(cumulative_returns, label='Strategy', linewidth=2)
 axes[0, 0].axhline(y=1, color='black', linestyle='--', alpha=0.5, label='Break-even')
 axes[0, 0].set_title('Cumulative Returns')
 axes[0, 0].set_xlabel('Time')
 axes[0, 0].set_ylabel('Cumulative Return')
 axes[0, 0].legend()
 axes[0, 0].grid(True, alpha=0.3)

 # –ü—Ä–æ—Å–∞–¥–∫–∞
 drawdown = metrics['drawdown']
 axes[0, 1].fill_between(range(len(drawdown)), drawdown, 0,
 color='red', alpha=0.3, label='Drawdown')
 axes[0, 1].plot(drawdown, color='red', linewidth=1)
 axes[0, 1].set_title('Drawdown')
 axes[0, 1].set_xlabel('Time')
 axes[0, 1].set_ylabel('Drawdown')
 axes[0, 1].legend()
 axes[0, 1].grid(True, alpha=0.3)

 # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏
 strategy_returns = metrics['strategy_returns']
 axes[1, 0].hist(strategy_returns, bins=50, alpha=0.7, edgecolor='black')
 axes[1, 0].axvline(x=0, color='red', linestyle='--', alpha=0.7)
 axes[1, 0].set_title('Return Distribution')
 axes[1, 0].set_xlabel('Return')
 axes[1, 0].set_ylabel('Frequency')
 axes[1, 0].grid(True, alpha=0.3)

 # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
 metric_names = ['Sharpe Ratio', 'Calmar Ratio', 'Win Rate', 'Profit Factor']
 metric_values = [
 metrics['sharpe_ratio'],
 metrics['calmar_ratio'],
 metrics['win_rate'],
 metrics['profit_factor']
 ]

 # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è for –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
 metric_values_limited = [min(val, 10) if val != np.inf else 10 for val in metric_values]

 bars = axes[1, 1].bar(metric_names, metric_values_limited,
 color=['skyblue', 'lightgreen', 'lightcoral', 'gold'], alpha=0.8)
 axes[1, 1].set_title('Key Trading Metrics')
 axes[1, 1].set_ylabel('Value')
 axes[1, 1].tick_params(axis='x', rotation=45)

 # add –∑–Ω–∞—á–µ–Ω–∏–π on —Å—Ç–æ–ª–±—Ü—ã
 for bar, value in zip(bars, metric_values):
 height = bar.get_height()
 if value == np.inf:
 label = '‚àû'
 else:
 label = f'{value:.3f}'
 axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.01,
 label, ha='center', va='bottom')

 axes[1, 1].grid(True, alpha=0.3)

 plt.tight_layout()
 plt.show()

def analyze_trading_performance(metrics):
 """–ê–Ω–∞–ª–∏–∑ —Ç–æ—Ä–≥–æ–≤–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""

 print("=== –ê–Ω–∞–ª–∏–∑ —Ç–æ—Ä–≥–æ–≤–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ ===")

 # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
 sharpe = metrics['sharpe_ratio']
 calmar = metrics['calmar_ratio']
 win_rate = metrics['win_rate']
 profit_factor = metrics['profit_factor']

 print(f"\n–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏:")

 # Sharpe Ratio
 if sharpe > 2:
 print(f"‚úÖ –û—Ç–ª–∏—á–Ω—ã–π Sharpe Ratio: {sharpe:.3f}")
 elif sharpe > 1:
 print(f"‚úÖ –•–æ—Ä–æ—à–∏–π Sharpe Ratio: {sharpe:.3f}")
 elif sharpe > 0.5:
 print(f"‚ö†Ô∏è –£–º–µ—Ä–µ–Ω–Ω—ã–π Sharpe Ratio: {sharpe:.3f}")
 else:
 print(f"‚ùå –ü–ª–æ—Ö–æ–π Sharpe Ratio: {sharpe:.3f}")

 # Calmar Ratio
 if calmar > 3:
 print(f"‚úÖ –û—Ç–ª–∏—á–Ω—ã–π Calmar Ratio: {calmar:.3f}")
 elif calmar > 1:
 print(f"‚úÖ –•–æ—Ä–æ—à–∏–π Calmar Ratio: {calmar:.3f}")
 elif calmar > 0.5:
 print(f"‚ö†Ô∏è –£–º–µ—Ä–µ–Ω–Ω—ã–π Calmar Ratio: {calmar:.3f}")
 else:
 print(f"‚ùå –ü–ª–æ—Ö–æ–π Calmar Ratio: {calmar:.3f}")

 # Win Rate
 if win_rate > 0.6:
 print(f"‚úÖ –í—ã—Å–æ–∫–∏–π Win Rate: {win_rate:.3f}")
 elif win_rate > 0.5:
 print(f"‚úÖ –•–æ—Ä–æ—à–∏–π Win Rate: {win_rate:.3f}")
 elif win_rate > 0.4:
 print(f"‚ö†Ô∏è –£–º–µ—Ä–µ–Ω–Ω—ã–π Win Rate: {win_rate:.3f}")
 else:
 print(f"‚ùå –ù–∏–∑–∫–∏–π Win Rate: {win_rate:.3f}")

 # Profit Factor
 if profit_factor > 2:
 print(f"‚úÖ –û—Ç–ª–∏—á–Ω—ã–π Profit Factor: {profit_factor:.3f}")
 elif profit_factor > 1.5:
 print(f"‚úÖ –•–æ—Ä–æ—à–∏–π Profit Factor: {profit_factor:.3f}")
 elif profit_factor > 1:
 print(f"‚ö†Ô∏è –£–º–µ—Ä–µ–Ω–Ω—ã–π Profit Factor: {profit_factor:.3f}")
 else:
 print(f"‚ùå –ü–ª–æ—Ö–æ–π Profit Factor: {profit_factor:.3f}")

 # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞
 print(f"\n–û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞:")
 if sharpe > 1 and calmar > 1 and win_rate > 0.5 and profit_factor > 1.5:
 print("üü¢ –°—Ç—Ä–∞—Ç–µ–≥–∏—è –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –æ—Ç–ª–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
 elif sharpe > 0.5 and calmar > 0.5 and win_rate > 0.4 and profit_factor > 1:
 print("üü° –°—Ç—Ä–∞—Ç–µ–≥–∏—è –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —É–º–µ—Ä–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
 else:
 print("üî¥ –°—Ç—Ä–∞—Ç–µ–≥–∏—è —Ç—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è")

# example –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
def example_trading_metrics_usage():
 """example –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ—Ä–≥–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫"""

 # create —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
 np.random.seed(42)
 n_samples = 1000

 # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏
 returns = np.random.normal(0.001, 0.02, n_samples) # 0.1% —Å—Ä–µ–¥–Ω—è—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å, 2% –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å

 # create –∏—Å—Ç–∏–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤ (—Å—Ç—Ä–∞—Ç–µ–≥–∏—è)
 y_true = np.random.choice([0, 1, 2], n_samples, p=[0.3, 0.4, 0.3])

 # create –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π (with –Ω–µ–∫–æ—Ç–æ—Ä–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é)
 y_pred = y_true.copy()
 # –î–æ–±–∞–≤–ª—è–µ–º –æ—à–∏–±–∫–∏
 error_indices = np.random.choice(n_samples, size=int(n_samples * 0.3), replace=False)
 y_pred[error_indices] = np.random.choice([0, 1, 2], len(error_indices))

 print("=== example —Ç–æ—Ä–≥–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫ ===")

 # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
 metrics = calculate_trading_metrics(y_true, y_pred, returns)

 # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
 plot_trading_metrics(metrics)

 # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
 analyze_trading_performance(metrics)

 return metrics

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–∞ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ for —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
# metrics = example_trading_metrics_usage()
```

## –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π example

**–¢–µ–æ—Ä–∏—è:** –ü–æ–ª–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è —Ç–æ—Ä–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –≤–∫–ª—é—á–∞–µ—Ç in —Å–µ–±—è –≤—Å–µ —ç—Ç–∞–ø—ã: from –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö to –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –≠—Ç–æ—Ç example –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é ML-–º–æ–¥–µ–ª–∏ for —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

**–≠—Ç–∞–ø—ã –ø–æ–ª–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞:**
1. **–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö:** –ó–∞–≥—Ä—É–∑–∫–∞ and –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
2. **–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:** Train/Validation/Test
3. **–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π:** –†–∞–∑–ª–∏—á–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã
4. **–í–∞–ª–∏–¥–∞—Ü–∏—è:** Time Series CV
5. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è:** Hyperparameter tuning
6. **–ê–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ:** –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
7. **–û—Ü–µ–Ω–∫–∞:** –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–µ and —Ç–æ—Ä–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞:**

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç –∫–æ–¥:**
1. **–ü–æ–ª–Ω—ã–π pipeline:** from –¥–∞–Ω–Ω—ã—Ö to –≥–æ—Ç–æ–≤–æ–π –º–æ–¥–µ–ª–∏
2. **–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã:** –¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Ä–∞–∑–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã
3. **–í–∞–ª–∏–¥–∞—Ü–∏—è:** –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã for –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
4. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è:** –ù–∞—Ö–æ–¥–∏—Ç –ª—É—á—à–∏–µ parameters
5. **–ê–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ:** –ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –ª—É—á—à–∏–µ –º–æ–¥–µ–ª–∏
6. **–û—Ü–µ–Ω–∫–∞:** –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.metrics import accuracy_score, classification_report
import xgboost as xgb
import lightgbm as lgb
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, Any, Tuple, List
import warnings
warnings.filterwarnings('ignore')

def train_complete_trading_model(X, y, returns=None, test_size=0.2,
 validation_size=0.2, random_state=42):
 """
 –ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ç–æ—Ä–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏

 Args:
 X: –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (samples, features)
 y: –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (samples,)
 returns: –î–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –∞–∫—Ç–∏–≤–æ–≤ (samples,)
 test_size: –î–æ–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
 validation_size: –î–æ–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
 random_state: Seed for –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏

 Returns:
 dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è and –º–µ—Ç—Ä–∏–∫–∏
 """

 print("=== –ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ç–æ—Ä–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏ ===")
 print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {X.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤, {X.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
 print(f"–ö–ª–∞—Å—Å—ã: {np.unique(y, return_counts=True)}")

 # 1. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
 print(f"\n1. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")

 # –°–Ω–∞—á–∞–ª–∞ –æ—Ç–¥–µ–ª—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
 X_temp, X_test, y_temp, y_test = train_test_split(
 X, y, test_size=test_size, random_state=random_state, stratify=y
 )

 # –ó–∞—Ç–µ–º —Ä–∞–∑–¥–µ–ª—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –¥–∞–Ω–Ω—ã–µ on train and validation
 X_train, X_val, y_train, y_val = train_test_split(
 X_temp, y_temp, test_size=validation_size/(1-test_size),
 random_state=random_state, stratify=y_temp
 )

 print(f" Train: {X_train.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
 print(f" Validation: {X_val.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
 print(f" Test: {X_test.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")

 # 2. –û–±—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
 print(f"\n2. –û–±—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π...")

 models = {}
 model_scores = {}

 # Random Forest
 print(" –û–±—É—á–µ–Ω–∏–µ Random Forest...")
 rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=random_state)
 rf.fit(X_train, y_train)
 rf_score = rf.score(X_val, y_val)
 models['rf'] = rf
 model_scores['rf'] = rf_score
 print(f" Validation accuracy: {rf_score:.4f}")

 # XGBoost
 print(" –û–±—É—á–µ–Ω–∏–µ XGBoost...")
 xgb_model = xgb.XGBClassifier(n_estimators=100, max_depth=6, random_state=random_state, verbosity=0)
 xgb_model.fit(X_train, y_train)
 xgb_score = xgb_model.score(X_val, y_val)
 models['xgb'] = xgb_model
 model_scores['xgb'] = xgb_score
 print(f" Validation accuracy: {xgb_score:.4f}")

 # LightGBM
 print(" –û–±—É—á–µ–Ω–∏–µ LightGBM...")
 lgb_model = lgb.LGBMClassifier(n_estimators=100, max_depth=6, random_state=random_state, verbose=-1)
 lgb_model.fit(X_train, y_train)
 lgb_score = lgb_model.score(X_val, y_val)
 models['lgb'] = lgb_model
 model_scores['lgb'] = lgb_score
 print(f" Validation accuracy: {lgb_score:.4f}")

 # 3. Time Series Cross Validation
 print(f"\n3. Time Series Cross Validation...")

 # –û–±—ä–µ–¥–∏–Ω—è–µ–º train and validation for CV
 X_cv = np.vstack([X_train, X_val])
 y_cv = np.hstack([y_train, y_val])

 # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å for CV
 best_model_name = max(model_scores, key=model_scores.get)
 best_model = models[best_model_name]

 print(f" –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_name} ({model_scores[best_model_name]:.4f})")

 # –í—ã–ø–æ–ª–Ω—è–µ–º TSCV
 tscv = TimeSeriesSplit(n_splits=5)
 cv_scores = []

 for fold, (train_idx, val_idx) in enumerate(tscv.split(X_cv)):
 X_fold_train, X_fold_val = X_cv[train_idx], X_cv[val_idx]
 y_fold_train, y_fold_val = y_cv[train_idx], y_cv[val_idx]

 # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –º–æ–¥–µ–ª–∏
 fold_model = type(best_model)(**best_model.get_params())
 fold_model.fit(X_fold_train, y_fold_train)

 fold_score = fold_model.score(X_fold_val, y_fold_val)
 cv_scores.append(fold_score)

 print(f" Fold {fold+1}: {fold_score:.4f}")

 cv_mean = np.mean(cv_scores)
 cv_std = np.std(cv_scores)
 print(f" CV Mean: {cv_mean:.4f} ¬± {cv_std:.4f}")

 # 4. create –∞–Ω—Å–∞–º–±–ª—è
 print(f"\n4. create –∞–Ω—Å–∞–º–±–ª—è...")

 # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ø-3 –º–æ–¥–µ–ª–∏
 top_models = sorted(model_scores.items(), key=lambda x: x[1], reverse=True)[:3]

 ensemble_models = []
 for name, score in top_models:
 ensemble_models.append((name, models[name]))
 print(f" {name}: {score:.4f}")

 # –°–æ–∑–¥–∞–µ–º Voting Classifier
 ensemble = VotingClassifier(
 estimators=ensemble_models,
 voting='soft',
 n_jobs=-1
 )

 # –û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è
 ensemble.fit(X_train, y_train)
 ensemble_score = ensemble.score(X_val, y_val)
 print(f" Ensemble validation accuracy: {ensemble_score:.4f}")

 # 5. –û—Ü–µ–Ω–∫–∞ on —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
 print(f"\n5. –û—Ü–µ–Ω–∫–∞ on —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")

 # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
 test_Predictions = {}
 test_scores = {}

 for name, model in models.items():
 pred = model.predict(X_test)
 score = accuracy_score(y_test, pred)
 test_Predictions[name] = pred
 test_scores[name] = score
 print(f" {name}: {score:.4f}")

 # –ê–Ω—Å–∞–º–±–ª—å
 ensemble_pred = ensemble.predict(X_test)
 ensemble_score = accuracy_score(y_test, ensemble_pred)
 test_Predictions['ensemble'] = ensemble_pred
 test_scores['ensemble'] = ensemble_score
 print(f" Ensemble: {ensemble_score:.4f}")

 # 6. –¢–æ—Ä–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)
 trading_metrics = None
 if returns is not None:
 print(f"\n6. –†–∞—Å—á–µ—Ç —Ç–æ—Ä–≥–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫...")

 # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ for —Ç–æ—Ä–≥–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫
 test_returns = returns[-len(y_test):]

 # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ for –∞–Ω—Å–∞–º–±–ª—è
 trading_metrics = calculate_trading_metrics(
 y_test, ensemble_pred, test_returns
 )

 # 7. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
 print(f"\n=== –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã ===")
 print(f"–õ—É—á—à–∞—è –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å: {max(test_scores, key=test_scores.get)}")
 print(f"–õ—É—á—à–∏–π –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π score: {max(test_scores.values()):.4f}")
 print(f"Ensemble score: {ensemble_score:.4f}")
 print(f"CV score: {cv_mean:.4f} ¬± {cv_std:.4f}")

 if trading_metrics:
 print(f"Sharpe Ratio: {trading_metrics['sharpe_ratio']:.4f}")
 print(f"Max Drawdown: {trading_metrics['max_drawdown']:.4f}")
 print(f"Win Rate: {trading_metrics['win_rate']:.4f}")

 # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
 print(f"\n=== Classification Report (Ensemble) ===")
 print(classification_report(y_test, ensemble_pred))

 # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã for –≤–æ–∑–≤—Ä–∞—Ç–∞
 results = {
 'models': models,
 'ensemble': ensemble,
 'model_scores': model_scores,
 'test_scores': test_scores,
 'cv_scores': cv_scores,
 'cv_mean': cv_mean,
 'cv_std': cv_std,
 'test_Predictions': test_Predictions,
 'trading_metrics': trading_metrics,
 'best_model': best_model_name,
 'ensemble_score': ensemble_score
 }

 return results

def plot_complete_results(results, figsize=(15, 12)):
 """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–ª–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""

 fig, axes = plt.subplots(2, 2, figsize=figsize)

 # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
 models = list(results['test_scores'].keys())
 scores = list(results['test_scores'].values())

 bars = axes[0, 0].bar(models, scores, color='skyblue', alpha=0.8)
 axes[0, 0].set_title('Model Performance Comparison')
 axes[0, 0].set_ylabel('Accuracy')
 axes[0, 0].set_ylim(0, 1)
 axes[0, 0].tick_params(axis='x', rotation=45)

 # add –∑–Ω–∞—á–µ–Ω–∏–π on —Å—Ç–æ–ª–±—Ü—ã
 for bar, score in zip(bars, scores):
 height = bar.get_height()
 axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + 0.01,
 f'{score:.3f}', ha='center', va='bottom')

 axes[0, 0].grid(True, alpha=0.3)

 # CV —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
 cv_scores = results['cv_scores']
 axes[0, 1].plot(range(1, len(cv_scores)+1), cv_scores, 'o-', linewidth=2, markersize=8)
 axes[0, 1].axhline(y=results['cv_mean'], color='red', linestyle='--',
 label=f'Mean: {results["cv_mean"]:.3f}')
 axes[0, 1].set_title('Cross-Validation Scores')
 axes[0, 1].set_xlabel('Fold')
 axes[0, 1].set_ylabel('Accuracy')
 axes[0, 1].legend()
 axes[0, 1].grid(True, alpha=0.3)

 # –¢–æ—Ä–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)
 if results['trading_metrics']:
 trading_metrics = results['trading_metrics']

 # –ö—É–º—É–ª—è—Ç–∏–≤–Ω–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å
 cumulative_returns = trading_metrics['cumulative_returns']
 axes[1, 0].plot(cumulative_returns, linewidth=2)
 axes[1, 0].axhline(y=1, color='black', linestyle='--', alpha=0.5)
 axes[1, 0].set_title('Cumulative Returns')
 axes[1, 0].set_xlabel('Time')
 axes[1, 0].set_ylabel('Cumulative Return')
 axes[1, 0].grid(True, alpha=0.3)

 # –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–æ—Ä–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
 metric_names = ['Sharpe', 'Calmar', 'Win Rate', 'Profit Factor']
 metric_values = [
 trading_metrics['sharpe_ratio'],
 trading_metrics['calmar_ratio'],
 trading_metrics['win_rate'],
 trading_metrics['profit_factor']
 ]

 # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è for –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
 metric_values_limited = [min(val, 10) if val != np.inf else 10 for val in metric_values]

 bars = axes[1, 1].bar(metric_names, metric_values_limited,
 color=['skyblue', 'lightgreen', 'lightcoral', 'gold'], alpha=0.8)
 axes[1, 1].set_title('Trading Metrics')
 axes[1, 1].set_ylabel('Value')
 axes[1, 1].tick_params(axis='x', rotation=45)

 # add –∑–Ω–∞—á–µ–Ω–∏–π on —Å—Ç–æ–ª–±—Ü—ã
 for bar, value in zip(bars, metric_values):
 height = bar.get_height()
 if value == np.inf:
 label = '‚àû'
 else:
 label = f'{value:.3f}'
 axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.01,
 label, ha='center', va='bottom')

 axes[1, 1].grid(True, alpha=0.3)
 else:
 axes[1, 0].text(0.5, 0.5, 'Trading metrics\nnot available',
 ha='center', va='center', transform=axes[1, 0].transAxes)
 axes[1, 0].set_title('Trading Metrics')

 axes[1, 1].text(0.5, 0.5, 'Trading metrics\nnot available',
 ha='center', va='center', transform=axes[1, 1].transAxes)
 axes[1, 1].set_title('Trading Metrics')

 plt.tight_layout()
 plt.show()

# example –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
def example_complete_training_usage():
 """example –ø–æ–ª–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Ç–æ—Ä–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏"""

 # create —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
 np.random.seed(42)
 n_samples, n_features = 2000, 20

 # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
 X = np.random.randn(n_samples, n_features)

 # create —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
 y = np.zeros(n_samples)
 for i in range(n_samples):
 if X[i, 0] > 0.5 and X[i, 1] < -0.3:
 y[i] = 1
 elif X[i, 2] > 1.0 or X[i, 3] < -1.0:
 y[i] = 2
 else:
 y[i] = 0

 # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏
 returns = np.random.normal(0.001, 0.02, n_samples)

 print("=== example –ø–æ–ª–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Ç–æ—Ä–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏ ===")

 # –ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
 results = train_complete_trading_model(X, y, returns)

 # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
 plot_complete_results(results)

 return results

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–∞ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ for —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
# results = example_complete_training_usage()
```

## –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏

**–¢–µ–æ—Ä–∏—è:** –ü–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞—Å—Ç—É–ø–∞–µ—Ç —ç—Ç–∞–ø –≤–∞–ª–∏–¥–∞—Ü–∏–∏ and —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã for –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —Ç–æ—Ä–≥–æ–≤–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏.

**–ü–æ—á–µ–º—É –≤–∞–∂–µ–Ω –∫–∞–∂–¥—ã–π —ç—Ç–∞–ø:**

1. **–ë—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥** - check –º–æ–¥–µ–ª–∏ on –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
 - **–¶–µ–ª—å:** –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –º–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç on –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–µ –æ–Ω–∞ not –≤–∏–¥–µ–ª–∞
 - **–ú–µ—Ç–æ–¥—ã:** Walk-forward analysis, Monte Carlo simulation
 - **–ö—Ä–∏—Ç–µ—Ä–∏–∏:** –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è

2. **–í–∞–ª–∏–¥–∞—Ü–∏—è on out-of-sample –¥–∞–Ω–Ω—ã—Ö** - –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ on –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
 - **–¶–µ–ª—å:** –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
 - **–ü–µ—Ä–∏–æ–¥:** –û–±—ã—á–Ω–æ 20-30% from –æ–±—â–µ–≥–æ –æ–±—ä–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö
 - **–ö—Ä–∏—Ç–µ—Ä–∏–∏:** –°—Ä–∞–≤–Ω–µ–Ω–∏–µ with –±–µ–Ω—á–º–∞—Ä–∫–æ–º, —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å

3. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤** - –¢–æ–Ω–∫–∞—è configuration –º–æ–¥–µ–ª–∏
 - **–¶–µ–ª—å:** –ú–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏ –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∏—Å–∫–∞
 - **–ú–µ—Ç–æ–¥—ã:** Grid search, Bayesian optimization, Genetic algorithms
 - **–ö—Ä–∏—Ç–µ—Ä–∏–∏:** –£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

4. **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏** - –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ in —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
 - **–¶–µ–ª—å:** –°–≤–æ–µ–≤—Ä–µ–º–µ–Ω–Ω–æ –≤—ã—è–≤–ª—è—Ç—å –¥–µ–≥—Ä–∞–¥–∞—Ü–∏—é –º–æ–¥–µ–ª–∏
 - **–ú–µ—Ç—Ä–∏–∫–∏:** Accuracy, Sharpe ratio, Drawdown, Win rate
 - **–î–µ–π—Å—Ç–≤–∏—è:** –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ, –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ —Ç–æ—Ä–≥–æ–≤–ª–∏, –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**

- **–ù–∞—á–Ω–∏—Ç–µ with –±—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥–∞** - —ç—Ç–æ –æ—Å–Ω–æ–≤–∞ for –≤—Å–µ—Ö –¥–∞–ª—å–Ω–µ–π—à–∏—Ö —Ä–µ—à–µ–Ω–∏–π
- **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ walk-forward –∞–Ω–∞–ª–∏–∑** - –æ–Ω –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–µ–Ω for —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- **–¢–µ—Å—Ç–∏—Ä—É–π—Ç–µ on —Ä–∞–∑–Ω—ã—Ö —Ä—ã–Ω–æ—á–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö** - –±—ã—á–∏–π/–º–µ–¥–≤–µ–∂–∏–π —Ä—ã–Ω–æ–∫, –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
- **–ü—Ä–æ–≤–µ—Ä—è–π—Ç–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤** - –∏–∑–±–µ–≥–∞–π—Ç–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
- **–î–æ–∫—É–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –≤—Å–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã** - —ç—Ç–æ –ø–æ–º–æ–∂–µ—Ç in –±—É–¥—É—â–∏—Ö –∏—Ç–µ—Ä–∞—Ü–∏—è—Ö

**–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–ª–µ–¥—É—é—â–∏—Ö —ç—Ç–∞–ø–æ–≤:**

```
–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ ‚Üí –ë—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥ ‚Üí –í–∞–ª–∏–¥–∞—Ü–∏—è ‚Üí –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è ‚Üí –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
 ‚Üì ‚Üì ‚Üì ‚Üì ‚Üì
 –¢–æ—á–Ω–æ—Å—Ç—å –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∞—è Out-of- parameters –†–µ–∞–ª—å–Ω–æ–µ
 on train –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å sample –º–æ–¥–µ–ª–∏ –≤—Ä–µ–º—è
```

–ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç–µ –∫:
- **[06_backtesting.md](06_backtesting.md)** - –ë—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥ —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π
- **[07_validation.md](07_validation.md)** - –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
- **[08_optimization.md](08_optimization.md)** - –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- **[09_monitoring.md](09_monitoring.md)** - –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- **[07_walk_forward_analysis.md](07_walk_forward_analysis.md)** - Walk-forward –∞–Ω–∞–ª–∏–∑

## –ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã

**–¢–µ–æ—Ä–∏—è:** –û–±—É—á–µ–Ω–∏–µ ML-–º–æ–¥–µ–ª–µ–π for —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–º–µ–µ—Ç —Å–≤–æ–∏ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ and —Ç—Ä–µ–±—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞. –ü–æ–Ω–∏–º–∞–Ω–∏–µ —ç—Ç–∏—Ö –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ for —Å–æ–∑–¥–∞–Ω–∏—è —É—Å–ø–µ—à–Ω—ã—Ö —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π.

**–û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã —É—Å–ø–µ—à–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è:**

### 1. **–ê–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –æ–¥–∏–Ω–æ—á–Ω—ã–µ –º–æ–¥–µ–ª–∏**
- **–ü–æ—á–µ–º—É:** –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–ª–æ–∂–Ω—ã and –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã
- **–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:** –°–Ω–∏–∂–µ–Ω–∏–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è, –ø–æ–≤—ã—à–µ–Ω–∏–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
- **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:** –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Voting, Stacking, Bagging
- **–ü—Ä–∞–∫—Ç–∏–∫–∞:** –ö–æ–º–±–∏–Ω–∏—Ä—É–π—Ç–µ 3-5 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤

### 2. **–í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã —Ç—Ä–µ–±—É—é—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏**
- **–ü—Ä–æ–±–ª–µ–º–∞:** –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è CV –Ω–∞—Ä—É—à–∞–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
- **–†–µ—à–µ–Ω–∏–µ:** Time Series CV, Walk-Forward Validation
- **–ö—Ä–∏—Ç–µ—Ä–∏–∏:** –í—Ä–µ–º–µ–Ω–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ data leakage
- **–ü—Ä–∞–∫—Ç–∏–∫–∞:** –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏

### 3. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–∞**
- **–¶–µ–ª—å:** –ù–∞–π—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å bias-variance
- **–ú–µ—Ç–æ–¥—ã:** Grid Search, Random Search, Bayesian optimization
- **–ö—Ä–∏—Ç–µ—Ä–∏–∏:** –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å, –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, —Å–∫–æ—Ä–æ—Å—Ç—å
- **–ü—Ä–∞–∫—Ç–∏–∫–∞:** –ù–∞—á–Ω–∏—Ç–µ with –ø—Ä–æ—Å—Ç—ã—Ö –º–µ—Ç–æ–¥–æ–≤, –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç–µ –∫ —Å–ª–æ–∂–Ω—ã–º

### 4. **–¢–æ—Ä–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤–∞–∂–Ω–µ–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã—Ö**
- **–ü—Ä–∏—á–∏–Ω–∞:** Accuracy not –æ—Ç—Ä–∞–∂–∞–µ—Ç —Ä–µ–∞–ª—å–Ω—É—é –ø—Ä–∏–±—ã–ª—å–Ω–æ—Å—Ç—å
- **–ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏:** Sharpe Ratio, Max Drawdown, Win Rate
- **–ê–Ω–∞–ª–∏–∑:** –†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–π—Ç–µ –º–µ—Ç—Ä–∏–∫–∏ in –∫–æ–º–ø–ª–µ–∫—Å–µ
- **–ü—Ä–∞–∫—Ç–∏–∫–∞:** –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ on —Ç–æ—Ä–≥–æ–≤—ã–º –º–µ—Ç—Ä–∏–∫–∞–º, –∞ not on accuracy

### 5. **–ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —É—Å–ø–µ—Ö**
- **–í–ª–∏—è–Ω–∏–µ:** –ü–ª–æ—Ö–∏–µ –¥–∞–Ω–Ω—ã–µ = –ø–ª–æ—Ö–∞—è –º–æ–¥–µ–ª—å
- **–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:** clean, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è, feature engineering
- **check:** –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π, –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π, –≤—ã–±—Ä–æ—Å–æ–≤
- **–ü—Ä–∞–∫—Ç–∏–∫–∞:** –ò–Ω–≤–µ—Å—Ç–∏—Ä—É–π—Ç–µ –≤—Ä–µ–º—è in –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –¥–∞–Ω–Ω—ã—Ö

### 6. **–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ**
- **–ü—Ä–æ–±–ª–µ–º–∞:** –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–∫–ª–æ–Ω–Ω—ã –∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—é
- **–ú–µ—Ç–æ–¥—ã:** L1/L2 regularization, dropout, early stopping
- **–ë–∞–ª–∞–Ω—Å:** –°–ª–æ–∂–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ vs. –æ–±–æ–±—â–∞—é—â–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å
- **–ü—Ä–∞–∫—Ç–∏–∫–∞:** –ù–∞—á–∏–Ω–∞–π—Ç–µ with –ø—Ä–æ—Å—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π, —É—Å–ª–æ–∂–Ω—è–π—Ç–µ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ

### 7. **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ and –∞–¥–∞–ø—Ç–∞—Ü–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã**
- **–†–µ–∞–ª—å–Ω–æ—Å—Ç—å:** –†—ã–Ω–∫–∏ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –º–µ–Ω—è—é—Ç—Å—è
- **–î–µ–π—Å—Ç–≤–∏—è:** –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –º–µ—Ç—Ä–∏–∫
- **–ö—Ä–∏—Ç–µ—Ä–∏–∏:** –î–µ–≥—Ä–∞–¥–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ä—ã–Ω–∫–∞
- **–ü—Ä–∞–∫—Ç–∏–∫–∞:** –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä—É–π—Ç–µ –ø—Ä–æ—Ü–µ—Å—Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**

1. **–ù–∞—á–Ω–∏—Ç–µ with –ø—Ä–æ—Å—Ç–æ–≥–æ** - Random Forest, –∑–∞—Ç–µ–º –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç–µ –∫ —Å–ª–æ–∂–Ω–æ–º—É
2. **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –≤–∞–ª–∏–¥–∞—Ü–∏—é** - Time Series CV for –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
3. **–û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ on —Ç–æ—Ä–≥–æ–≤—ã–º –º–µ—Ç—Ä–∏–∫–∞–º** - not on accuracy
4. **–¢–µ—Å—Ç–∏—Ä—É–π—Ç–µ on —Ä–∞–∑–Ω—ã—Ö –ø–µ—Ä–∏–æ–¥–∞—Ö** - –±—ã—á–∏–π/–º–µ–¥–≤–µ–∂–∏–π —Ä—ã–Ω–æ–∫
5. **–î–æ–∫—É–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –≤—Å–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã** - —ç—Ç–æ –ø–æ–º–æ–∂–µ—Ç in –±—É–¥—É—â–µ–º
6. **–ü–ª–∞–Ω–∏—Ä—É–π—Ç–µ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥** - –º–æ–¥–µ–ª—å –Ω—É–∂–Ω–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å

**–¢–∏–ø–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏:**

- ‚ùå –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π CV for –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
- ‚ùå –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ on accuracy
- ‚ùå –ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ—Ä–≥–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫
- ‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- ‚ùå –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ on –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö

**–£—Å–ø–µ—à–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è:**

- ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è (Time Series CV)
- ‚úÖ –ê–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã
- ‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è on —Ç–æ—Ä–≥–æ–≤—ã–º –º–µ—Ç—Ä–∏–∫–∞–º
- ‚úÖ –†–µ–≥—É–ª—è—Ä–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
- ‚úÖ –ê–¥–∞–ø—Ç–∞—Ü–∏—è –∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º —Ä—ã–Ω–∫–∞

**–ó–∞–∫–ª—é—á–µ–Ω–∏–µ:**

–û–±—É—á–µ–Ω–∏–µ ML-–º–æ–¥–µ–ª–µ–π for —Ñ–∏–Ω–∞–Ω—Å–æ–≤ - —ç—Ç–æ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å, —Ç—Ä–µ–±—É—é—â–∏–π –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫–∞–∫ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, —Ç–∞–∫ and —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ä—ã–Ω–∫–æ–≤. –£—Å–ø–µ—Ö –ø—Ä–∏—Ö–æ–¥–∏—Ç –∫ —Ç–µ–º, –∫—Ç–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –≤–∞–ª–∏–¥–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏ and –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º —Ä—ã–Ω–∫–∞.

---

**–í–∞–∂–Ω–æ:** not –≥–æ–Ω–∏—Ç–µ—Å—å –∑–∞ –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é - –≤–∞–∂–Ω–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–∞—è –ø—Ä–∏–±—ã–ª—å–Ω–æ—Å—Ç—å!
