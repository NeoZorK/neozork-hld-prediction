# 05. ü§ñ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π

**–¶–µ–ª—å:** –ù–∞—É—á–∏—Ç—å—Å—è –æ–±—É—á–∞—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ ML-–º–æ–¥–µ–ª–∏ –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

## –í—ã–±–æ—Ä –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏

**–¢–µ–æ—Ä–∏—è:** –í—ã–±–æ—Ä –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–µ–Ω –¥–ª—è —É—Å–ø–µ—Ö–∞ ML-—Å–∏—Å—Ç–µ–º. –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–º–µ—é—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —Ç—Ä–µ–±—É—é—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –∫ –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π.

### –ü–æ—á–µ–º—É –Ω–µ –≤—Å–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –ø–æ–¥—Ö–æ–¥—è—Ç?

**–¢–µ–æ—Ä–∏—è:** –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–º–µ—é—Ç —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –¥–µ–ª–∞—é—Ç –º–Ω–æ–≥–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ ML-–∞–ª–≥–æ—Ä–∏—Ç–º—ã –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º–∏ –∏–ª–∏ –¥–∞–∂–µ –æ–ø–∞—Å–Ω—ã–º–∏. –ü–æ–Ω–∏–º–∞–Ω–∏–µ —ç—Ç–∏—Ö –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –≤—ã–±–æ—Ä–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤.

**–§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–º–µ—é—Ç –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:**

**1. –ù–µ—Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ—Å—Ç—å**
- **–¢–µ–æ—Ä–∏—è:** –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –º–µ–Ω—è—é—Ç—Å—è –≤–æ –≤—Ä–µ–º–µ–Ω–∏ –∏–∑-–∑–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ä—ã–Ω–æ—á–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π
- **–ü–æ—á–µ–º—É –ø—Ä–æ–±–ª–µ–º–∞—Ç–∏—á–Ω–æ:** –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞—é—Ç —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ—Å—Ç—å
- **–ü–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è:** –ú–æ–¥–µ–ª–∏ –±—ã—Å—Ç—Ä–æ —É—Å—Ç–∞—Ä–µ–≤–∞—é—Ç, —Å–Ω–∏–∂–∞–µ—Ç—Å—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
- **–ü–ª—é—Å—ã:** –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º
- **–ú–∏–Ω—É—Å—ã:** –°–ª–æ–∂–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —Ä–µ–≥—É–ª—è—Ä–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è

**2. –í—ã—Å–æ–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å**
- **–¢–µ–æ—Ä–∏—è:** –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –º–Ω–æ–≥–æ —à—É–º–∞ –∏ —Å–ª—É—á–∞–π–Ω—ã—Ö –∫–æ–ª–µ–±–∞–Ω–∏–π
- **–ü–æ—á–µ–º—É –ø—Ä–æ–±–ª–µ–º–∞—Ç–∏—á–Ω–æ:** –®—É–º –º–æ–∂–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ —Å–ª—É—á–∞–π–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö
- **–ü–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è:** –õ–æ–∂–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã, –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ, –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
- **–ü–ª—é—Å—ã:** –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≤—ã—è–≤–ª–µ–Ω–∏—è —Ä–µ–∞–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
- **–ú–∏–Ω—É—Å—ã:** –°–ª–æ–∂–Ω–æ—Å—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —à—É–º–∞, —Ä–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è

**3. –ù–µ—Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç—å**
- **–¢–µ–æ—Ä–∏—è:** –†–µ–¥–∫–∏–µ, –Ω–æ –≤–∞–∂–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è (–∫—Ä–∏–∑–∏—Å—ã, –∫—Ä–∞—Ö–∏) –∏–º–µ—é—Ç –Ω–µ–ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –±–æ–ª—å—à–æ–µ –≤–ª–∏—è–Ω–∏–µ
- **–ü–æ—á–µ–º—É –ø—Ä–æ–±–ª–µ–º–∞—Ç–∏—á–Ω–æ:** –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –º–æ–≥—É—Ç –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–¥–∫–∏–µ —Å–æ–±—ã—Ç–∏—è
- **–ü–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è:** –ú–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –Ω–µ —É—á–∏—Ç—ã–≤–∞—Ç—å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–±—ã—Ç–∏—è
- **–ü–ª—é—Å—ã:** –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≤—ã—è–≤–ª–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π
- **–ú–∏–Ω—É—Å—ã:** –°–ª–æ–∂–Ω–æ—Å—Ç—å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤, —Ä–∏—Å–∫ –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∞–∂–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π

**4. –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏**
- **–¢–µ–æ—Ä–∏—è:** –ü—Ä–∏–∑–Ω–∞–∫–∏ —á–∞—Å—Ç–æ —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω—ã, —á—Ç–æ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏
- **–ü–æ—á–µ–º—É –ø—Ä–æ–±–ª–µ–º–∞—Ç–∏—á–Ω–æ:** –ö–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –º–æ–≥—É—Ç –∏—Å–∫–∞–∂–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
- **–ü–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è:** –ù–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏, —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏
- **–ü–ª—é—Å—ã:** –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≤—ã—è–≤–ª–µ–Ω–∏—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
- **–ú–∏–Ω—É—Å—ã:** –°–ª–æ–∂–Ω–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏, —Ä–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è

### –õ—É—á—à–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤

**–¢–µ–æ—Ä–∏—è:** –í—ã–±–æ—Ä –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–æ–ª–∂–µ–Ω –æ—Å–Ω–æ–≤—ã–≤–∞—Ç—å—Å—è –Ω–∞ –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç–∞—Ç—å —Å –Ω–µ—Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω—ã–º–∏, –∑–∞—à—É–º–ª–µ–Ω–Ω—ã–º–∏ –∏ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏. –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –ø–æ–∫–∞–∑–∞–ª–∏ –æ—Å–æ–±—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Å—Ñ–µ—Ä–µ.

**1. –ê–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã**
- **–ü–æ—á–µ–º—É —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã:** –ö–æ–º–±–∏–Ω–∏—Ä—É—é—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π, —Å–Ω–∏–∂–∞—è —Ä–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
- **–ü–ª—é—Å—ã:** –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å, —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –≤—ã–±—Ä–æ—Å–∞–º, –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å
- **–ú–∏–Ω—É—Å—ã:** –í—ã—Å–æ–∫–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã, —Å–ª–æ–∂–Ω–æ—Å—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
- **–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ:** Random Forest, XGBoost, LightGBM –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏

**2. –ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏**
- **–ü–æ—á–µ–º—É —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã:** –ú–æ–≥—É—Ç –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
- **–ü–ª—é—Å—ã:** –í—ã—Å–æ–∫–∞—è –≥–∏–±–∫–æ—Å—Ç—å, —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –æ–±—É—á–µ–Ω–∏—é —Å–ª–æ–∂–Ω—ã–º –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
- **–ú–∏–Ω—É—Å—ã:** –¢—Ä–µ–±—É—é—Ç –º–Ω–æ–≥–æ –¥–∞–Ω–Ω—ã—Ö, —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏, —Ä–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
- **–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ:** LSTM, GRU –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤, Transformer –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π

**3. SVM (Support Vector Machine)**
- **–ü–æ—á–µ–º—É —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã:** –•–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞—é—Ç —Å –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–º–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏
- **–ü–ª—é—Å—ã:** –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã –Ω–∞ –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —É—Å—Ç–æ–π—á–∏–≤—ã –∫ –≤—ã–±—Ä–æ—Å–∞–º
- **–ú–∏–Ω—É—Å—ã:** –ú–µ–¥–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö, —Å–ª–æ–∂–Ω–æ—Å—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
- **–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ:** –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω

**4. Logistic Regression**
- **–ü–æ—á–µ–º—É —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã:** –ü—Ä–æ—Å—Ç—ã–µ, –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–µ, –±—ã—Å—Ç—Ä—ã–µ
- **–ü–ª—é—Å—ã:** –õ–µ–≥–∫–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è, –±—ã—Å—Ç—Ä–∞—è —Ä–∞–±–æ—Ç–∞, —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
- **–ú–∏–Ω—É—Å—ã:** –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
- **–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ:** –ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–µ —Å–∏—Å—Ç–µ–º—ã

**–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–æ–æ–±—Ä–∞–∂–µ–Ω–∏—è:**
- **–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è:** –í–∞–∂–Ω–∞ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
- **–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è:** –ö—Ä–∏—Ç–∏—á–Ω–∞ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
- **–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è:** –ú–æ–∂–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
- **–ê–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ:** –ö–æ–º–±–∏–Ω–∞—Ü–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ —á–∞—Å—Ç–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏

## –ê–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã

**–¢–µ–æ—Ä–∏—è:** –ê–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã –∫–æ–º–±–∏–Ω–∏—Ä—É—é—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –û–Ω–∏ –æ—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —Ç–∞–∫ –∫–∞–∫ —Å–Ω–∏–∂–∞—é—Ç —Ä–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –∏ –ø–æ–≤—ã—à–∞—é—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π.

**–ü–æ—á–µ–º—É –∞–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤:**
- **–°–Ω–∏–∂–µ–Ω–∏–µ —Ä–∏—Å–∫–∞:** –ö–æ–º–±–∏–Ω–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π —Å–Ω–∏–∂–∞–µ—Ç —Ä–∏—Å–∫ –æ—à–∏–±–æ–∫
- **–£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –≤—ã–±—Ä–æ—Å–∞–º:** –†–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ-—Ä–∞–∑–Ω–æ–º—É —Ä–µ–∞–≥–∏—Ä—É—é—Ç –Ω–∞ –≤—ã–±—Ä–æ—Å—ã
- **–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å:** –ê–Ω—Å–∞–º–±–ª–∏ –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã, —á–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
- **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å:** –ú–æ–∂–Ω–æ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

### 1. Random Forest

**–¢–µ–æ—Ä–∏—è:** Random Forest - —ç—Ç–æ –∞–Ω—Å–∞–º–±–ª—å —Ä–µ—à–∞—é—â–∏—Ö –¥–µ—Ä–µ–≤—å–µ–≤, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±—É—Ç—Å—Ç—Ä–∞–ø –∞–≥—Ä–µ–≥–∞—Ü–∏—é (bagging) –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π. –ö–∞–∂–¥–æ–µ –¥–µ—Ä–µ–≤–æ –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ —Å–ª—É—á–∞–π–Ω–æ–π –ø–æ–¥–≤—ã–±–æ—Ä–∫–µ –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.

**–î–µ—Ç–∞–ª—å–Ω–∞—è —Ç–µ–æ—Ä–∏—è Random Forest:**

**–ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã:**
1. **Bootstrap Sampling:** –ö–∞–∂–¥–æ–µ –¥–µ—Ä–µ–≤–æ –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ —Å–ª—É—á–∞–π–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ —Å –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ–º (–æ–±—ã—á–Ω–æ 63% –¥–∞–Ω–Ω—ã—Ö)
2. **Feature Randomness:** –ù–∞ –∫–∞–∂–¥–æ–º —É–∑–ª–µ –¥–µ—Ä–µ–≤–∞ –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è —Å–ª—É—á–∞–π–Ω–æ–µ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
3. **Voting/Averaging:** –§–∏–Ω–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ - —ç—Ç–æ —Å—Ä–µ–¥–Ω–µ–µ (—Ä–µ–≥—Ä–µ—Å—Å–∏—è) –∏–ª–∏ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è) –≤—Å–µ—Ö –¥–µ—Ä–µ–≤—å–µ–≤

**–ü–æ—á–µ–º—É Random Forest —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤:**
- **–£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—é:** –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤ —Å–Ω–∏–∂–∞—é—Ç —Ä–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –Ω–∞ —à—É–º–µ
- **–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤:** –î–µ—Ä–µ–≤—å—è –ø–æ-—Ä–∞–∑–Ω–æ–º—É —Ä–µ–∞–≥–∏—Ä—É—é—Ç –Ω–∞ –≤—ã–±—Ä–æ—Å—ã, —Å–Ω–∏–∂–∞—è –∏—Ö –≤–ª–∏—è–Ω–∏–µ
- **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å:** –ú–æ–∂–Ω–æ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —á–µ—Ä–µ–∑ feature importance
- **–ë—ã—Å—Ç—Ä–æ—Ç–∞:** –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–µ—Ä–µ–≤—å–µ–≤ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –±–æ–ª—å—à–∏–µ –æ–±—ä–µ–º—ã –¥–∞–Ω–Ω—ã—Ö
- **–£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏:** –°–ª—É—á–∞–π–Ω—ã–π –≤—ã–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–Ω–∏–∂–∞–µ—Ç –≤–ª–∏—è–Ω–∏–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π

**–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Å–Ω–æ–≤–∞:**
- **Bootstrap:** –î–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞ t, –æ–±—É—á–∞–µ–º –Ω–∞ –≤—ã–±–æ—Ä–∫–µ D_t, –ø–æ–ª—É—á–µ–Ω–Ω–æ–π –∏–∑ D —Å –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ–º
- **Feature Selection:** –ù–∞ –∫–∞–∂–¥–æ–º —É–∑–ª–µ –≤—ã–±–∏—Ä–∞–µ–º ‚àöp –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ p –¥–æ—Å—Ç—É–ø–Ω—ã—Ö
- **Prediction:** ≈∑ = (1/T) * Œ£(t=1 to T) f_t(x), –≥–¥–µ T - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤

**–ü–ª—é—Å—ã Random Forest:**
- –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–µ –∑–∞–¥–∞—á
- –£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—é
- –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å —á–µ—Ä–µ–∑ feature importance
- –ë—ã—Å—Ç—Ä–æ—Ç–∞ –æ–±—É—á–µ–Ω–∏—è –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
- –†–∞–±–æ—Ç–∞–µ—Ç —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
- –ù–µ —Ç—Ä–µ–±—É–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

**–ú–∏–Ω—É—Å—ã Random Forest:**
- –ú–æ–≥—É—Ç –±—ã—Ç—å –º–µ–Ω–µ–µ —Ç–æ—á–Ω—ã–º–∏ –Ω–∞ –æ—á–µ–Ω—å —Å–ª–æ–∂–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- –¢—Ä–µ–±—É—é—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (n_estimators, max_depth, etc.)
- –ú–æ–≥—É—Ç –±—ã—Ç—å –∏–∑–±—ã—Ç–æ—á–Ω—ã–º–∏ –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –∑–∞–¥–∞—á
- –ü–ª–æ—Ö–æ —Ä–∞–±–æ—Ç–∞—é—Ç —Å –æ—á–µ–Ω—å —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
- –ú–æ–≥—É—Ç –ø–µ—Ä–µ–æ–±—É—á–∏—Ç—å—Å—è –Ω–∞ –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö
**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Random Forest:**

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç –∫–æ–¥:**
1. **–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:** –°–æ–∑–¥–∞–µ—Ç –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø—Ä–æ–ø–æ—Ä—Ü–∏–π –∫–ª–∞—Å—Å–æ–≤
2. **–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏:** –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã Random Forest –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
3. **–û–±—É—á–µ–Ω–∏–µ:** –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
4. **–û—Ü–µ–Ω–∫–∞:** –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –∏ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∞—Ö

**–û–±—ä—è—Å–Ω–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:**
- `n_estimators=100`: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤ –≤ –ª–µ—Å—É (–±–æ–ª—å—à–µ = –ª—É—á—à–µ, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ)
- `max_depth=10`: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤–∞ (–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ)
- `min_samples_split=5`: –ú–∏–Ω–∏–º—É–º –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —É–∑–ª–∞
- `min_samples_leaf=2`: –ú–∏–Ω–∏–º—É–º –æ–±—Ä–∞–∑—Ü–æ–≤ –≤ –ª–∏—Å—Ç–µ
- `n_jobs=-1`: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —è–¥—Ä–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞

```python
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

def train_random_forest(X, y, test_size=0.2, random_state=42):
    """
    –û–±—É—á–µ–Ω–∏–µ Random Forest –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    
    Args:
        X (array-like): –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (samples, features)
        y (array-like): –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (samples,)
        test_size (float): –î–æ–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö (0.0-1.0)
        random_state (int): Seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    
    Returns:
        tuple: (–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å, –º–µ—Ç—Ä–∏–∫–∏, –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
    """
    
    print("=== –û–±—É—á–µ–Ω–∏–µ Random Forest ===")
    print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {X.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤, {X.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    print(f"–ö–ª–∞—Å—Å—ã: {np.unique(y, return_counts=True)}")
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø—Ä–æ–ø–æ—Ä—Ü–∏–π –∫–ª–∞—Å—Å–æ–≤
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state, stratify=y
    )
    
    print(f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤
    rf = RandomForestClassifier(
        n_estimators=100,        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤
        max_depth=10,            # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ (–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ)
        min_samples_split=5,     # –ú–∏–Ω–∏–º—É–º –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —É–∑–ª–∞
        min_samples_leaf=2,      # –ú–∏–Ω–∏–º—É–º –≤ –ª–∏—Å—Ç–µ
        max_features='sqrt',     # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
        bootstrap=True,          # Bootstrap sampling
        oob_score=True,          # Out-of-bag –æ—Ü–µ–Ω–∫–∞
        random_state=random_state,
        n_jobs=-1,               # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
        verbose=0
    )
    
    print("\n–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏:")
    print(f"n_estimators: {rf.n_estimators}")
    print(f"max_depth: {rf.max_depth}")
    print(f"max_features: {rf.max_features}")
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("\n–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    rf.fit(X_train, y_train)
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    y_train_pred = rf.predict(X_train)
    y_test_pred = rf.predict(X_test)
    
    # –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    train_score = rf.score(X_train, y_train)
    test_score = rf.score(X_test, y_test)
    oob_score = rf.oob_score_
    
    print(f"\n=== –†–µ–∑—É–ª—å—Ç–∞—Ç—ã ===")
    print(f"Train accuracy: {train_score:.4f}")
    print(f"Test accuracy: {test_score:.4f}")
    print(f"OOB score: {oob_score:.4f}")
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    print(f"\n=== Classification Report (Test) ===")
    print(classification_report(y_test, y_test_pred))
    
    # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_importance = rf.feature_importances_
    feature_names = [f'feature_{i}' for i in range(X.shape[1])]
    
    # –°–æ–∑–¥–∞–Ω–∏–µ DataFrame —Å –≤–∞–∂–Ω–æ—Å—Ç—å—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': feature_importance
    }).sort_values('importance', ascending=False)
    
    print(f"\n=== –¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ===")
    print(importance_df.head(10))
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞
    metrics = {
        'train_accuracy': train_score,
        'test_accuracy': test_score,
        'oob_score': oob_score,
        'feature_importance': importance_df,
        'confusion_matrix': confusion_matrix(y_test, y_test_pred)
    }
    
    return rf, metrics, importance_df

def plot_feature_importance(importance_df, top_n=15):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    
    plt.figure(figsize=(10, 8))
    top_features = importance_df.head(top_n)
    
    sns.barplot(data=top_features, x='importance', y='feature')
    plt.title(f'–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (Top {top_n})')
    plt.xlabel('–í–∞–∂–Ω–æ—Å—Ç—å')
    plt.ylabel('–ü—Ä–∏–∑–Ω–∞–∫–∏')
    plt.tight_layout()
    plt.show()

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
def example_random_forest_usage():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Random Forest"""
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
    np.random.seed(42)
    n_samples, n_features = 1000, 20
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    X = np.random.randn(n_samples, n_features)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π —Å –Ω–µ–∫–æ—Ç–æ—Ä–æ–π –ª–æ–≥–∏–∫–æ–π
    y = np.zeros(n_samples)
    for i in range(n_samples):
        if X[i, 0] > 0.5 and X[i, 1] < -0.3:
            y[i] = 1  # –ö–ª–∞—Å—Å 1
        elif X[i, 2] > 1.0 or X[i, 3] < -1.0:
            y[i] = 2  # –ö–ª–∞—Å—Å 2
        else:
            y[i] = 0  # –ö–ª–∞—Å—Å 0
    
    print("=== –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Random Forest ===")
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model, metrics, importance_df = train_random_forest(X, y)
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    plot_feature_importance(importance_df)
    
    return model, metrics

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–∞ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
# model, metrics = example_random_forest_usage()
```

### 2. XGBoost

**–¢–µ–æ—Ä–∏—è:** XGBoost (eXtreme Gradient Boosting) - —ç—Ç–æ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –±—É—Å—Ç–∏–Ω–≥–∞, –∫–æ—Ç–æ—Ä–∞—è –æ—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞ –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –±–ª–∞–≥–æ–¥–∞—Ä—è —Å–≤–æ–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏ –≤—ã–±—Ä–æ—Å—ã.

**–î–µ—Ç–∞–ª—å–Ω–∞—è —Ç–µ–æ—Ä–∏—è XGBoost:**

**–ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã:**
1. **Gradient Boosting:** –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –¥–æ–±–∞–≤–ª—è–µ—Ç –¥–µ—Ä–µ–≤—å—è, –∫–∞–∂–¥–æ–µ –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç –æ—à–∏–±–∫–∏ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö
2. **Regularization:** –ò—Å–ø–æ–ª—å–∑—É–µ—Ç L1 –∏ L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
3. **Parallel Processing:** –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
4. **Missing Value Handling:** –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è

**–ü–æ—á–µ–º—É XGBoost —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤:**
- **–í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å:** –ß–∞—Å—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- **–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤:** –£—Å—Ç–æ–π—á–∏–≤ –∫ –∞–Ω–æ–º–∞–ª—å–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º
- **Feature Importance:** –ü–æ–∑–≤–æ–ª—è–µ—Ç –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
- **–ë—ã—Å—Ç—Ä–æ—Ç–∞:** –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
- **–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è:** –í—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –∑–∞—â–∏—Ç–∞ –æ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è

**–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Å–Ω–æ–≤–∞:**
- **Objective Function:** L(œÜ) = Œ£ l(yi, ≈∑i) + Œ£ Œ©(fk)
- **Gradient Boosting:** F_m(x) = F_{m-1}(x) + Œ≥_m * h_m(x)
- **Regularization:** Œ©(f) = Œ≥T + (1/2)Œª||w||¬≤

**–ö–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `learning_rate`: –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (0.01-0.3)
- `max_depth`: –ì–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤—å–µ–≤ (3-10)
- `n_estimators`: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±—É—Å—Ç–µ—Ä–æ–≤ (50-1000)
- `subsample`: –î–æ–ª—è –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞ (0.6-1.0)
- `colsample_bytree`: –î–æ–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞ (0.6-1.0)

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è XGBoost:**

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç –∫–æ–¥:**
1. **–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:** –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
2. **Early Stopping:** –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –≤–∞–ª–∏–¥–∞—Ü–∏—é
3. **–û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:** –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç—Ä–∏–∫–∏, –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤
4. **Feature Importance:** –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

```python
import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

def train_xgboost(X, y, test_size=0.2, random_state=42, early_stopping_rounds=10):
    """
    –û–±—É—á–µ–Ω–∏–µ XGBoost –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    
    Args:
        X (array-like): –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (samples, features)
        y (array-like): –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (samples,)
        test_size (float): –î–æ–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö (0.0-1.0)
        random_state (int): Seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        early_stopping_rounds (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞—É–Ω–¥–æ–≤ –¥–ª—è early stopping
    
    Returns:
        tuple: (–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å, –º–µ—Ç—Ä–∏–∫–∏, –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
    """
    
    print("=== –û–±—É—á–µ–Ω–∏–µ XGBoost ===")
    print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {X.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤, {X.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    print(f"–ö–ª–∞—Å—Å—ã: {np.unique(y, return_counts=True)}")
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state, stratify=y
    )
    
    print(f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
    
    # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    params = {
        'objective': 'multi:softprob',    # –ú–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏
        'num_class': len(np.unique(y)),   # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤
        'max_depth': 6,                   # –ì–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤—å–µ–≤ (–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ)
        'learning_rate': 0.1,             # –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
        'n_estimators': 100,              # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±—É—Å—Ç–µ—Ä–æ–≤
        'subsample': 0.8,                 # –î–æ–ª—è –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞
        'colsample_bytree': 0.8,          # –î–æ–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞
        'reg_alpha': 0.1,                 # L1 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
        'reg_lambda': 1.0,                # L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
        'random_state': random_state,
        'n_jobs': -1,                     # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
        'verbosity': 0                    # –û—Ç–∫–ª—é—á–∏—Ç—å –≤—ã–≤–æ–¥
    }
    
    print("\n–ü–∞—Ä–∞–º–µ—Ç—Ä—ã XGBoost:")
    for key, value in params.items():
        print(f"{key}: {value}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    xgb_model = xgb.XGBClassifier(**params)
    
    # –û–±—É—á–µ–Ω–∏–µ —Å early stopping
    print("\n–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    xgb_model.fit(
        X_train, y_train,
        eval_set=[(X_test, y_test)],
        early_stopping_rounds=early_stopping_rounds,
        verbose=False
    )
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    y_train_pred = xgb_model.predict(X_train)
    y_test_pred = xgb_model.predict(X_test)
    y_test_proba = xgb_model.predict_proba(X_test)
    
    # –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    train_accuracy = accuracy_score(y_train, y_train_pred)
    test_accuracy = accuracy_score(y_test, y_test_pred)
    
    print(f"\n=== –†–µ–∑—É–ª—å—Ç–∞—Ç—ã ===")
    print(f"Train accuracy: {train_accuracy:.4f}")
    print(f"Test accuracy: {test_accuracy:.4f}")
    print(f"Best iteration: {xgb_model.best_iteration}")
    print(f"Best score: {xgb_model.best_score:.4f}")
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    print(f"\n=== Classification Report (Test) ===")
    print(classification_report(y_test, y_test_pred))
    
    # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_importance = xgb_model.feature_importances_
    feature_names = [f'feature_{i}' for i in range(X.shape[1])]
    
    # –°–æ–∑–¥–∞–Ω–∏–µ DataFrame —Å –≤–∞–∂–Ω–æ—Å—Ç—å—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': feature_importance
    }).sort_values('importance', ascending=False)
    
    print(f"\n=== –¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ===")
    print(importance_df.head(10))
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞
    metrics = {
        'train_accuracy': train_accuracy,
        'test_accuracy': test_accuracy,
        'best_iteration': xgb_model.best_iteration,
        'best_score': xgb_model.best_score,
        'feature_importance': importance_df,
        'confusion_matrix': confusion_matrix(y_test, y_test_pred),
        'predictions': y_test_pred,
        'probabilities': y_test_proba
    }
    
    return xgb_model, metrics, importance_df

def plot_xgboost_importance(importance_df, top_n=15):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ XGBoost"""
    
    plt.figure(figsize=(12, 8))
    top_features = importance_df.head(top_n)
    
    sns.barplot(data=top_features, x='importance', y='feature')
    plt.title(f'–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ XGBoost (Top {top_n})')
    plt.xlabel('–í–∞–∂–Ω–æ—Å—Ç—å')
    plt.ylabel('–ü—Ä–∏–∑–Ω–∞–∫–∏')
    plt.tight_layout()
    plt.show()

def plot_learning_curve(model, X_train, y_train, X_test, y_test):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫—Ä–∏–≤–æ–π –æ–±—É—á–µ–Ω–∏—è"""
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è
    results = model.evals_result()
    
    plt.figure(figsize=(12, 4))
    
    # –ì—Ä–∞—Ñ–∏–∫ –æ—à–∏–±–∫–∏
    plt.subplot(1, 2, 1)
    plt.plot(results['validation_0']['mlogloss'], label='Train')
    plt.plot(results['validation_1']['mlogloss'], label='Test')
    plt.title('Learning Curve - Log Loss')
    plt.xlabel('Iterations')
    plt.ylabel('Log Loss')
    plt.legend()
    plt.grid(True)
    
    # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏
    plt.subplot(1, 2, 2)
    train_acc = [1 - x for x in results['validation_0']['mlogloss']]
    test_acc = [1 - x for x in results['validation_1']['mlogloss']]
    plt.plot(train_acc, label='Train')
    plt.plot(test_acc, label='Test')
    plt.title('Learning Curve - Accuracy')
    plt.xlabel('Iterations')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.show()

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
def example_xgboost_usage():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è XGBoost"""
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
    np.random.seed(42)
    n_samples, n_features = 1000, 20
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –Ω–µ–∫–æ—Ç–æ—Ä–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
    X = np.random.randn(n_samples, n_features)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π —Å –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–º–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏
    y = np.zeros(n_samples)
    for i in range(n_samples):
        # –°–ª–æ–∂–Ω–∞—è –Ω–µ–ª–∏–Ω–µ–π–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å
        score = (X[i, 0] ** 2 + X[i, 1] * X[i, 2] + 
                np.sin(X[i, 3]) + X[i, 4] * X[i, 5])
        
        if score > 2.0:
            y[i] = 2  # –ö–ª–∞—Å—Å 2
        elif score > 0.5:
            y[i] = 1  # –ö–ª–∞—Å—Å 1
        else:
            y[i] = 0  # –ö–ª–∞—Å—Å 0
    
    print("=== –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è XGBoost ===")
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model, metrics, importance_df = train_xgboost(X, y)
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    plot_xgboost_importance(importance_df)
    
    return model, metrics

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–∞ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
# model, metrics = example_xgboost_usage()
```

### 3. LightGBM

**–¢–µ–æ—Ä–∏—è:** LightGBM (Light Gradient Boosting Machine) - —ç—Ç–æ –±—ã—Å—Ç—Ä–∞—è –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –±—É—Å—Ç–∏–Ω–≥–∞, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è Microsoft. –û—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –±–ª–∞–≥–æ–¥–∞—Ä—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É –∞–ª–≥–æ—Ä–∏—Ç–º—É –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –¥–µ—Ä–µ–≤—å–µ–≤.

**–î–µ—Ç–∞–ª—å–Ω–∞—è —Ç–µ–æ—Ä–∏—è LightGBM:**

**–ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã:**
1. **Leaf-wise Growth:** –°—Ç—Ä–æ–∏—Ç –¥–µ—Ä–µ–≤—å—è –ø–æ –ª–∏—Å—Ç—å—è–º, –∞ –Ω–µ –ø–æ —É—Ä–æ–≤–Ω—è–º (–∫–∞–∫ XGBoost)
2. **Gradient-based One-Side Sampling (GOSS):** –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ –æ–±—Ä–∞–∑—Ü—ã —Å –±–æ–ª—å—à–∏–º–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏
3. **Exclusive Feature Bundling (EFB):** –ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç –≤–∑–∞–∏–º–Ω–æ –∏—Å–∫–ª—é—á–∞—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
4. **Categorical Feature Support:** –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏

**–ü–æ—á–µ–º—É LightGBM —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤:**
- **–°–∫–æ—Ä–æ—Å—Ç—å:** –í 10-100 —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ XGBoost –Ω–∞ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö
- **–ü–∞–º—è—Ç—å:** –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏ –±–ª–∞–≥–æ–¥–∞—Ä—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º
- **–¢–æ—á–Ω–æ—Å—Ç—å:** –ß–∞—Å—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
- **–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:** –û—Ç–ª–∏—á–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏
- **–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è:** –í—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –∑–∞—â–∏—Ç–∞ –æ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è

**–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Å–Ω–æ–≤–∞:**
- **Leaf-wise Growth:** –í—ã–±–∏—Ä–∞–µ—Ç –ª–∏—Å—Ç —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –ø—Ä–∏—Ä–æ—Å—Ç–æ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
- **GOSS:** –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ø-a% –æ–±—Ä–∞–∑—Ü–æ–≤ —Å –±–æ–ª—å—à–∏–º–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏ + —Å–ª—É—á–∞–π–Ω—ã–µ b% –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
- **EFB:** –ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –Ω–∏–∑–∫–æ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π

**–ö–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `num_leaves`: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏—Å—Ç—å–µ–≤ (31-255)
- `learning_rate`: –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (0.01-0.3)
- `feature_fraction`: –î–æ–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (0.6-1.0)
- `bagging_fraction`: –î–æ–ª—è –æ–±—Ä–∞–∑—Ü–æ–≤ (0.6-1.0)
- `min_data_in_leaf`: –ú–∏–Ω–∏–º—É–º –¥–∞–Ω–Ω—ã—Ö –≤ –ª–∏—Å—Ç–µ (20-100)

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è LightGBM:**

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç –∫–æ–¥:**
1. **–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:** –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
2. **Early Stopping:** –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
3. **–í–∞–ª–∏–¥–∞—Ü–∏—è:** –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
4. **Feature Importance:** –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

```python
import numpy as np
import pandas as pd
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

def train_lightgbm(X, y, test_size=0.2, random_state=42, early_stopping_rounds=10):
    """
    –û–±—É—á–µ–Ω–∏–µ LightGBM –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    
    Args:
        X (array-like): –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (samples, features)
        y (array-like): –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (samples,)
        test_size (float): –î–æ–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö (0.0-1.0)
        random_state (int): Seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        early_stopping_rounds (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞—É–Ω–¥–æ–≤ –¥–ª—è early stopping
    
    Returns:
        tuple: (–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å, –º–µ—Ç—Ä–∏–∫–∏, –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
    """
    
    print("=== –û–±—É—á–µ–Ω–∏–µ LightGBM ===")
    print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {X.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤, {X.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    print(f"–ö–ª–∞—Å—Å—ã: {np.unique(y, return_counts=True)}")
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state, stratify=y
    )
    
    print(f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
    
    # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    params = {
        'objective': 'multiclass',           # –ú–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        'num_class': len(np.unique(y)),      # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤
        'boosting_type': 'gbdt',             # –¢–∏–ø –±—É—Å—Ç–∏–Ω–≥–∞ (Gradient Boosting Decision Tree)
        'num_leaves': 31,                    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏—Å—Ç—å–µ–≤ (2^max_depth - 1)
        'learning_rate': 0.05,               # –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
        'feature_fraction': 0.9,             # –î–æ–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞
        'bagging_fraction': 0.8,             # –î–æ–ª—è –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞
        'bagging_freq': 5,                   # –ß–∞—Å—Ç–æ—Ç–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è bagging
        'min_data_in_leaf': 20,              # –ú–∏–Ω–∏–º—É–º –¥–∞–Ω–Ω—ã—Ö –≤ –ª–∏—Å—Ç–µ
        'min_sum_hessian_in_leaf': 1e-3,     # –ú–∏–Ω–∏–º—É–º —Å—É–º–º—ã –≥–µ—Å—Å–∏–∞–Ω–æ–≤ –≤ –ª–∏—Å—Ç–µ
        'lambda_l1': 0.1,                    # L1 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
        'lambda_l2': 1.0,                    # L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
        'min_gain_to_split': 0.0,            # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–∏—Ä–æ—Å—Ç –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
        'max_depth': -1,                     # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ (-1 = –Ω–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ)
        'save_binary': True,                 # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –±–∏–Ω–∞—Ä–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        'seed': random_state,                # Seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        'feature_fraction_seed': random_state,
        'bagging_seed': random_state,
        'drop_seed': random_state,
        'data_random_seed': random_state,
        'verbose': -1,                       # –û—Ç–∫–ª—é—á–∏—Ç—å –≤—ã–≤–æ–¥
        'n_jobs': -1                         # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
    }
    
    print("\n–ü–∞—Ä–∞–º–µ—Ç—Ä—ã LightGBM:")
    for key, value in params.items():
        print(f"{key}: {value}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ LightGBM
    train_data = lgb.Dataset(X_train, label=y_train)
    test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("\n–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    model = lgb.train(
        params,
        train_data,
        valid_sets=[test_data],
        num_boost_round=100,
        callbacks=[
            lgb.early_stopping(early_stopping_rounds),
            lgb.log_evaluation(0)  # –û—Ç–∫–ª—é—á–∏—Ç—å –≤—ã–≤–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        ]
    )
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    y_train_pred = model.predict(X_train, num_iteration=model.best_iteration)
    y_test_pred = model.predict(X_test, num_iteration=model.best_iteration)
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –≤ –∫–ª–∞—Å—Å—ã
    y_train_pred_class = np.argmax(y_train_pred, axis=1)
    y_test_pred_class = np.argmax(y_test_pred, axis=1)
    
    # –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    train_accuracy = accuracy_score(y_train, y_train_pred_class)
    test_accuracy = accuracy_score(y_test, y_test_pred_class)
    
    print(f"\n=== –†–µ–∑—É–ª—å—Ç–∞—Ç—ã ===")
    print(f"Train accuracy: {train_accuracy:.4f}")
    print(f"Test accuracy: {test_accuracy:.4f}")
    print(f"Best iteration: {model.best_iteration}")
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    print(f"\n=== Classification Report (Test) ===")
    print(classification_report(y_test, y_test_pred_class))
    
    # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_importance = model.feature_importance(importance_type='gain')
    feature_names = [f'feature_{i}' for i in range(X.shape[1])]
    
    # –°–æ–∑–¥–∞–Ω–∏–µ DataFrame —Å –≤–∞–∂–Ω–æ—Å—Ç—å—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': feature_importance
    }).sort_values('importance', ascending=False)
    
    print(f"\n=== –¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ===")
    print(importance_df.head(10))
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞
    metrics = {
        'train_accuracy': train_accuracy,
        'test_accuracy': test_accuracy,
        'best_iteration': model.best_iteration,
        'feature_importance': importance_df,
        'confusion_matrix': confusion_matrix(y_test, y_test_pred_class),
        'predictions': y_test_pred_class,
        'probabilities': y_test_pred
    }
    
    return model, metrics, importance_df

def plot_lightgbm_importance(importance_df, top_n=15):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ LightGBM"""
    
    plt.figure(figsize=(12, 8))
    top_features = importance_df.head(top_n)
    
    sns.barplot(data=top_features, x='importance', y='feature')
    plt.title(f'–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ LightGBM (Top {top_n})')
    plt.xlabel('–í–∞–∂–Ω–æ—Å—Ç—å')
    plt.ylabel('–ü—Ä–∏–∑–Ω–∞–∫–∏')
    plt.tight_layout()
    plt.show()

def plot_lightgbm_learning_curve(model):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫—Ä–∏–≤–æ–π –æ–±—É—á–µ–Ω–∏—è LightGBM"""
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è
    history = model.evals_result_
    
    plt.figure(figsize=(12, 4))
    
    # –ì—Ä–∞—Ñ–∏–∫ –æ—à–∏–±–∫–∏
    plt.subplot(1, 2, 1)
    train_loss = history['training']['multi_logloss']
    valid_loss = history['valid_0']['multi_logloss']
    
    plt.plot(train_loss, label='Train')
    plt.plot(valid_loss, label='Validation')
    plt.title('Learning Curve - Multi Log Loss')
    plt.xlabel('Iterations')
    plt.ylabel('Multi Log Loss')
    plt.legend()
    plt.grid(True)
    
    # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏
    plt.subplot(1, 2, 2)
    train_acc = [1 - x for x in train_loss]
    valid_acc = [1 - x for x in valid_loss]
    
    plt.plot(train_acc, label='Train')
    plt.plot(valid_acc, label='Validation')
    plt.title('Learning Curve - Accuracy')
    plt.xlabel('Iterations')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.show()

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
def example_lightgbm_usage():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è LightGBM"""
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
    np.random.seed(42)
    n_samples, n_features = 1000, 20
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –Ω–µ–∫–æ—Ç–æ—Ä–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
    X = np.random.randn(n_samples, n_features)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π —Å –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–º–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏
    y = np.zeros(n_samples)
    for i in range(n_samples):
        # –°–ª–æ–∂–Ω–∞—è –Ω–µ–ª–∏–Ω–µ–π–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å
        score = (X[i, 0] ** 2 + X[i, 1] * X[i, 2] + 
                np.sin(X[i, 3]) + X[i, 4] * X[i, 5])
        
        if score > 2.0:
            y[i] = 2  # –ö–ª–∞—Å—Å 2
        elif score > 0.5:
            y[i] = 1  # –ö–ª–∞—Å—Å 1
        else:
            y[i] = 0  # –ö–ª–∞—Å—Å 0
    
    print("=== –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è LightGBM ===")
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model, metrics, importance_df = train_lightgbm(X, y)
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    plot_lightgbm_importance(importance_df)
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫—Ä–∏–≤–æ–π –æ–±—É—á–µ–Ω–∏—è
    plot_lightgbm_learning_curve(model)
    
    return model, metrics

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–∞ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
# model, metrics = example_lightgbm_usage()
```

## –ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏

**–¢–µ–æ—Ä–∏—è:** –ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ - —ç—Ç–æ –º–æ—â–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –Ω–µ–ª–∏–Ω–µ–π–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –û–Ω–∏ –æ—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è —Å–∫—Ä—ã—Ç—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏.

**–ü–æ—á–µ–º—É –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤:**
- **–ù–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å:** –ú–æ–≥—É—Ç –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
- **–í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è:** –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã—è–≤–ª—è—é—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
- **–ê–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å:** –ú–æ–≥—É—Ç –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ –∏–∑–º–µ–Ω—è—é—â–∏–º—Å—è —Ä—ã–Ω–æ—á–Ω—ã–º —É—Å–ª–æ–≤–∏—è–º
- **–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å:** –•–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞—é—Ç —Å –±–æ–ª—å—à–∏–º–∏ –æ–±—ä–µ–º–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö

### 1. –ü—Ä–æ—Å—Ç–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å

**–¢–µ–æ—Ä–∏—è:** –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å (Multi-Layer Perceptron) —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–ª–æ–µ–≤ –Ω–µ–π—Ä–æ–Ω–æ–≤, —Å–æ–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –≤–µ—Å–∞–º–∏. –ö–∞–∂–¥—ã–π –Ω–µ–π—Ä–æ–Ω –ø—Ä–∏–º–µ–Ω—è–µ—Ç –Ω–µ–ª–∏–Ω–µ–π–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –∫ –≤–∑–≤–µ—à–µ–Ω–Ω–æ–π —Å—É–º–º–µ –≤—Ö–æ–¥–æ–≤.

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–µ—Ç–∏:**
- **–í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π:** –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ–Ω–æ–≤ = –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
- **–°–∫—Ä—ã—Ç—ã–µ —Å–ª–æ–∏:** 2-3 —Å–ª–æ—è —Å 64-256 –Ω–µ–π—Ä–æ–Ω–∞–º–∏ –∫–∞–∂–¥—ã–π
- **–í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π:** –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ–Ω–æ–≤ = –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤
- **Dropout:** –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
- **–ê–∫—Ç–∏–≤–∞—Ü–∏—è:** ReLU –¥–ª—è —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ–µ–≤, Softmax –¥–ª—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:**

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç –∫–æ–¥:**
1. **–°–æ–∑–¥–∞–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã:** –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏
2. **–û–±—É—á–µ–Ω–∏–µ:** –ò—Å–ø–æ–ª—å–∑—É–µ—Ç backpropagation –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤–µ—Å–æ–≤
3. **–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è:** –ü—Ä–∏–º–µ–Ω—è–µ—Ç dropout –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
4. **–í–∞–ª–∏–¥–∞—Ü–∏—è:** –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split

class TradingNN(nn.Module):
    """
    –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –¥–ª—è —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    
    –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
    - –í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π: input_size –Ω–µ–π—Ä–æ–Ω–æ–≤
    - 3 —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—è: hidden_size –Ω–µ–π—Ä–æ–Ω–æ–≤ –∫–∞–∂–¥—ã–π
    - –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π: num_classes –Ω–µ–π—Ä–æ–Ω–æ–≤
    - Dropout: 0.2 –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
    - –ê–∫—Ç–∏–≤–∞—Ü–∏—è: ReLU –¥–ª—è —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ–µ–≤
    """
    
    def __init__(self, input_size, hidden_size=128, num_classes=3, dropout_rate=0.2):
        super(TradingNN, self).__init__()
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ª–æ–µ–≤
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, hidden_size)
        self.fc4 = nn.Linear(hidden_size, num_classes)
        
        # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
        self.dropout = nn.Dropout(dropout_rate)
        
        # –§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏
        self.relu = nn.ReLU()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
        self._initialize_weights()
    
    def _initialize_weights(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –¥–ª—è –ª—É—á—à–µ–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        """–ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ —Å–µ—Ç—å"""
        # –ü–µ—Ä–≤—ã–π —Å–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        
        # –í—Ç–æ—Ä–æ–π —Å–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π
        x = self.relu(self.fc2(x))
        x = self.dropout(x)
        
        # –¢—Ä–µ—Ç–∏–π —Å–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π
        x = self.relu(self.fc3(x))
        x = self.dropout(x)
        
        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π (–±–µ–∑ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ - –±—É–¥–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω –≤ loss function)
        x = self.fc4(x)
        
        return x

def train_neural_network(X, y, epochs=100, batch_size=32, learning_rate=0.001, 
                        test_size=0.2, random_state=42):
    """
    –û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –¥–ª—è —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    
    Args:
        X (array-like): –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        y (array-like): –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        epochs (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è
        batch_size (int): –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        learning_rate (float): –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
        test_size (float): –î–æ–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        random_state (int): Seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    
    Returns:
        tuple: (–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å, –º–µ—Ç—Ä–∏–∫–∏, –∏—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è)
    """
    
    print("=== –û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ ===")
    print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {X.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤, {X.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    print(f"–ö–ª–∞—Å—Å—ã: {np.unique(y, return_counts=True)}")
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state, stratify=y
    )
    
    print(f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ç–µ–Ω–∑–æ—Ä—ã PyTorch
    X_train_tensor = torch.FloatTensor(X_train)
    y_train_tensor = torch.LongTensor(y_train)
    X_test_tensor = torch.FloatTensor(X_test)
    y_test_tensor = torch.LongTensor(y_test)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ DataLoader
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model = TradingNN(X.shape[1], num_classes=len(np.unique(y)))
    
    # –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    
    # –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
    train_losses = []
    train_accuracies = []
    test_accuracies = []
    
    print(f"\n–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:")
    print(f"Epochs: {epochs}")
    print(f"Batch size: {batch_size}")
    print(f"Learning rate: {learning_rate}")
    print(f"Model parameters: {sum(p.numel() for p in model.parameters())}")
    
    # –û–±—É—á–µ–Ω–∏–µ
    print("\n–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
    model.train()
    
    for epoch in range(epochs):
        epoch_loss = 0.0
        correct = 0
        total = 0
        
        for batch_X, batch_y in train_dataloader:
            # –û–±–Ω—É–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
            optimizer.zero_grad()
            
            # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            
            # –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥
            loss.backward()
            optimizer.step()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            epoch_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        avg_loss = epoch_loss / len(train_dataloader)
        train_accuracy = correct / total
        
        train_losses.append(avg_loss)
        train_accuracies.append(train_accuracy)
        
        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        model.eval()
        with torch.no_grad():
            test_outputs = model(X_test_tensor)
            _, test_predicted = torch.max(test_outputs.data, 1)
            test_accuracy = accuracy_score(y_test, test_predicted.numpy())
            test_accuracies.append(test_accuracy)
        model.train()
        
        # –í—ã–≤–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        if epoch % 10 == 0 or epoch == epochs - 1:
            print(f'Epoch {epoch:3d}/{epochs}: '
                  f'Loss: {avg_loss:.4f}, '
                  f'Train Acc: {train_accuracy:.4f}, '
                  f'Test Acc: {test_accuracy:.4f}')
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
    model.eval()
    with torch.no_grad():
        test_outputs = model(X_test_tensor)
        _, test_predicted = torch.max(test_outputs.data, 1)
        test_predictions = test_predicted.numpy()
        test_probabilities = torch.softmax(test_outputs, dim=1).numpy()
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    final_accuracy = accuracy_score(y_test, test_predictions)
    
    print(f"\n=== –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã ===")
    print(f"Test accuracy: {final_accuracy:.4f}")
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    print(f"\n=== Classification Report ===")
    print(classification_report(y_test, test_predictions))
    
    # –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
    history = {
        'train_losses': train_losses,
        'train_accuracies': train_accuracies,
        'test_accuracies': test_accuracies
    }
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞
    metrics = {
        'test_accuracy': final_accuracy,
        'confusion_matrix': confusion_matrix(y_test, test_predictions),
        'predictions': test_predictions,
        'probabilities': test_probabilities,
        'history': history
    }
    
    return model, metrics, history

def plot_training_history(history):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è"""
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å
    ax1.plot(history['train_losses'], label='Train Loss')
    ax1.set_title('Training Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True)
    
    # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏
    ax2.plot(history['train_accuracies'], label='Train Accuracy')
    ax2.plot(history['test_accuracies'], label='Test Accuracy')
    ax2.set_title('Training and Test Accuracy')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy')
    ax2.legend()
    ax2.grid(True)
    
    plt.tight_layout()
    plt.show()

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
def example_neural_network_usage():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏"""
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    np.random.seed(42)
    n_samples, n_features = 1000, 20
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    X = np.random.randn(n_samples, n_features)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π —Å –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–º–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏
    y = np.zeros(n_samples)
    for i in range(n_samples):
        # –°–ª–æ–∂–Ω–∞—è –Ω–µ–ª–∏–Ω–µ–π–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å
        score = (X[i, 0] ** 2 + X[i, 1] * X[i, 2] + 
                np.sin(X[i, 3]) + X[i, 4] * X[i, 5])
        
        if score > 2.0:
            y[i] = 2  # –ö–ª–∞—Å—Å 2
        elif score > 0.5:
            y[i] = 1  # –ö–ª–∞—Å—Å 1
        else:
            y[i] = 0  # –ö–ª–∞—Å—Å 0
    
    print("=== –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ ===")
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model, metrics, history = train_neural_network(X, y, epochs=50)
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è
    plot_training_history(history)
    
    return model, metrics

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–∞ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
# model, metrics = example_neural_network_usage()
```

### 2. LSTM –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤

**–¢–µ–æ—Ä–∏—è:** LSTM (Long Short-Term Memory) - —ç—Ç–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ç–∏–ø —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º–∏. LSTM –æ—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —Ç–∞–∫ –∫–∞–∫ –º–æ–∂–µ—Ç –∑–∞–ø–æ–º–∏–Ω–∞—Ç—å –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã.

**–î–µ—Ç–∞–ª—å–Ω–∞—è —Ç–µ–æ—Ä–∏—è LSTM:**

**–ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã:**
1. **–ó–∞–±—ã–≤–∞—é—â–∏–π –≥–µ–π—Ç (Forget Gate):** –†–µ—à–∞–µ—Ç, –∫–∞–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∑–∞–±—ã—Ç—å –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
2. **–í—Ö–æ–¥–Ω–æ–π –≥–µ–π—Ç (Input Gate):** –†–µ—à–∞–µ—Ç, –∫–∞–∫—É—é –Ω–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å
3. **–ì–µ–π—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è (Update Gate):** –û–±–Ω–æ–≤–ª—è–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ —è—á–µ–π–∫–∏
4. **–í—ã—Ö–æ–¥–Ω–æ–π –≥–µ–π—Ç (Output Gate):** –†–µ—à–∞–µ—Ç, –∫–∞–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤—ã–≤–µ—Å—Ç–∏

**–ü–æ—á–µ–º—É LSTM —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤:**
- **–í—Ä–µ–º–µ–Ω–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:** –ú–æ–∂–µ—Ç –∑–∞–ø–æ–º–∏–Ω–∞—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞—Ö
- **–£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–º—É –∏—Å—á–µ–∑–Ω–æ–≤–µ–Ω–∏—é:** –†–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É RNN
- **–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π:** –ò–¥–µ–∞–ª—å–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
- **–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:** –£—á–∏—Ç—ã–≤–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π

**–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Å–Ω–æ–≤–∞:**
- **Forget Gate:** f_t = œÉ(W_f ¬∑ [h_{t-1}, x_t] + b_f)
- **Input Gate:** i_t = œÉ(W_i ¬∑ [h_{t-1}, x_t] + b_i)
- **Cell State:** CÃÉ_t = tanh(W_C ¬∑ [h_{t-1}, x_t] + b_C)
- **Update:** C_t = f_t * C_{t-1} + i_t * CÃÉ_t
- **Output Gate:** o_t = œÉ(W_o ¬∑ [h_{t-1}, x_t] + b_o)
- **Hidden State:** h_t = o_t * tanh(C_t)

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è LSTM:**

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç –∫–æ–¥:**
1. **–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π:** –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
2. **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ LSTM:** –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ–π —Å–µ—Ç–∏
3. **–û–±—É—á–µ–Ω–∏–µ:** –ò—Å–ø–æ–ª—å–∑—É–µ—Ç backpropagation through time (BPTT)
4. **–í–∞–ª–∏–¥–∞—Ü–∏—è:** –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split

class LSTMTradingModel(nn.Module):
    """
    LSTM –º–æ–¥–µ–ª—å –¥–ª—è —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–∞—Ö
    
    –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
    - LSTM —Å–ª–æ–∏: –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
    - Dropout: –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
    - –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π: –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    """
    
    def __init__(self, input_size, hidden_size=64, num_layers=2, num_classes=3, dropout_rate=0.2):
        super(LSTMTradingModel, self).__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # LSTM —Å–ª–æ–∏
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout_rate if num_layers > 1 else 0,
            bidirectional=False
        )
        
        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        self.fc = nn.Linear(hidden_size, num_classes)
        
        # Dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
        self.dropout = nn.Dropout(dropout_rate)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
        self._initialize_weights()
    
    def _initialize_weights(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ LSTM"""
        for name, param in self.named_parameters():
            if 'weight_ih' in name:
                nn.init.xavier_uniform_(param.data)
            elif 'weight_hh' in name:
                nn.init.orthogonal_(param.data)
            elif 'bias' in name:
                param.data.fill_(0)
                # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ forget gate bias –≤ 1 –¥–ª—è –ª—É—á—à–µ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
                n = param.size(0)
                param.data[(n//4):(n//2)].fill_(1)
    
    def forward(self, x):
        """
        –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ LSTM
        
        Args:
            x: –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ñ–æ—Ä–º—ã (batch_size, sequence_length, input_size)
        
        Returns:
            –í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ñ–æ—Ä–º—ã (batch_size, num_classes)
        """
        batch_size = x.size(0)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=x.device)
        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=x.device)
        
        # LSTM forward pass
        lstm_out, (hn, cn) = self.lstm(x, (h0, c0))
        
        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—ã—Ö–æ–¥ –∏–∑ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        last_output = lstm_out[:, -1, :]  # (batch_size, hidden_size)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º dropout
        last_output = self.dropout(last_output)
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        output = self.fc(last_output)
        
        return output

def create_sequences(X, y, sequence_length):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è LSTM
    
    Args:
        X: –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (samples, features)
        y: –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (samples,)
        sequence_length: –î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    
    Returns:
        X_seq: –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (samples-seq_len+1, seq_len, features)
        y_seq: –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π (samples-seq_len+1,)
    """
    X_seq, y_seq = [], []
    
    for i in range(sequence_length, len(X)):
        X_seq.append(X[i-sequence_length:i])
        y_seq.append(y[i])
    
    return np.array(X_seq), np.array(y_seq)

def train_lstm_model(X, y, sequence_length=10, epochs=100, batch_size=32, 
                    learning_rate=0.001, test_size=0.2, random_state=42):
    """
    –û–±—É—á–µ–Ω–∏–µ LSTM –º–æ–¥–µ–ª–∏ –¥–ª—è —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    
    Args:
        X: –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        y: –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        sequence_length: –î–ª–∏–Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è
        batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        learning_rate: –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
        test_size: –î–æ–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        random_state: Seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    
    Returns:
        tuple: (–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å, –º–µ—Ç—Ä–∏–∫–∏, –∏—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è)
    """
    
    print("=== –û–±—É—á–µ–Ω–∏–µ LSTM –º–æ–¥–µ–ª–∏ ===")
    print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {X.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤, {X.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    print(f"–ö–ª–∞—Å—Å—ã: {np.unique(y, return_counts=True)}")
    print(f"–î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {sequence_length}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
    X_seq, y_seq = create_sequences(X, y, sequence_length)
    print(f"–†–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π: {X_seq.shape}")
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    X_train, X_test, y_train, y_test = train_test_split(
        X_seq, y_seq, test_size=test_size, random_state=random_state, stratify=y_seq
    )
    
    print(f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train.shape[0]} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
    print(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test.shape[0]} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ç–µ–Ω–∑–æ—Ä—ã PyTorch
    X_train_tensor = torch.FloatTensor(X_train)
    y_train_tensor = torch.LongTensor(y_train)
    X_test_tensor = torch.FloatTensor(X_test)
    y_test_tensor = torch.LongTensor(y_test)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ DataLoader
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model = LSTMTradingModel(
        input_size=X.shape[1], 
        num_classes=len(np.unique(y)),
        hidden_size=64,
        num_layers=2
    )
    
    # –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    
    # –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
    train_losses = []
    train_accuracies = []
    test_accuracies = []
    
    print(f"\n–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:")
    print(f"Epochs: {epochs}")
    print(f"Batch size: {batch_size}")
    print(f"Learning rate: {learning_rate}")
    print(f"Sequence length: {sequence_length}")
    print(f"Model parameters: {sum(p.numel() for p in model.parameters())}")
    
    # –û–±—É—á–µ–Ω–∏–µ
    print("\n–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
    model.train()
    
    for epoch in range(epochs):
        epoch_loss = 0.0
        correct = 0
        total = 0
        
        for batch_X, batch_y in train_dataloader:
            # –û–±–Ω—É–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
            optimizer.zero_grad()
            
            # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            
            # –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥
            loss.backward()
            
            # –û–±—Ä–µ–∑–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            epoch_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        avg_loss = epoch_loss / len(train_dataloader)
        train_accuracy = correct / total
        
        train_losses.append(avg_loss)
        train_accuracies.append(train_accuracy)
        
        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        model.eval()
        with torch.no_grad():
            test_outputs = model(X_test_tensor)
            _, test_predicted = torch.max(test_outputs.data, 1)
            test_accuracy = accuracy_score(y_test, test_predicted.numpy())
            test_accuracies.append(test_accuracy)
        model.train()
        
        # –í—ã–≤–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        if epoch % 10 == 0 or epoch == epochs - 1:
            print(f'Epoch {epoch:3d}/{epochs}: '
                  f'Loss: {avg_loss:.4f}, '
                  f'Train Acc: {train_accuracy:.4f}, '
                  f'Test Acc: {test_accuracy:.4f}')
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
    model.eval()
    with torch.no_grad():
        test_outputs = model(X_test_tensor)
        _, test_predicted = torch.max(test_outputs.data, 1)
        test_predictions = test_predicted.numpy()
        test_probabilities = torch.softmax(test_outputs, dim=1).numpy()
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    final_accuracy = accuracy_score(y_test, test_predictions)
    
    print(f"\n=== –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã ===")
    print(f"Test accuracy: {final_accuracy:.4f}")
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    print(f"\n=== Classification Report ===")
    print(classification_report(y_test, test_predictions))
    
    # –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
    history = {
        'train_losses': train_losses,
        'train_accuracies': train_accuracies,
        'test_accuracies': test_accuracies
    }
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞
    metrics = {
        'test_accuracy': final_accuracy,
        'confusion_matrix': confusion_matrix(y_test, test_predictions),
        'predictions': test_predictions,
        'probabilities': test_probabilities,
        'history': history
    }
    
    return model, metrics, history

def plot_lstm_training_history(history):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è LSTM"""
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å
    ax1.plot(history['train_losses'], label='Train Loss')
    ax1.set_title('LSTM Training Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True)
    
    # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏
    ax2.plot(history['train_accuracies'], label='Train Accuracy')
    ax2.plot(history['test_accuracies'], label='Test Accuracy')
    ax2.set_title('LSTM Training and Test Accuracy')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy')
    ax2.legend()
    ax2.grid(True)
    
    plt.tight_layout()
    plt.show()

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
def example_lstm_usage():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è LSTM"""
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    np.random.seed(42)
    n_samples, n_features = 1000, 10
    sequence_length = 10
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
    X = np.random.randn(n_samples, n_features)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏
    y = np.zeros(n_samples)
    for i in range(n_samples):
        # –í—Ä–µ–º–µ–Ω–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å: —Ç–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö
        if i < sequence_length:
            y[i] = 0  # –ù–∞—á–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        else:
            # –°–ª–æ–∂–Ω–∞—è –≤—Ä–µ–º–µ–Ω–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å
            recent_sum = np.sum(X[i-sequence_length:i, 0])
            recent_volatility = np.std(X[i-sequence_length:i, 1])
            
            if recent_sum > 2.0 and recent_volatility < 1.0:
                y[i] = 2  # –ö–ª–∞—Å—Å 2
            elif recent_sum > 0.5 or recent_volatility > 1.5:
                y[i] = 1  # –ö–ª–∞—Å—Å 1
            else:
                y[i] = 0  # –ö–ª–∞—Å—Å 0
    
    print("=== –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è LSTM ===")
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model, metrics, history = train_lstm_model(
        X, y, 
        sequence_length=sequence_length, 
        epochs=50
    )
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è
    plot_lstm_training_history(history)
    
    return model, metrics

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–∞ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
# model, metrics = example_lstm_usage()
```

## –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π

**–¢–µ–æ—Ä–∏—è:** –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–∞, —Ç–∞–∫ –∫–∞–∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏ –º–æ–≥—É—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ data leakage (—É—Ç–µ—á–∫–µ –¥–∞–Ω–Ω—ã—Ö) –∏–∑-–∑–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø—Ä–∏—Ä–æ–¥—ã —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

**–ü–æ—á–µ–º—É —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç:**
- **Data Leakage:** –ë—É–¥—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ –º–æ–≥—É—Ç "–ø—Ä–æ—Ç–µ–∫–∞—Ç—å" –≤ –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É
- **–í—Ä–µ–º–µ–Ω–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å:** –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–º–µ—é—Ç –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
- **–ù–µ—Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ—Å—Ç—å:** –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–µ–Ω—è—é—Ç—Å—è –≤–æ –≤—Ä–µ–º–µ–Ω–∏
- **–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç—å:** –ù—É–∂–Ω–æ –∏–º–∏—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ —Ç–æ—Ä–≥–æ–≤—ã–µ —É—Å–ª–æ–≤–∏—è

### 1. Time Series Cross Validation

**–¢–µ–æ—Ä–∏—è:** Time Series Cross Validation (TSCV) - —ç—Ç–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç data leakage, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –ø—Ä–æ—à–ª—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±—É–¥—É—â–∏—Ö.

**–ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã TSCV:**
1. **–í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ:** –î–∞–Ω–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—è—é—Ç—Å—è –ø–æ –≤—Ä–µ–º–µ–Ω–∏, –∞ –Ω–µ —Å–ª—É—á–∞–π–Ω–æ
2. **–°—Ç—Ä–æ–≥–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å:** –ö–∞–∂–¥–∞—è —Å–ª–µ–¥—É—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞ –≤–∫–ª—é—á–∞–µ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ
3. **–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç—å:** –ò–º–∏—Ç–∏—Ä—É–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ —É—Å–ª–æ–≤–∏—è —Ç–æ—Ä–≥–æ–≤–ª–∏
4. **–ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ leakage:** –ë—É–¥—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è TSCV:**

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç –∫–æ–¥:**
1. **–í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ:** –°–æ–∑–¥–∞–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–æ–ª–¥—ã –±–µ–∑ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π
2. **–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –∏—Å—Ç–æ—Ä–∏–∏:** –ö–∞–∂–¥–∞—è –º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ –ø—Ä–æ—à–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö
3. **–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –±—É–¥—É—â–µ–º:** –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–µ–ª–∞—é—Ç—Å—è –Ω–∞ –±—É–¥—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
4. **–ú–µ—Ç—Ä–∏–∫–∏:** –í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–æ–ª–¥–∞

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                           f1_score, confusion_matrix, classification_report)
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Any, Tuple

def time_series_cv(model, X, y, n_splits=5, test_size=None, random_state=42):
    """
    –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
    
    Args:
        model: –ú–æ–¥–µ–ª—å –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–¥–æ–ª–∂–Ω–∞ –∏–º–µ—Ç—å –º–µ—Ç–æ–¥—ã fit –∏ predict)
        X: –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (samples, features)
        y: –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (samples,)
        n_splits: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ–ª–¥–æ–≤
        test_size: –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —Ñ–æ–ª–¥–∞ (–µ—Å–ª–∏ None, –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
        random_state: Seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    
    Returns:
        dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–æ–ª–¥–∞
    """
    
    print("=== Time Series Cross Validation ===")
    print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {X.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤, {X.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ–ª–¥–æ–≤: {n_splits}")
    print(f"–ö–ª–∞—Å—Å—ã: {np.unique(y, return_counts=True)}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ TimeSeriesSplit
    tscv = TimeSeriesSplit(n_splits=n_splits, test_size=test_size)
    
    # –°–ø–∏—Å–∫–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    fold_scores = []
    fold_predictions = []
    fold_confusion_matrices = []
    
    print(f"\n–ù–∞—á–∞–ª–æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏...")
    
    for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):
        print(f"\n--- Fold {fold + 1}/{n_splits} ---")
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        X_train, X_test = X[train_idx], X[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]
        
        print(f"Train size: {len(train_idx)} ({len(train_idx)/len(X)*100:.1f}%)")
        print(f"Test size: {len(test_idx)} ({len(test_idx)/len(X)*100:.1f}%)")
        print(f"Train period: {train_idx[0]} - {train_idx[-1]}")
        print(f"Test period: {test_idx[0]} - {test_idx[-1]}")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ø–∏–∏ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–æ–ª–¥–∞
        fold_model = type(model)(**model.get_params())
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        print("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        fold_model.fit(X_train, y_train)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_pred = fold_model.predict(X_test)
        y_pred_proba = fold_model.predict_proba(X_test) if hasattr(fold_model, 'predict_proba') else None
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
        
        # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
        cm = confusion_matrix(y_test, y_pred)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        fold_score = {
            'fold': fold + 1,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'train_size': len(train_idx),
            'test_size': len(test_idx)
        }
        
        fold_scores.append(fold_score)
        fold_predictions.append({
            'y_true': y_test,
            'y_pred': y_pred,
            'y_pred_proba': y_pred_proba
        })
        fold_confusion_matrices.append(cm)
        
        print(f"Accuracy: {accuracy:.4f}")
        print(f"Precision: {precision:.4f}")
        print(f"Recall: {recall:.4f}")
        print(f"F1: {f1:.4f}")
    
    # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results = {
        'fold_scores': fold_scores,
        'fold_predictions': fold_predictions,
        'fold_confusion_matrices': fold_confusion_matrices,
        'mean_accuracy': np.mean([s['accuracy'] for s in fold_scores]),
        'std_accuracy': np.std([s['accuracy'] for s in fold_scores]),
        'mean_precision': np.mean([s['precision'] for s in fold_scores]),
        'mean_recall': np.mean([s['recall'] for s in fold_scores]),
        'mean_f1': np.mean([s['f1'] for s in fold_scores])
    }
    
    # –í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print(f"\n=== –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã TSCV ===")
    print(f"Mean Accuracy: {results['mean_accuracy']:.4f} ¬± {results['std_accuracy']:.4f}")
    print(f"Mean Precision: {results['mean_precision']:.4f}")
    print(f"Mean Recall: {results['mean_recall']:.4f}")
    print(f"Mean F1: {results['mean_f1']:.4f}")
    
    return results

def plot_tscv_results(results, figsize=(15, 10)):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Time Series Cross Validation"""
    
    fig, axes = plt.subplots(2, 2, figsize=figsize)
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    fold_scores = results['fold_scores']
    folds = [s['fold'] for s in fold_scores]
    accuracies = [s['accuracy'] for s in fold_scores]
    precisions = [s['precision'] for s in fold_scores]
    recalls = [s['recall'] for s in fold_scores]
    f1_scores = [s['f1'] for s in fold_scores]
    
    # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ —Ñ–æ–ª–¥–∞–º
    axes[0, 0].plot(folds, accuracies, 'o-', label='Accuracy')
    axes[0, 0].axhline(y=results['mean_accuracy'], color='r', linestyle='--', 
                      label=f'Mean: {results["mean_accuracy"]:.3f}')
    axes[0, 0].set_title('Accuracy by Fold')
    axes[0, 0].set_xlabel('Fold')
    axes[0, 0].set_ylabel('Accuracy')
    axes[0, 0].legend()
    axes[0, 0].grid(True)
    
    # –ì—Ä–∞—Ñ–∏–∫ –º–µ—Ç—Ä–∏–∫ –ø–æ —Ñ–æ–ª–¥–∞–º
    axes[0, 1].plot(folds, accuracies, 'o-', label='Accuracy')
    axes[0, 1].plot(folds, precisions, 's-', label='Precision')
    axes[0, 1].plot(folds, recalls, '^-', label='Recall')
    axes[0, 1].plot(folds, f1_scores, 'd-', label='F1')
    axes[0, 1].set_title('Metrics by Fold')
    axes[0, 1].set_xlabel('Fold')
    axes[0, 1].set_ylabel('Score')
    axes[0, 1].legend()
    axes[0, 1].grid(True)
    
    # Box plot –º–µ—Ç—Ä–∏–∫
    metrics_data = [accuracies, precisions, recalls, f1_scores]
    metrics_labels = ['Accuracy', 'Precision', 'Recall', 'F1']
    axes[1, 0].boxplot(metrics_data, labels=metrics_labels)
    axes[1, 0].set_title('Distribution of Metrics')
    axes[1, 0].set_ylabel('Score')
    axes[1, 0].grid(True)
    
    # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
    if results['fold_confusion_matrices']:
        # –°—É–º–º–∏—Ä—É–µ–º –≤—Å–µ –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫
        total_cm = np.sum(results['fold_confusion_matrices'], axis=0)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–ª—è –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤
        total_cm_norm = total_cm.astype('float') / total_cm.sum(axis=1)[:, np.newaxis]
        
        sns.heatmap(total_cm_norm, annot=True, fmt='.2f', cmap='Blues', ax=axes[1, 1])
        axes[1, 1].set_title('Aggregated Confusion Matrix')
        axes[1, 1].set_xlabel('Predicted')
        axes[1, 1].set_ylabel('True')
    
    plt.tight_layout()
    plt.show()

def analyze_tscv_stability(results):
    """–ê–Ω–∞–ª–∏–∑ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ TSCV"""
    
    fold_scores = results['fold_scores']
    accuracies = [s['accuracy'] for s in fold_scores]
    
    print("=== –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ TSCV ===")
    print(f"Accuracy - Min: {min(accuracies):.4f}, Max: {max(accuracies):.4f}")
    print(f"Accuracy - Range: {max(accuracies) - min(accuracies):.4f}")
    print(f"Accuracy - Std: {np.std(accuracies):.4f}")
    print(f"Accuracy - CV: {np.std(accuracies) / np.mean(accuracies):.4f}")
    
    # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–∞
    if len(accuracies) >= 3:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ç—Ä–µ–Ω–¥ (—É–ª—É—á—à–µ–Ω–∏–µ/—É—Ö—É–¥—à–µ–Ω–∏–µ —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º)
        from scipy import stats
        slope, intercept, r_value, p_value, std_err = stats.linregress(range(len(accuracies)), accuracies)
        
        print(f"\n–¢—Ä–µ–Ω–¥ —Ç–æ—á–Ω–æ—Å—Ç–∏:")
        print(f"Slope: {slope:.6f} (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π = —É–ª—É—á—à–µ–Ω–∏–µ)")
        print(f"R-squared: {r_value**2:.4f}")
        print(f"P-value: {p_value:.4f}")
        
        if p_value < 0.05:
            if slope > 0:
                print("–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º")
            else:
                print("–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º–æ–µ —É—Ö—É–¥—à–µ–Ω–∏–µ —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º")
        else:
            print("–ù–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º–æ–≥–æ —Ç—Ä–µ–Ω–¥–∞")

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
def example_tscv_usage():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Time Series Cross Validation"""
    
    from sklearn.ensemble import RandomForestClassifier
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    np.random.seed(42)
    n_samples, n_features = 1000, 20
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
    X = np.random.randn(n_samples, n_features)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏
    y = np.zeros(n_samples)
    for i in range(n_samples):
        # –í—Ä–µ–º–µ–Ω–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å
        if i < 100:
            y[i] = 0
        elif i < 500:
            y[i] = 1 if X[i, 0] > 0 else 0
        else:
            y[i] = 2 if X[i, 0] > 0.5 else (1 if X[i, 0] > -0.5 else 0)
    
    print("=== –ü—Ä–∏–º–µ—Ä Time Series Cross Validation ===")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model = RandomForestClassifier(n_estimators=50, random_state=42)
    
    # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ TSCV
    results = time_series_cv(model, X, y, n_splits=5)
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    plot_tscv_results(results)
    
    # –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    analyze_tscv_stability(results)
    
    return results

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–∞ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
# results = example_tscv_usage()
```

### 2. Walk-Forward Validation

**–¢–µ–æ—Ä–∏—è:** Walk-Forward Validation (WFV) - —ç—Ç–æ –º–µ—Ç–æ–¥ –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –∏–º–∏—Ç–∏—Ä—É–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ —Ç–æ—Ä–≥–æ–≤—ã–µ —É—Å–ª–æ–≤–∏—è, –≥–¥–µ –º–æ–¥–µ–ª—å –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –ø–µ—Ä–µ–æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –¥–µ–ª–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –ø–µ—Ä–∏–æ–¥.

**–ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã WFV:**
1. **–°–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ:** –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞ –∏–º–µ–µ—Ç —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä
2. **–ü–æ—à–∞–≥–æ–≤–æ–µ –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏–µ:** –û–∫–Ω–æ —Å–¥–≤–∏–≥–∞–µ—Ç—Å—è –Ω–∞ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —à–∞–≥
3. **–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç—å:** –ò–º–∏—Ç–∏—Ä—É–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ —Ç–æ—Ä–≥–æ–≤—ã–µ —É—Å–ª–æ–≤–∏—è
4. **–ê–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å:** –ú–æ–¥–µ–ª—å –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –∫ –Ω–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º

**–ü–æ—á–µ–º—É WFV —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤:**
- **–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç—å:** –¢–æ—á–Ω–æ –∏–º–∏—Ç–∏—Ä—É–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ —Ç–æ—Ä–≥–æ–≤—ã–µ —É—Å–ª–æ–≤–∏—è
- **–ê–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å:** –ú–æ–¥–µ–ª—å –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è
- **–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å:** –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –∫–∞–∫ –º–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–µ
- **–î—Ä–µ–π—Ñ –¥–∞–Ω–Ω—ã—Ö:** –ü–æ–º–æ–≥–∞–µ—Ç –≤—ã—è–≤–∏—Ç—å, –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å —É—Å—Ç–∞—Ä–µ–≤–∞–µ—Ç

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è WFV:**

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç –∫–æ–¥:**
1. **–°–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ:** –°–æ–∑–¥–∞–µ—Ç –æ–±—É—á–∞—é—â–∏–µ –≤—ã–±–æ—Ä–∫–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
2. **–ü–æ—à–∞–≥–æ–≤–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ:** –¢–µ—Å—Ç–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ —Å–ª–µ–¥—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
3. **–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ:** –ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ
4. **–ú–µ—Ç—Ä–∏–∫–∏:** –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤–æ –≤—Ä–µ–º–µ–Ω–∏

```python
import numpy as np
import pandas as pd
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                           f1_score, confusion_matrix)
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Any, Tuple

def walk_forward_validation(model, X, y, train_size=1000, step_size=100, 
                          min_test_size=50, random_state=42):
    """
    Walk-Forward –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
    
    Args:
        model: –ú–æ–¥–µ–ª—å –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–¥–æ–ª–∂–Ω–∞ –∏–º–µ—Ç—å –º–µ—Ç–æ–¥—ã fit –∏ predict)
        X: –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (samples, features)
        y: –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (samples,)
        train_size: –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–≥–æ –æ–∫–Ω–∞
        step_size: –†–∞–∑–º–µ—Ä —à–∞–≥–∞ –¥–ª—è –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏—è –æ–∫–Ω–∞
        min_test_size: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏
        random_state: Seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    
    Returns:
        dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã walk-forward –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    """
    
    print("=== Walk-Forward Validation ===")
    print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {X.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤, {X.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    print(f"–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–≥–æ –æ–∫–Ω–∞: {train_size}")
    print(f"–†–∞–∑–º–µ—Ä —à–∞–≥–∞: {step_size}")
    print(f"–ö–ª–∞—Å—Å—ã: {np.unique(y, return_counts=True)}")
    
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π
    n_iterations = (len(X) - train_size) // step_size
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π: {n_iterations}")
    
    # –°–ø–∏—Å–∫–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    iteration_results = []
    all_predictions = []
    all_true_labels = []
    
    print(f"\n–ù–∞—á–∞–ª–æ walk-forward –≤–∞–ª–∏–¥–∞—Ü–∏–∏...")
    
    for i in range(n_iterations):
        start_idx = i * step_size
        train_end_idx = start_idx + train_size
        test_start_idx = train_end_idx
        test_end_idx = min(test_start_idx + step_size, len(X))
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏
        if test_end_idx - test_start_idx < min_test_size:
            print(f"–ò—Ç–µ—Ä–∞—Ü–∏—è {i+1}: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            continue
        
        print(f"\n--- –ò—Ç–µ—Ä–∞—Ü–∏—è {i+1}/{n_iterations} ---")
        print(f"–û–±—É—á–∞—é—â–∏–π –ø–µ—Ä–∏–æ–¥: {start_idx} - {train_end_idx-1}")
        print(f"–¢–µ—Å—Ç–æ–≤—ã–π –ø–µ—Ä–∏–æ–¥: {test_start_idx} - {test_end_idx-1}")
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        X_train = X[start_idx:train_end_idx]
        y_train = y[start_idx:train_end_idx]
        X_test = X[test_start_idx:test_end_idx]
        y_test = y[test_start_idx:test_end_idx]
        
        print(f"Train size: {len(X_train)}")
        print(f"Test size: {len(X_test)}")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ø–∏–∏ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏
        fold_model = type(model)(**model.get_params())
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        print("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        fold_model.fit(X_train, y_train)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_pred = fold_model.predict(X_test)
        y_pred_proba = fold_model.predict_proba(X_test) if hasattr(fold_model, 'predict_proba') else None
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
        
        # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
        cm = confusion_matrix(y_test, y_pred)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        iteration_result = {
            'iteration': i + 1,
            'train_start': start_idx,
            'train_end': train_end_idx - 1,
            'test_start': test_start_idx,
            'test_end': test_end_idx - 1,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'train_size': len(X_train),
            'test_size': len(X_test),
            'confusion_matrix': cm
        }
        
        iteration_results.append(iteration_result)
        all_predictions.extend(y_pred)
        all_true_labels.extend(y_test)
        
        print(f"Accuracy: {accuracy:.4f}")
        print(f"Precision: {precision:.4f}")
        print(f"Recall: {recall:.4f}")
        print(f"F1: {f1:.4f}")
    
    # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    if iteration_results:
        accuracies = [r['accuracy'] for r in iteration_results]
        precisions = [r['precision'] for r in iteration_results]
        recalls = [r['recall'] for r in iteration_results]
        f1_scores = [r['f1'] for r in iteration_results]
        
        results = {
            'iteration_results': iteration_results,
            'all_predictions': np.array(all_predictions),
            'all_true_labels': np.array(all_true_labels),
            'mean_accuracy': np.mean(accuracies),
            'std_accuracy': np.std(accuracies),
            'mean_precision': np.mean(precisions),
            'mean_recall': np.mean(recalls),
            'mean_f1': np.mean(f1_scores),
            'n_iterations': len(iteration_results)
        }
        
        # –í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print(f"\n=== –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã Walk-Forward ===")
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π: {results['n_iterations']}")
        print(f"Mean Accuracy: {results['mean_accuracy']:.4f} ¬± {results['std_accuracy']:.4f}")
        print(f"Mean Precision: {results['mean_precision']:.4f}")
        print(f"Mean Recall: {results['mean_recall']:.4f}")
        print(f"Mean F1: {results['mean_f1']:.4f}")
        
        # –û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ –≤—Å–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º
        overall_accuracy = accuracy_score(all_true_labels, all_predictions)
        print(f"Overall Accuracy: {overall_accuracy:.4f}")
        
    else:
        print("–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        results = None
    
    return results

def plot_walk_forward_results(results, figsize=(15, 12)):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Walk-Forward –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
    
    if results is None:
        print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")
        return
    
    fig, axes = plt.subplots(2, 2, figsize=figsize)
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    iteration_results = results['iteration_results']
    iterations = [r['iteration'] for r in iteration_results]
    accuracies = [r['accuracy'] for r in iteration_results]
    precisions = [r['precision'] for r in iteration_results]
    recalls = [r['recall'] for r in iteration_results]
    f1_scores = [r['f1'] for r in iteration_results]
    
    # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ –∏—Ç–µ—Ä–∞—Ü–∏—è–º
    axes[0, 0].plot(iterations, accuracies, 'o-', label='Accuracy')
    axes[0, 0].axhline(y=results['mean_accuracy'], color='r', linestyle='--', 
                      label=f'Mean: {results["mean_accuracy"]:.3f}')
    axes[0, 0].set_title('Accuracy by Iteration')
    axes[0, 0].set_xlabel('Iteration')
    axes[0, 0].set_ylabel('Accuracy')
    axes[0, 0].legend()
    axes[0, 0].grid(True)
    
    # –ì—Ä–∞—Ñ–∏–∫ –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫ –ø–æ –∏—Ç–µ—Ä–∞—Ü–∏—è–º
    axes[0, 1].plot(iterations, accuracies, 'o-', label='Accuracy')
    axes[0, 1].plot(iterations, precisions, 's-', label='Precision')
    axes[0, 1].plot(iterations, recalls, '^-', label='Recall')
    axes[0, 1].plot(iterations, f1_scores, 'd-', label='F1')
    axes[0, 1].set_title('Metrics by Iteration')
    axes[0, 1].set_xlabel('Iteration')
    axes[0, 1].set_ylabel('Score')
    axes[0, 1].legend()
    axes[0, 1].grid(True)
    
    # –°–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ —Ç–æ—á–Ω–æ—Å—Ç–∏
    window_size = max(1, len(accuracies) // 5)  # 20% –æ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π
    if window_size > 1:
        rolling_accuracy = pd.Series(accuracies).rolling(window=window_size).mean()
        axes[1, 0].plot(iterations, accuracies, 'o-', alpha=0.3, label='Raw Accuracy')
        axes[1, 0].plot(iterations, rolling_accuracy, 'r-', linewidth=2, 
                       label=f'Rolling Mean (window={window_size})')
        axes[1, 0].set_title('Accuracy with Rolling Mean')
        axes[1, 0].set_xlabel('Iteration')
        axes[1, 0].set_ylabel('Accuracy')
        axes[1, 0].legend()
        axes[1, 0].grid(True)
    else:
        axes[1, 0].plot(iterations, accuracies, 'o-')
        axes[1, 0].set_title('Accuracy by Iteration')
        axes[1, 0].set_xlabel('Iteration')
        axes[1, 0].set_ylabel('Accuracy')
        axes[1, 0].grid(True)
    
    # –û–±—â–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
    if len(results['all_true_labels']) > 0:
        overall_cm = confusion_matrix(results['all_true_labels'], results['all_predictions'])
        overall_cm_norm = overall_cm.astype('float') / overall_cm.sum(axis=1)[:, np.newaxis]
        
        sns.heatmap(overall_cm_norm, annot=True, fmt='.2f', cmap='Blues', ax=axes[1, 1])
        axes[1, 1].set_title('Overall Confusion Matrix')
        axes[1, 1].set_xlabel('Predicted')
        axes[1, 1].set_ylabel('True')
    
    plt.tight_layout()
    plt.show()

def analyze_walk_forward_stability(results):
    """–ê–Ω–∞–ª–∏–∑ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Walk-Forward –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
    
    if results is None:
        print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return
    
    iteration_results = results['iteration_results']
    accuracies = [r['accuracy'] for r in iteration_results]
    
    print("=== –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ Walk-Forward ===")
    print(f"Accuracy - Min: {min(accuracies):.4f}, Max: {max(accuracies):.4f}")
    print(f"Accuracy - Range: {max(accuracies) - min(accuracies):.4f}")
    print(f"Accuracy - Std: {np.std(accuracies):.4f}")
    print(f"Accuracy - CV: {np.std(accuracies) / np.mean(accuracies):.4f}")
    
    # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–∞
    if len(accuracies) >= 3:
        from scipy import stats
        slope, intercept, r_value, p_value, std_err = stats.linregress(range(len(accuracies)), accuracies)
        
        print(f"\n–¢—Ä–µ–Ω–¥ —Ç–æ—á–Ω–æ—Å—Ç–∏:")
        print(f"Slope: {slope:.6f} (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π = —É–ª—É—á—à–µ–Ω–∏–µ)")
        print(f"R-squared: {r_value**2:.4f}")
        print(f"P-value: {p_value:.4f}")
        
        if p_value < 0.05:
            if slope > 0:
                print("–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º")
            else:
                print("–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º–æ–µ —É—Ö—É–¥—à–µ–Ω–∏–µ —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º")
        else:
            print("–ù–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º–æ–≥–æ —Ç—Ä–µ–Ω–¥–∞")
    
    # –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ (—Å–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ)
    if len(accuracies) >= 10:
        window_size = max(3, len(accuracies) // 5)
        rolling_std = pd.Series(accuracies).rolling(window=window_size).std()
        
        print(f"\n–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å (—Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ, –æ–∫–Ω–æ={window_size}):")
        print(f"Mean Rolling Std: {rolling_std.mean():.4f}")
        print(f"Max Rolling Std: {rolling_std.max():.4f}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        recent_acc = np.mean(accuracies[-window_size:])
        early_acc = np.mean(accuracies[:window_size])
        degradation = early_acc - recent_acc
        
        print(f"\n–î–µ–≥—Ä–∞–¥–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:")
        print(f"Early accuracy: {early_acc:.4f}")
        print(f"Recent accuracy: {recent_acc:.4f}")
        print(f"Degradation: {degradation:.4f}")
        
        if degradation > 0.05:  # 5% –¥–µ–≥—Ä–∞–¥–∞—Ü–∏—è
            print("‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–∞—è –¥–µ–≥—Ä–∞–¥–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏!")
        elif degradation > 0.02:  # 2% –¥–µ–≥—Ä–∞–¥–∞—Ü–∏—è
            print("‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –£–º–µ—Ä–µ–Ω–Ω–∞—è –¥–µ–≥—Ä–∞–¥–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
        else:
            print("‚úÖ –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å—Ç–∞–±–∏–ª—å–Ω–∞")

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
def example_walk_forward_usage():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Walk-Forward –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
    
    from sklearn.ensemble import RandomForestClassifier
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    np.random.seed(42)
    n_samples, n_features = 2000, 20
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
    X = np.random.randn(n_samples, n_features)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏ –∏ –¥—Ä–µ–π—Ñ–æ–º
    y = np.zeros(n_samples)
    for i in range(n_samples):
        # –î—Ä–µ–π—Ñ: –ø–∞—Ç—Ç–µ—Ä–Ω—ã –º–µ–Ω—è—é—Ç—Å—è —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º
        if i < 500:
            # –†–∞–Ω–Ω–∏–π –ø–µ—Ä–∏–æ–¥: –ø—Ä–æ—Å—Ç—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            y[i] = 1 if X[i, 0] > 0 else 0
        elif i < 1000:
            # –°—Ä–µ–¥–Ω–∏–π –ø–µ—Ä–∏–æ–¥: –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            y[i] = 2 if X[i, 0] > 0.5 else (1 if X[i, 0] > -0.5 else 0)
        else:
            # –ü–æ–∑–¥–Ω–∏–π –ø–µ—Ä–∏–æ–¥: –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å–Ω–æ–≤–∞ –º–µ–Ω—è—é—Ç—Å—è
            y[i] = 1 if X[i, 0] > -0.2 else (2 if X[i, 0] > 0.8 else 0)
    
    print("=== –ü—Ä–∏–º–µ—Ä Walk-Forward –≤–∞–ª–∏–¥–∞—Ü–∏–∏ ===")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model = RandomForestClassifier(n_estimators=50, random_state=42)
    
    # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ Walk-Forward –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    results = walk_forward_validation(
        model, X, y, 
        train_size=500, 
        step_size=100
    )
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    plot_walk_forward_results(results)
    
    # –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    analyze_walk_forward_stability(results)
    
    return results

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–∞ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
# results = example_walk_forward_usage()
```

## –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

**–¢–µ–æ—Ä–∏—è:** –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è - —ç—Ç–æ –ø—Ä–æ—Ü–µ—Å—Å –ø–æ–∏—Å–∫–∞ –Ω–∞–∏–ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –î–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö —ç—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ, —Ç–∞–∫ –∫–∞–∫ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–≥—É—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—é –∏–ª–∏ –Ω–µ–¥–æ–æ–±—É—á–µ–Ω–∏—é.

**–ü–æ—á–µ–º—É –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–∞ –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤:**
- **–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ:** –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–∫–ª–æ–Ω–Ω—ã –∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—é
- **–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å:** –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
- **–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:** –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —É–ª—É—á—à–∞—é—Ç —Ç–æ—á–Ω–æ—Å—Ç—å
- **–†–∏—Å–∫-–¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å:** –ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å—é

**–ú–µ—Ç–æ–¥—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:**
1. **Grid Search:** –ü–æ–ª–Ω—ã–π –ø–µ—Ä–µ–±–æ—Ä –≤—Å–µ—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
2. **Random Search:** –°–ª—É—á–∞–π–Ω—ã–π –ø–æ–∏—Å–∫ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
3. **Bayesian Optimization:** –£–º–Ω—ã–π –ø–æ–∏—Å–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
4. **Optuna:** –°–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

### 1. Grid Search

**–¢–µ–æ—Ä–∏—è:** Grid Search - —ç—Ç–æ –º–µ—Ç–æ–¥ –ø–æ–ª–Ω–æ–≥–æ –ø–µ—Ä–µ–±–æ—Ä–∞, –∫–æ—Ç–æ—Ä—ã–π —Ç–µ—Å—Ç–∏—Ä—É–µ—Ç –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–∑ –∑–∞–¥–∞–Ω–Ω–æ–π —Å–µ—Ç–∫–∏. –•–æ—Ç—è –æ–Ω –º–æ–∂–µ—Ç –±—ã—Ç—å –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ –¥–æ—Ä–æ–≥–∏–º, –æ–Ω –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –∑–∞–¥–∞–Ω–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ.

**–ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã Grid Search:**
1. **–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–µ—Ç–∫–∏:** –ó–∞–¥–∞–µ—Ç—Å—è –¥–∏–∞–ø–∞–∑–æ–Ω –∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞
2. **–ü–æ–ª–Ω—ã–π –ø–µ—Ä–µ–±–æ—Ä:** –¢–µ—Å—Ç–∏—Ä—É—é—Ç—Å—è –≤—Å–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
3. **–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è:** –ö–∞–∂–¥–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é CV
4. **–í—ã–±–æ—Ä –ª—É—á—à–∏—Ö:** –í—ã–±–∏—Ä–∞–µ—Ç—Å—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è —Å –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é

**–ü–ª—é—Å—ã Grid Search:**
- –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ —Å–µ—Ç–∫–µ
- –ü—Ä–æ—Å—Ç–æ–π –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏
- –•–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –Ω–µ–±–æ–ª—å—à–∏–º–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

**–ú–∏–Ω—É—Å—ã Grid Search:**
- –í—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ –¥–æ—Ä–æ–≥–æ–π
- –ù–µ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –±–æ–ª—å—à–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- –ú–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Grid Search:**

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç –∫–æ–¥:**
1. **–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–µ—Ç–∫–∏:** –°–æ–∑–¥–∞–µ—Ç —Å–µ—Ç–∫—É –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è Random Forest
2. **–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è:** –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Time Series CV –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
3. **–ü–æ–∏—Å–∫:** –¢–µ—Å—Ç–∏—Ä—É–µ—Ç –≤—Å–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
4. **–û—Ü–µ–Ω–∫–∞:** –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, Any, Tuple

def optimize_random_forest(X, y, param_grid=None, cv_folds=5, 
                          scoring='accuracy', n_jobs=-1, random_state=42):
    """
    –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è Random Forest —Å –ø–æ–º–æ—â—å—é Grid Search
    
    Args:
        X: –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (samples, features)
        y: –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (samples,)
        param_grid: –°–µ—Ç–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞
        cv_folds: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ–ª–¥–æ–≤ –¥–ª—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏
        scoring: –ú–µ—Ç—Ä–∏–∫–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
        n_jobs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤
        random_state: Seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    
    Returns:
        tuple: (–ª—É—á—à–∞—è –º–æ–¥–µ–ª—å, –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞)
    """
    
    print("=== Grid Search –¥–ª—è Random Forest ===")
    print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {X.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤, {X.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    print(f"–ö–ª–∞—Å—Å—ã: {np.unique(y, return_counts=True)}")
    
    # –°–µ—Ç–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    if param_grid is None:
        param_grid = {
            'n_estimators': [50, 100, 200],
            'max_depth': [5, 10, 15, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4],
            'max_features': ['sqrt', 'log2', None]
        }
    
    print(f"\n–°–µ—Ç–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:")
    for param, values in param_grid.items():
        print(f"{param}: {values}")
    
    # –ü–æ–¥—Å—á–µ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ–º–±–∏–Ω–∞—Ü–∏–π
    total_combinations = 1
    for values in param_grid.values():
        total_combinations *= len(values)
    print(f"–í—Å–µ–≥–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏–π: {total_combinations}")
    print(f"–í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤: {total_combinations * cv_folds}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
    rf = RandomForestClassifier(random_state=random_state, n_jobs=1)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ Time Series CV –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    tscv = TimeSeriesSplit(n_splits=cv_folds)
    
    # Grid Search
    grid_search = GridSearchCV(
        estimator=rf,
        param_grid=param_grid,
        cv=tscv,
        scoring=scoring,
        n_jobs=n_jobs,
        verbose=1,
        return_train_score=True
    )
    
    print(f"\n–ù–∞—á–∞–ª–æ Grid Search...")
    grid_search.fit(X, y)
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    best_model = grid_search.best_estimator_
    best_params = grid_search.best_params_
    best_score = grid_search.best_score_
    
    print(f"\n=== –†–µ–∑—É–ª—å—Ç–∞—Ç—ã Grid Search ===")
    print(f"–õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_score:.4f}")
    print(f"–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
    for param, value in best_params.items():
        print(f"  {param}: {value}")
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    results_df = pd.DataFrame(grid_search.cv_results_)
    
    # –¢–æ–ø-5 –∫–æ–º–±–∏–Ω–∞—Ü–∏–π
    top_results = results_df.nlargest(5, 'mean_test_score')[
        ['params', 'mean_test_score', 'std_test_score']
    ]
    
    print(f"\n–¢–æ–ø-5 –∫–æ–º–±–∏–Ω–∞—Ü–∏–π:")
    for i, (_, row) in enumerate(top_results.iterrows(), 1):
        print(f"{i}. Score: {row['mean_test_score']:.4f} ¬± {row['std_test_score']:.4f}")
        print(f"   Params: {row['params']}")
    
    return best_model, best_params, grid_search

def plot_grid_search_results(grid_search, param_name, figsize=(12, 8)):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Grid Search –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞"""
    
    results_df = pd.DataFrame(grid_search.cv_results_)
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—É
    param_results = results_df[results_df['param_' + param_name].notna()]
    
    if param_results.empty:
        print(f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ {param_name}")
        return
    
    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∑–Ω–∞—á–µ–Ω–∏—è–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞
    param_values = param_results['param_' + param_name].unique()
    mean_scores = []
    std_scores = []
    
    for value in param_values:
        value_results = param_results[param_results['param_' + param_name] == value]
        mean_scores.append(value_results['mean_test_score'].mean())
        std_scores.append(value_results['std_test_score'].mean())
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)
    
    # –ì—Ä–∞—Ñ–∏–∫ —Å—Ä–µ–¥–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    ax1.errorbar(param_values, mean_scores, yerr=std_scores, 
                marker='o', capsize=5, capthick=2)
    ax1.set_title(f'Grid Search Results: {param_name}')
    ax1.set_xlabel(param_name)
    ax1.set_ylabel('Mean Test Score')
    ax1.grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π
    ax2.bar(param_values, std_scores, alpha=0.7)
    ax2.set_title(f'Score Variability: {param_name}')
    ax2.set_xlabel(param_name)
    ax2.set_ylabel('Std Test Score')
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

def analyze_grid_search_stability(grid_search):
    """–ê–Ω–∞–ª–∏–∑ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Grid Search"""
    
    results_df = pd.DataFrame(grid_search.cv_results_)
    
    print("=== –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ Grid Search ===")
    
    # –õ—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    best_score = grid_search.best_score_
    best_std = results_df.loc[grid_search.best_index_, 'std_test_score']
    
    print(f"–õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:")
    print(f"  Score: {best_score:.4f} ¬± {best_std:.4f}")
    print(f"  CV: {best_std / best_score:.4f}")
    
    # –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    all_scores = results_df['mean_test_score']
    all_stds = results_df['std_test_score']
    
    print(f"\n–û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"  Score range: {all_scores.min():.4f} - {all_scores.max():.4f}")
    print(f"  Mean std: {all_stds.mean():.4f}")
    print(f"  Max std: {all_stds.max():.4f}")
    
    # –¢–æ–ø-10 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    top_10 = results_df.nlargest(10, 'mean_test_score')
    
    print(f"\n–¢–æ–ø-10 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
    for i, (_, row) in enumerate(top_10.iterrows(), 1):
        print(f"{i:2d}. {row['mean_test_score']:.4f} ¬± {row['std_test_score']:.4f} - {row['params']}")

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
def example_grid_search_usage():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Grid Search"""
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    np.random.seed(42)
    n_samples, n_features = 1000, 20
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    X = np.random.randn(n_samples, n_features)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    y = np.zeros(n_samples)
    for i in range(n_samples):
        if X[i, 0] > 0.5 and X[i, 1] < -0.3:
            y[i] = 1
        elif X[i, 2] > 1.0 or X[i, 3] < -1.0:
            y[i] = 2
        else:
            y[i] = 0
    
    print("=== –ü—Ä–∏–º–µ—Ä Grid Search ===")
    
    # –ü—Ä–æ—Å—Ç–∞—è —Å–µ—Ç–∫–∞ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
    param_grid = {
        'n_estimators': [50, 100],
        'max_depth': [5, 10],
        'min_samples_split': [2, 5]
    }
    
    # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ Grid Search
    best_model, best_params, grid_search = optimize_random_forest(
        X, y, param_grid=param_grid, cv_folds=3
    )
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    analyze_grid_search_stability(grid_search)
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è (–µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ)
    if 'n_estimators' in best_params:
        plot_grid_search_results(grid_search, 'n_estimators')
    
    return best_model, best_params, grid_search

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–∞ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
# best_model, best_params, grid_search = example_grid_search_usage()
```

### 2. Optuna –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

**–¢–µ–æ—Ä–∏—è:** Optuna - —ç—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Bayesian Optimization –∏ –¥—Ä—É–≥–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.

**–ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã Optuna:**
1. **Bayesian Optimization:** –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –≤—ã–±–æ—Ä–∞ —Å–ª–µ–¥—É—é—â–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
2. **Tree-structured Parzen Estimator (TPE):** –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
3. **Pruning:** –ü—Ä–µ–∫—Ä–∞—â–∞–µ—Ç –Ω–µ–ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–µ –∏—Å–ø—ã—Ç–∞–Ω–∏—è —Ä–∞–Ω—å—à–µ
4. **–ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—è:** –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∏—Å–ø—ã—Ç–∞–Ω–∏–π

**–ü–æ—á–µ–º—É Optuna —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤:**
- **–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å:** –ù–∞—Ö–æ–¥–∏—Ç —Ö–æ—Ä–æ—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –±—ã—Å—Ç—Ä–µ–µ Grid Search
- **–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å:** –†–∞–±–æ—Ç–∞–µ—Ç —Å –±–æ–ª—å—à–∏–º–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- **Pruning:** –≠–∫–æ–Ω–æ–º–∏—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã
- **–ì–∏–±–∫–æ—Å—Ç—å:** –õ–µ–≥–∫–æ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∑–∞–¥–∞—á–∏

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Optuna:**

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç –∫–æ–¥:**
1. **–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞:** –°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
2. **Objective —Ñ—É–Ω–∫—Ü–∏—è:** –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
3. **–ò—Å–ø—ã—Ç–∞–Ω–∏—è:** –í—ã–ø–æ–ª–Ω—è–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∏—Å–ø—ã—Ç–∞–Ω–∏–π —Å —Ä–∞–∑–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
4. **Pruning:** –ü—Ä–µ–∫—Ä–∞—â–∞–µ—Ç –Ω–µ–ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–µ –∏—Å–ø—ã—Ç–∞–Ω–∏—è

```python
import numpy as np
import pandas as pd
import optuna
import xgboost as xgb
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, Any, Tuple
import warnings
warnings.filterwarnings('ignore')

def optimize_xgboost_optuna(X, y, n_trials=100, cv_folds=5, 
                           timeout=None, n_jobs=1, random_state=42):
    """
    –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è XGBoost —Å –ø–æ–º–æ—â—å—é Optuna
    
    Args:
        X: –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (samples, features)
        y: –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (samples,)
        n_trials: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å–ø—ã—Ç–∞–Ω–∏–π
        cv_folds: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ–ª–¥–æ–≤ –¥–ª—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏
        timeout: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        n_jobs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤
        random_state: Seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    
    Returns:
        tuple: (–ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –æ–±—ä–µ–∫—Ç study, –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å)
    """
    
    print("=== Optuna –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è XGBoost ===")
    print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {X.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤, {X.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    print(f"–ö–ª–∞—Å—Å—ã: {np.unique(y, return_counts=True)}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å–ø—ã—Ç–∞–Ω–∏–π: {n_trials}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ Time Series CV –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    tscv = TimeSeriesSplit(n_splits=cv_folds)
    
    def objective(trial):
        """Objective —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è Optuna"""
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        params = {
            'objective': 'multi:softprob',
            'num_class': len(np.unique(y)),
            'max_depth': trial.suggest_int('max_depth', 3, 12),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
            'n_estimators': trial.suggest_int('n_estimators', 50, 500),
            'subsample': trial.suggest_float('subsample', 0.6, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
            'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),
            'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0),
            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
            'gamma': trial.suggest_float('gamma', 0.0, 5.0),
            'random_state': random_state,
            'n_jobs': 1,
            'verbosity': 0
        }
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        model = xgb.XGBClassifier(**params)
        
        # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
        try:
            scores = cross_val_score(
                model, X, y, 
                cv=tscv, 
                scoring='accuracy',
                n_jobs=1
            )
            return scores.mean()
        except Exception as e:
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–ª–æ—Ö–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ
            return 0.0
    
    # –°–æ–∑–¥–∞–Ω–∏–µ study
    study = optuna.create_study(
        direction='maximize',
        sampler=optuna.samplers.TPESampler(seed=random_state),
        pruner=optuna.pruners.MedianPruner(
            n_startup_trials=10,
            n_warmup_steps=5,
            interval_steps=1
        )
    )
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
    print(f"\n–ù–∞—á–∞–ª–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏...")
    study.optimize(
        objective, 
        n_trials=n_trials,
        timeout=timeout,
        n_jobs=n_jobs,
        show_progress_bar=True
    )
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    best_params = study.best_params_
    best_score = study.best_value
    
    print(f"\n=== –†–µ–∑—É–ª—å—Ç–∞—Ç—ã Optuna ===")
    print(f"–õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_score:.4f}")
    print(f"–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
    for param, value in best_params.items():
        print(f"  {param}: {value}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
    best_model = xgb.XGBClassifier(**best_params)
    best_model.fit(X, y)
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print(f"\n=== –ê–Ω–∞–ª–∏–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ ===")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –∏—Å–ø—ã—Ç–∞–Ω–∏–π: {len(study.trials)}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ—Ä–≤–∞–Ω–Ω—ã—Ö –∏—Å–ø—ã—Ç–∞–Ω–∏–π: {len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])}")
    
    return best_params, study, best_model

def plot_optuna_results(study, figsize=(15, 10)):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Optuna"""
    
    fig, axes = plt.subplots(2, 2, figsize=figsize)
    
    # –ì—Ä–∞—Ñ–∏–∫ –∏—Å—Ç–æ—Ä–∏–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    trials = study.trials
    trial_numbers = [t.number for t in trials if t.state == optuna.trial.TrialState.COMPLETE]
    values = [t.value for t in trials if t.state == optuna.trial.TrialState.COMPLETE]
    
    axes[0, 0].plot(trial_numbers, values, 'o-', alpha=0.7)
    axes[0, 0].set_title('Optimization History')
    axes[0, 0].set_xlabel('Trial Number')
    axes[0, 0].set_ylabel('Objective Value')
    axes[0, 0].grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    try:
        importance = optuna.importance.get_param_importances(study)
        params = list(importance.keys())
        importances = list(importance.values())
        
        axes[0, 1].barh(params, importances)
        axes[0, 1].set_title('Parameter Importance')
        axes[0, 1].set_xlabel('Importance')
        axes[0, 1].grid(True, alpha=0.3)
    except Exception as e:
        axes[0, 1].text(0.5, 0.5, f'Importance not available:\n{str(e)}', 
                       ha='center', va='center', transform=axes[0, 1].transAxes)
        axes[0, 1].set_title('Parameter Importance')
    
    # –ì—Ä–∞—Ñ–∏–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∑–Ω–∞—á–µ–Ω–∏–π
    if len(values) > 0:
        axes[1, 0].hist(values, bins=20, alpha=0.7, edgecolor='black')
        axes[1, 0].axvline(np.mean(values), color='red', linestyle='--', 
                          label=f'Mean: {np.mean(values):.4f}')
        axes[1, 0].axvline(np.max(values), color='green', linestyle='--', 
                          label=f'Best: {np.max(values):.4f}')
        axes[1, 0].set_title('Value Distribution')
        axes[1, 0].set_xlabel('Objective Value')
        axes[1, 0].set_ylabel('Frequency')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏
    if len(values) > 1:
        best_values = np.maximum.accumulate(values)
        axes[1, 1].plot(trial_numbers, best_values, 'g-', linewidth=2, label='Best Value')
        axes[1, 1].plot(trial_numbers, values, 'o-', alpha=0.3, label='All Values')
        axes[1, 1].set_title('Convergence')
        axes[1, 1].set_xlabel('Trial Number')
        axes[1, 1].set_ylabel('Best Objective Value')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

def analyze_optuna_study(study):
    """–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Optuna study"""
    
    print("=== –ê–Ω–∞–ª–∏–∑ Optuna Study ===")
    
    # –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    trials = study.trials
    completed_trials = [t for t in trials if t.state == optuna.trial.TrialState.COMPLETE]
    pruned_trials = [t for t in trials if t.state == optuna.trial.TrialState.PRUNED]
    
    print(f"–í—Å–µ–≥–æ –∏—Å–ø—ã—Ç–∞–Ω–∏–π: {len(trials)}")
    print(f"–ó–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö: {len(completed_trials)}")
    print(f"–ü—Ä–µ—Ä–≤–∞–Ω–Ω—ã—Ö: {len(pruned_trials)}")
    
    if completed_trials:
        values = [t.value for t in completed_trials]
        print(f"\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–Ω–∞—á–µ–Ω–∏–π:")
        print(f"  –õ—É—á—à–µ–µ: {max(values):.4f}")
        print(f"  –•—É–¥—à–µ–µ: {min(values):.4f}")
        print(f"  –°—Ä–µ–¥–Ω–µ–µ: {np.mean(values):.4f}")
        print(f"  –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {np.std(values):.4f}")
        
        # –ê–Ω–∞–ª–∏–∑ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        best_values = np.maximum.accumulate(values)
        improvement = best_values[-1] - best_values[0]
        print(f"\n–£–ª—É—á—à–µ–Ω–∏–µ: {improvement:.4f}")
        
        # –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        recent_trials = min(10, len(values))
        recent_values = values[-recent_trials:]
        recent_std = np.std(recent_values)
        print(f"–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å (–ø–æ—Å–ª–µ–¥–Ω–∏–µ {recent_trials} –∏—Å–ø—ã—Ç–∞–Ω–∏–π): {recent_std:.4f}")
    
    # –ê–Ω–∞–ª–∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    if completed_trials:
        print(f"\n–ê–Ω–∞–ª–∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:")
        param_names = list(completed_trials[0].params.keys())
        
        for param_name in param_names:
            param_values = [t.params[param_name] for t in completed_trials]
            if isinstance(param_values[0], (int, float)):
                print(f"  {param_name}: {min(param_values):.4f} - {max(param_values):.4f}")
            else:
                unique_values = list(set(param_values))
                print(f"  {param_name}: {unique_values}")

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
def example_optuna_usage():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Optuna"""
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    np.random.seed(42)
    n_samples, n_features = 1000, 20
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    X = np.random.randn(n_samples, n_features)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    y = np.zeros(n_samples)
    for i in range(n_samples):
        if X[i, 0] > 0.5 and X[i, 1] < -0.3:
            y[i] = 1
        elif X[i, 2] > 1.0 or X[i, 3] < -1.0:
            y[i] = 2
        else:
            y[i] = 0
    
    print("=== –ü—Ä–∏–º–µ—Ä Optuna –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ ===")
    
    # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    best_params, study, best_model = optimize_xgboost_optuna(
        X, y, n_trials=50, cv_folds=3
    )
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    plot_optuna_results(study)
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    analyze_optuna_study(study)
    
    return best_params, study, best_model

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–∞ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
# best_params, study, best_model = example_optuna_usage()
```

## –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω—Å–∞–º–±–ª–µ–≤—ã—Ö –º–µ—Ç–æ–¥–æ–≤

**–¢–µ–æ—Ä–∏—è:** –ê–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã –∫–æ–º–±–∏–Ω–∏—Ä—É—é—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –í —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Å—Ñ–µ—Ä–µ —ç—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ –≤–∞–∂–Ω–æ, —Ç–∞–∫ –∫–∞–∫ —Ä–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –≤—ã—è–≤–ª—è—Ç—å —Ä–∞–∑–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ –¥–∞–Ω–Ω—ã—Ö.

**–ü–æ—á–µ–º—É –∞–Ω—Å–∞–º–±–ª–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤:**
- **–°–Ω–∏–∂–µ–Ω–∏–µ —Ä–∏—Å–∫–∞:** –ö–æ–º–±–∏–Ω–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π —Å–Ω–∏–∂–∞–µ—Ç —Ä–∏—Å–∫ –æ—à–∏–±–æ–∫
- **–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ:** –†–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤—ã—è–≤–ª—è—é—Ç —Ä–∞–∑–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
- **–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å:** –ê–Ω—Å–∞–º–±–ª–∏ –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã, —á–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
- **–†–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å:** –£—Å—Ç–æ–π—á–∏–≤—ã –∫ –≤—ã–±—Ä–æ—Å–∞–º –∏ —à—É–º—É

**–¢–∏–ø—ã –∞–Ω—Å–∞–º–±–ª–µ–π:**
1. **Voting:** –ü—Ä–æ—Å—Ç–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
2. **Stacking:** –ú–µ—Ç–∞-–º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
3. **Blending:** –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
4. **Bagging:** –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –ø–æ–¥–≤—ã–±–æ—Ä–∫–∞—Ö –¥–∞–Ω–Ω—ã—Ö

### 1. Voting Classifier

**–¢–µ–æ—Ä–∏—è:** Voting Classifier - —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ–π –º–µ—Ç–æ–¥ –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ. –ú–æ–∂–µ—Ç –±—ã—Ç—å hard voting (–≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Å–∞–º) –∏–ª–∏ soft voting (–≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –ø–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º).

**–ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã Voting:**
1. **Hard Voting:** –ö–∞–∂–¥–∞—è –º–æ–¥–µ–ª—å –≥–æ–ª–æ—Å—É–µ—Ç –∑–∞ –∫–ª–∞—Å—Å, –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –∫–ª–∞—Å—Å —Å –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ–º –≥–æ–ª–æ—Å–æ–≤
2. **Soft Voting:** –ö–∞–∂–¥–∞—è –º–æ–¥–µ–ª—å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏, –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è —Å—Ä–µ–¥–Ω–µ–µ –∏ –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –∫–ª–∞—Å—Å —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é

**–ü–ª—é—Å—ã Voting:**
- –ü—Ä–æ—Å—Ç–æ—Ç–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏
- –•–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
- –õ–µ–≥–∫–æ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å

**–ú–∏–Ω—É—Å—ã Voting:**
- –ù–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
- –ú–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º –ø—Ä–∏ –ø–ª–æ—Ö–∏—Ö –º–æ–¥–µ–ª—è—Ö

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Voting Classifier:**

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç –∫–æ–¥:**
1. **–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π:** –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω—Å–∞–º–±–ª—è
2. **Voting:** –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —Ç–∏–ø –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è (hard/soft)
3. **–û–±—É—á–µ–Ω–∏–µ:** –û–±—É—á–∞–µ—Ç –≤–µ—Å—å –∞–Ω—Å–∞–º–±–ª—å
4. **–û—Ü–µ–Ω–∫–∞:** –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞–Ω—Å–∞–º–±–ª—è

```python
import numpy as np
import pandas as pd
from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
import lightgbm as lgb
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Any, Tuple

def create_ensemble_model(X, y, voting='soft', test_size=0.2, random_state=42):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏ —Å Voting Classifier
    
    Args:
        X: –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (samples, features)
        y: –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (samples,)
        voting: –¢–∏–ø –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è ('hard' –∏–ª–∏ 'soft')
        test_size: –î–æ–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        random_state: Seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    
    Returns:
        tuple: (–∞–Ω—Å–∞–º–±–ª–µ–≤–∞—è –º–æ–¥–µ–ª—å, –º–µ—Ç—Ä–∏–∫–∏, –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏)
    """
    
    print("=== –°–æ–∑–¥–∞–Ω–∏–µ Voting Ensemble ===")
    print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {X.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤, {X.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    print(f"–ö–ª–∞—Å—Å—ã: {np.unique(y, return_counts=True)}")
    print(f"–¢–∏–ø –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è: {voting}")
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state, stratify=y
    )
    
    print(f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    models = {
        'rf': RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=random_state,
            n_jobs=-1
        ),
        'xgb': xgb.XGBClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            random_state=random_state,
            n_jobs=-1,
            verbosity=0
        ),
        'lgb': lgb.LGBMClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            random_state=random_state,
            n_jobs=-1,
            verbose=-1
        )
    }
    
    print(f"\n–ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏:")
    for name, model in models.items():
        print(f"  {name}: {type(model).__name__}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ Voting Classifier
    ensemble = VotingClassifier(
        estimators=list(models.items()),
        voting=voting,
        n_jobs=-1
    )
    
    # –û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è
    print(f"\n–û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è...")
    ensemble.fit(X_train, y_train)
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∞–Ω—Å–∞–º–±–ª—è
    y_pred_ensemble = ensemble.predict(X_test)
    y_pred_proba_ensemble = ensemble.predict_proba(X_test)
    
    # –û—Ü–µ–Ω–∫–∞ –∞–Ω—Å–∞–º–±–ª—è
    ensemble_accuracy = accuracy_score(y_test, y_pred_ensemble)
    
    print(f"\n=== –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω—Å–∞–º–±–ª—è ===")
    print(f"Ensemble accuracy: {ensemble_accuracy:.4f}")
    
    # –û—Ü–µ–Ω–∫–∞ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    individual_scores = {}
    individual_predictions = {}
    
    print(f"\n=== –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π ===")
    for name, model in models.items():
        # –û–±—É—á–µ–Ω–∏–µ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
        model.fit(X_train, y_train)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None
        
        # –û—Ü–µ–Ω–∫–∞
        accuracy = accuracy_score(y_test, y_pred)
        individual_scores[name] = accuracy
        individual_predictions[name] = y_pred
        
        print(f"{name}: {accuracy:.4f}")
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print(f"\n=== –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ===")
    best_individual = max(individual_scores, key=individual_scores.get)
    best_individual_score = individual_scores[best_individual]
    
    print(f"–õ—É—á—à–∞—è –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å: {best_individual} ({best_individual_score:.4f})")
    print(f"–ê–Ω—Å–∞–º–±–ª—å: {ensemble_accuracy:.4f}")
    print(f"–£–ª—É—á—à–µ–Ω–∏–µ: {ensemble_accuracy - best_individual_score:.4f}")
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    print(f"\n=== Classification Report (Ensemble) ===")
    print(classification_report(y_test, y_pred_ensemble))
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞
    metrics = {
        'ensemble_accuracy': ensemble_accuracy,
        'individual_scores': individual_scores,
        'best_individual': best_individual,
        'improvement': ensemble_accuracy - best_individual_score,
        'confusion_matrix': confusion_matrix(y_test, y_pred_ensemble),
        'predictions': y_pred_ensemble,
        'probabilities': y_pred_proba_ensemble
    }
    
    return ensemble, metrics, models

def plot_ensemble_comparison(metrics, figsize=(12, 8)):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∞–Ω—Å–∞–º–±–ª—è –∏ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)
    
    # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏
    models = list(metrics['individual_scores'].keys()) + ['Ensemble']
    scores = list(metrics['individual_scores'].values()) + [metrics['ensemble_accuracy']]
    colors = ['lightblue'] * len(metrics['individual_scores']) + ['red']
    
    bars = ax1.bar(models, scores, color=colors, alpha=0.7)
    ax1.set_title('Model Performance Comparison')
    ax1.set_ylabel('Accuracy')
    ax1.set_ylim(0, 1)
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
    for bar, score in zip(bars, scores):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{score:.3f}', ha='center', va='bottom')
    
    ax1.grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ —É–ª—É—á—à–µ–Ω–∏—è
    individual_scores = list(metrics['individual_scores'].values())
    ensemble_score = metrics['ensemble_accuracy']
    improvements = [ensemble_score - score for score in individual_scores]
    
    ax2.bar(metrics['individual_scores'].keys(), improvements, 
           color='green', alpha=0.7)
    ax2.set_title('Improvement over Individual Models')
    ax2.set_ylabel('Improvement')
    ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

def analyze_ensemble_diversity(individual_predictions, y_test):
    """–ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –∞–Ω—Å–∞–º–±–ª—è"""
    
    print("=== –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –∞–Ω—Å–∞–º–±–ª—è ===")
    
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏
    model_names = list(individual_predictions.keys())
    n_models = len(model_names)
    
    # –ú–∞—Ç—Ä–∏—Ü–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
    agreement_matrix = np.zeros((n_models, n_models))
    
    for i, model1 in enumerate(model_names):
        for j, model2 in enumerate(model_names):
            if i != j:
                agreement = np.mean(individual_predictions[model1] == individual_predictions[model2])
                agreement_matrix[i, j] = agreement
    
    print(f"–ú–∞—Ç—Ä–∏—Ü–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏:")
    print(f"–°—Ä–µ–¥–Ω—è—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å: {np.mean(agreement_matrix):.4f}")
    print(f"–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å: {np.min(agreement_matrix):.4f}")
    print(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å: {np.max(agreement_matrix):.4f}")
    
    # –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫
    correct_predictions = {}
    for name, pred in individual_predictions.items():
        correct_predictions[name] = (pred == y_test)
    
    # –°–ª—É—á–∞–∏, –≥–¥–µ –≤—Å–µ –º–æ–¥–µ–ª–∏ –æ—à–∏–±–ª–∏—Å—å
    all_wrong = np.all([~correct_predictions[name] for name in model_names], axis=0)
    all_wrong_count = np.sum(all_wrong)
    
    # –°–ª—É—á–∞–∏, –≥–¥–µ –≤—Å–µ –º–æ–¥–µ–ª–∏ –±—ã–ª–∏ –ø—Ä–∞–≤—ã
    all_correct = np.all([correct_predictions[name] for name in model_names], axis=0)
    all_correct_count = np.sum(all_correct)
    
    print(f"\n–ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫:")
    print(f"–í—Å–µ –º–æ–¥–µ–ª–∏ –ø—Ä–∞–≤—ã: {all_correct_count} ({all_correct_count/len(y_test)*100:.1f}%)")
    print(f"–í—Å–µ –º–æ–¥–µ–ª–∏ –æ—à–∏–±–ª–∏—Å—å: {all_wrong_count} ({all_wrong_count/len(y_test)*100:.1f}%)")
    
    # –°–ª—É—á–∞–∏, –≥–¥–µ –º–Ω–µ–Ω–∏—è —Ä–∞–∑–¥–µ–ª–∏–ª–∏—Å—å
    mixed_cases = len(y_test) - all_correct_count - all_wrong_count
    print(f"–°–º–µ—à–∞–Ω–Ω—ã–µ —Å–ª—É—á–∞–∏: {mixed_cases} ({mixed_cases/len(y_test)*100:.1f}%)")

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
def example_voting_ensemble_usage():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Voting Ensemble"""
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    np.random.seed(42)
    n_samples, n_features = 1000, 20
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    X = np.random.randn(n_samples, n_features)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    y = np.zeros(n_samples)
    for i in range(n_samples):
        if X[i, 0] > 0.5 and X[i, 1] < -0.3:
            y[i] = 1
        elif X[i, 2] > 1.0 or X[i, 3] < -1.0:
            y[i] = 2
        else:
            y[i] = 0
    
    print("=== –ü—Ä–∏–º–µ—Ä Voting Ensemble ===")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è
    ensemble, metrics, models = create_ensemble_model(X, y, voting='soft')
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    plot_ensemble_comparison(metrics)
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
    individual_predictions = {}
    for name, model in models.items():
        individual_predictions[name] = model.predict(X)
    
    analyze_ensemble_diversity(individual_predictions, y)
    
    return ensemble, metrics, models

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–∞ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
# ensemble, metrics, models = example_voting_ensemble_usage()
```

### 2. Stacking

**–¢–µ–æ—Ä–∏—è:** Stacking (Stacked Generalization) - —ç—Ç–æ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –º–µ—Ç–æ–¥ –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–∞-–º–æ–¥–µ–ª—å –¥–ª—è –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ú–µ—Ç–∞-–º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –µ–π –Ω–∞—Ö–æ–¥–∏—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã –∏—Ö –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è.

**–ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã Stacking:**
1. **–ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏:** –û–±—É—á–∞—é—Ç—Å—è –Ω–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
2. **–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è:** –ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–µ–ª–∞—é—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
3. **–ú–µ—Ç–∞-–º–æ–¥–µ–ª—å:** –û–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
4. **–§–∏–Ω–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ:** –ú–µ—Ç–∞-–º–æ–¥–µ–ª—å –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π

**–ü–ª—é—Å—ã Stacking:**
- –ë–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π
- –ú–µ—Ç–∞-–º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –≤—ã—É—á–∏—Ç—å –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
- –ß–∞—Å—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —á–µ–º Voting

**–ú–∏–Ω—É—Å—ã Stacking:**
- –ë–æ–ª–µ–µ —Å–ª–æ–∂–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
- –¢—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤
- –ú–æ–∂–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–∏—Ç—å—Å—è –ø—Ä–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–µ

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Stacking:**

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç –∫–æ–¥:**
1. **–ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏:** –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–∞–±–æ—Ä —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
2. **–ú–µ—Ç–∞-–º–æ–¥–µ–ª—å:** –í—ã–±–∏—Ä–∞–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
3. **–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è:** –ò—Å–ø–æ–ª—å–∑—É–µ—Ç CV –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
4. **–û–±—É—á–µ–Ω–∏–µ:** –û–±—É—á–∞–µ—Ç –≤–µ—Å—å —Å—Ç–µ–∫ –º–æ–¥–µ–ª–µ–π

```python
import numpy as np
import pandas as pd
from sklearn.ensemble import StackingClassifier, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
import xgboost as xgb
import lightgbm as lgb
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Any, Tuple

def create_stacking_model(X, y, test_size=0.2, cv_folds=5, random_state=42):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ Stacking –º–æ–¥–µ–ª–∏
    
    Args:
        X: –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (samples, features)
        y: –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (samples,)
        test_size: –î–æ–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        cv_folds: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ–ª–¥–æ–≤ –¥–ª—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏
        random_state: Seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    
    Returns:
        tuple: (stacking –º–æ–¥–µ–ª—å, –º–µ—Ç—Ä–∏–∫–∏, –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏)
    """
    
    print("=== –°–æ–∑–¥–∞–Ω–∏–µ Stacking –º–æ–¥–µ–ª–∏ ===")
    print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {X.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤, {X.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    print(f"–ö–ª–∞—Å—Å—ã: {np.unique(y, return_counts=True)}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ–ª–¥–æ–≤ CV: {cv_folds}")
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state, stratify=y
    )
    
    print(f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
    base_models = [
        ('rf', RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=random_state,
            n_jobs=-1
        )),
        ('xgb', xgb.XGBClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            random_state=random_state,
            n_jobs=-1,
            verbosity=0
        )),
        ('lgb', lgb.LGBMClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            random_state=random_state,
            n_jobs=-1,
            verbose=-1
        )),
        ('svm', SVC(
            probability=True,
            random_state=random_state
        )),
        ('mlp', MLPClassifier(
            hidden_layer_sizes=(100, 50),
            max_iter=500,
            random_state=random_state
        ))
    ]
    
    print(f"\n–ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏:")
    for name, model in base_models:
        print(f"  {name}: {type(model).__name__}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏
    meta_models = {
        'logistic': LogisticRegression(random_state=random_state, max_iter=1000),
        'rf_meta': RandomForestClassifier(n_estimators=50, random_state=random_state),
        'xgb_meta': xgb.XGBClassifier(n_estimators=50, random_state=random_state, verbosity=0)
    }
    
    print(f"\n–ú–µ—Ç–∞-–º–æ–¥–µ–ª–∏:")
    for name, model in meta_models.items():
        print(f"  {name}: {type(model).__name__}")
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –º–µ—Ç–∞-–º–æ–¥–µ–ª–µ–π
    best_meta_model = None
    best_score = 0
    meta_scores = {}
    
    print(f"\n–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–∞-–º–æ–¥–µ–ª–µ–π...")
    
    for meta_name, meta_model in meta_models.items():
        # –°–æ–∑–¥–∞–Ω–∏–µ Stacking –º–æ–¥–µ–ª–∏
        stacking_model = StackingClassifier(
            estimators=base_models,
            final_estimator=meta_model,
            cv=cv_folds,
            n_jobs=-1
        )
        
        # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
        scores = cross_val_score(
            stacking_model, X_train, y_train, 
            cv=cv_folds, scoring='accuracy'
        )
        
        mean_score = scores.mean()
        meta_scores[meta_name] = mean_score
        
        print(f"  {meta_name}: {mean_score:.4f} ¬± {scores.std():.4f}")
        
        if mean_score > best_score:
            best_score = mean_score
            best_meta_model = meta_model
    
    print(f"\n–õ—É—á—à–∞—è –º–µ—Ç–∞-–º–æ–¥–µ–ª—å: {max(meta_scores, key=meta_scores.get)}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π Stacking –º–æ–¥–µ–ª–∏
    final_stacking_model = StackingClassifier(
        estimators=base_models,
        final_estimator=best_meta_model,
        cv=cv_folds,
        n_jobs=-1
    )
    
    # –û–±—É—á–µ–Ω–∏–µ
    print(f"\n–û–±—É—á–µ–Ω–∏–µ Stacking –º–æ–¥–µ–ª–∏...")
    final_stacking_model.fit(X_train, y_train)
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    y_pred_stacking = final_stacking_model.predict(X_test)
    y_pred_proba_stacking = final_stacking_model.predict_proba(X_test)
    
    # –û—Ü–µ–Ω–∫–∞ Stacking –º–æ–¥–µ–ª–∏
    stacking_accuracy = accuracy_score(y_test, y_pred_stacking)
    
    print(f"\n=== –†–µ–∑—É–ª—å—Ç–∞—Ç—ã Stacking –º–æ–¥–µ–ª–∏ ===")
    print(f"Stacking accuracy: {stacking_accuracy:.4f}")
    
    # –û—Ü–µ–Ω–∫–∞ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
    base_scores = {}
    base_predictions = {}
    
    print(f"\n=== –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π ===")
    for name, model in base_models:
        # –û–±—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
        model.fit(X_train, y_train)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None
        
        # –û—Ü–µ–Ω–∫–∞
        accuracy = accuracy_score(y_test, y_pred)
        base_scores[name] = accuracy
        base_predictions[name] = y_pred
        
        print(f"{name}: {accuracy:.4f}")
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print(f"\n=== –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ===")
    best_base = max(base_scores, key=base_scores.get)
    best_base_score = base_scores[best_base]
    
    print(f"–õ—É—á—à–∞—è –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: {best_base} ({best_base_score:.4f})")
    print(f"Stacking –º–æ–¥–µ–ª—å: {stacking_accuracy:.4f}")
    print(f"–£–ª—É—á—à–µ–Ω–∏–µ: {stacking_accuracy - best_base_score:.4f}")
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    print(f"\n=== Classification Report (Stacking) ===")
    print(classification_report(y_test, y_pred_stacking))
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞
    metrics = {
        'stacking_accuracy': stacking_accuracy,
        'base_scores': base_scores,
        'meta_scores': meta_scores,
        'best_base': best_base,
        'best_meta': max(meta_scores, key=meta_scores.get),
        'improvement': stacking_accuracy - best_base_score,
        'confusion_matrix': confusion_matrix(y_test, y_pred_stacking),
        'predictions': y_pred_stacking,
        'probabilities': y_pred_proba_stacking
    }
    
    return final_stacking_model, metrics, base_models

def plot_stacking_results(metrics, figsize=(15, 10)):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Stacking"""
    
    fig, axes = plt.subplots(2, 2, figsize=figsize)
    
    # –ì—Ä–∞—Ñ–∏–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ Stacking
    models = list(metrics['base_scores'].keys()) + ['Stacking']
    scores = list(metrics['base_scores'].values()) + [metrics['stacking_accuracy']]
    colors = ['lightblue'] * len(metrics['base_scores']) + ['red']
    
    bars = axes[0, 0].bar(models, scores, color=colors, alpha=0.7)
    axes[0, 0].set_title('Base Models vs Stacking')
    axes[0, 0].set_ylabel('Accuracy')
    axes[0, 0].set_ylim(0, 1)
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
    for bar, score in zip(bars, scores):
        height = bar.get_height()
        axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + 0.01,
                       f'{score:.3f}', ha='center', va='bottom')
    
    axes[0, 0].grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ –º–µ—Ç–∞-–º–æ–¥–µ–ª–µ–π
    meta_models = list(metrics['meta_scores'].keys())
    meta_scores = list(metrics['meta_scores'].values())
    
    bars = axes[0, 1].bar(meta_models, meta_scores, color='green', alpha=0.7)
    axes[0, 1].set_title('Meta-Model Performance')
    axes[0, 1].set_ylabel('Accuracy')
    axes[0, 1].set_ylim(0, 1)
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
    for bar, score in zip(bars, meta_scores):
        height = bar.get_height()
        axes[0, 1].text(bar.get_x() + bar.get_width()/2., height + 0.01,
                       f'{score:.3f}', ha='center', va='bottom')
    
    axes[0, 1].grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ —É–ª—É—á—à–µ–Ω–∏—è
    base_scores = list(metrics['base_scores'].values())
    stacking_score = metrics['stacking_accuracy']
    improvements = [stacking_score - score for score in base_scores]
    
    axes[1, 0].bar(metrics['base_scores'].keys(), improvements, 
                  color='orange', alpha=0.7)
    axes[1, 0].set_title('Stacking Improvement over Base Models')
    axes[1, 0].set_ylabel('Improvement')
    axes[1, 0].axhline(y=0, color='black', linestyle='-', alpha=0.3)
    axes[1, 0].grid(True, alpha=0.3)
    
    # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
    cm = metrics['confusion_matrix']
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 1])
    axes[1, 1].set_title('Confusion Matrix (Stacking)')
    axes[1, 1].set_xlabel('Predicted')
    axes[1, 1].set_ylabel('True')
    
    plt.tight_layout()
    plt.show()

def analyze_stacking_contribution(stacking_model, X_test, y_test):
    """–ê–Ω–∞–ª–∏–∑ –≤–∫–ª–∞–¥–∞ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ Stacking"""
    
    print("=== –ê–Ω–∞–ª–∏–∑ –≤–∫–ª–∞–¥–∞ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π ===")
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
    base_predictions = stacking_model.transform(X_test)
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏
    if hasattr(stacking_model.final_estimator_, 'coef_'):
        # –î–ª—è –ª–∏–Ω–µ–π–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        weights = stacking_model.final_estimator_.coef_[0]
        print(f"–í–µ—Å–∞ –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏: {weights}")
        
        # –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
        base_names = [name for name, _ in stacking_model.estimators]
        for name, weight in zip(base_names, weights):
            print(f"  {name}: {weight:.4f}")
    
    # –ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –º–µ–∂–¥—É –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
    base_predictions_df = pd.DataFrame(
        base_predictions, 
        columns=[name for name, _ in stacking_model.estimators]
    )
    
    correlation_matrix = base_predictions_df.corr()
    
    print(f"\n–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π:")
    print(correlation_matrix.round(3))
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
    mean_correlation = correlation_matrix.values[np.triu_indices_from(correlation_matrix.values, k=1)].mean()
    print(f"\n–°—Ä–µ–¥–Ω—è—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è: {mean_correlation:.4f}")
    
    if mean_correlation < 0.5:
        print("‚úÖ –•–æ—Ä–æ—à–µ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π")
    elif mean_correlation < 0.7:
        print("‚ö†Ô∏è  –£–º–µ—Ä–µ–Ω–Ω–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π")
    else:
        print("‚ùå –ù–∏–∑–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π")

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
def example_stacking_usage():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Stacking"""
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    np.random.seed(42)
    n_samples, n_features = 1000, 20
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    X = np.random.randn(n_samples, n_features)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    y = np.zeros(n_samples)
    for i in range(n_samples):
        if X[i, 0] > 0.5 and X[i, 1] < -0.3:
            y[i] = 1
        elif X[i, 2] > 1.0 or X[i, 3] < -1.0:
            y[i] = 2
        else:
            y[i] = 0
    
    print("=== –ü—Ä–∏–º–µ—Ä Stacking ===")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ Stacking –º–æ–¥–µ–ª–∏
    stacking_model, metrics, base_models = create_stacking_model(X, y)
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    plot_stacking_results(metrics)
    
    # –ê–Ω–∞–ª–∏–∑ –≤–∫–ª–∞–¥–∞ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
    analyze_stacking_contribution(stacking_model, X, y)
    
    return stacking_model, metrics, base_models

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–∞ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
# stacking_model, metrics, base_models = example_stacking_usage()
```

## –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

**–¢–µ–æ—Ä–∏—è:** –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Ç—Ä–µ–±—É–µ—Ç –æ—Å–æ–±–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞, —Ç–∞–∫ –∫–∞–∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –º–æ–≥—É—Ç –Ω–µ –æ—Ç—Ä–∞–∂–∞—Ç—å —Ä–µ–∞–ª—å–Ω—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ç–æ—Ä–≥–æ–≤–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏.

**–ü–æ—á–µ–º—É —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã:**
- **–¢–æ—á–Ω–æ—Å—Ç—å –Ω–µ —Ä–∞–≤–Ω–∞ –ø—Ä–∏–±—ã–ª—å–Ω–æ—Å—Ç–∏:** –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–∂–µ—Ç –Ω–µ –æ–∑–Ω–∞—á–∞—Ç—å –ø—Ä–∏–±—ã–ª—å–Ω–æ—Å—Ç—å
- **–ö–ª–∞—Å—Å–æ–≤—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å:** –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —á–∞—Å—Ç–æ –∏–º–µ—é—Ç –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
- **–í—Ä–µ–º–µ–Ω–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å:** –í–∞–∂–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
- **–†–∏—Å–∫-–¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å:** –ù—É–∂–Ω–æ —É—á–∏—Ç—ã–≤–∞—Ç—å —Ä–∏—Å–∫, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å

**–¢–∏–ø—ã –º–µ—Ç—Ä–∏–∫:**
1. **–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:** Accuracy, Precision, Recall, F1
2. **–¢–æ—Ä–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏:** Sharpe Ratio, Maximum Drawdown, Win Rate
3. **–í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:** –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –≤–æ –≤—Ä–µ–º–µ–Ω–∏
4. **–†–∏—Å–∫–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏:** VaR, CVaR, Volatility

### 1. –ú–µ—Ç—Ä–∏–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏

**–¢–µ–æ—Ä–∏—è:** –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏–∑–º–µ—Ä—è—é—Ç –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –æ–±—Ä–∞–∑—Ü–æ–≤ –ø–æ –∫–ª–∞—Å—Å–∞–º.

**–û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:**
- **Accuracy:** –î–æ–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
- **Precision:** –î–æ–ª—è –∏—Å—Ç–∏–Ω–Ω–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö —Å—Ä–µ–¥–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö
- **Recall:** –î–æ–ª—è –∏—Å—Ç–∏–Ω–Ω–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö
- **F1-Score:** –ì–∞—Ä–º–æ–Ω–∏—á–µ—Å–∫–æ–µ —Å—Ä–µ–¥–Ω–µ–µ Precision –∏ Recall

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:**

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç –∫–æ–¥:**
1. **–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è:** –ü–æ–ª—É—á–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
2. **–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫:** –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
3. **–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è:** –°–æ–∑–¥–∞–µ—Ç –≥—Ä–∞—Ñ–∏–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
4. **–ê–Ω–∞–ª–∏–∑:** –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

```python
import numpy as np
import pandas as pd
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                           f1_score, confusion_matrix, classification_report,
                           roc_auc_score, roc_curve, precision_recall_curve)
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, Any, Tuple, List

def evaluate_model(model, X_test, y_test, model_name="Model"):
    """
    –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
    
    Args:
        model: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        X_test: –¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        y_test: –¢–µ—Å—Ç–æ–≤—ã–µ –º–µ—Ç–∫–∏
        model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –æ—Ç—á–µ—Ç–æ–≤
    
    Returns:
        dict: –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    """
    
    print(f"=== –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏: {model_name} ===")
    print(f"–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: {len(y_test)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"–ö–ª–∞—Å—Å—ã: {np.unique(y_test, return_counts=True)}")
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º
    precision_per_class = precision_score(y_test, y_pred, average=None, zero_division=0)
    recall_per_class = recall_score(y_test, y_pred, average=None, zero_division=0)
    f1_per_class = f1_score(y_test, y_pred, average=None, zero_division=0)
    
    # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
    cm = confusion_matrix(y_test, y_pred)
    
    # ROC AUC (–¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏)
    roc_auc = None
    if len(np.unique(y_test)) == 2 and y_pred_proba is not None:
        roc_auc = roc_auc_score(y_test, y_pred_proba[:, 1])
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results = {
        'model_name': model_name,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'precision_per_class': precision_per_class,
        'recall_per_class': recall_per_class,
        'f1_per_class': f1_per_class,
        'confusion_matrix': cm,
        'roc_auc': roc_auc,
        'predictions': y_pred,
        'probabilities': y_pred_proba
    }
    
    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print(f"\n=== –†–µ–∑—É–ª—å—Ç–∞—Ç—ã ===")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1-Score: {f1:.4f}")
    
    if roc_auc is not None:
        print(f"ROC AUC: {roc_auc:.4f}")
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    print(f"\n=== Classification Report ===")
    print(classification_report(y_test, y_pred))
    
    return results

def plot_classification_metrics(results, figsize=(15, 10)):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    
    fig, axes = plt.subplots(2, 2, figsize=figsize)
    
    # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
    cm = results['confusion_matrix']
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0])
    axes[0, 0].set_title('Confusion Matrix')
    axes[0, 0].set_xlabel('Predicted')
    axes[0, 0].set_ylabel('True')
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º
    classes = range(len(results['precision_per_class']))
    x = np.arange(len(classes))
    width = 0.25
    
    axes[0, 1].bar(x - width, results['precision_per_class'], width, label='Precision', alpha=0.8)
    axes[0, 1].bar(x, results['recall_per_class'], width, label='Recall', alpha=0.8)
    axes[0, 1].bar(x + width, results['f1_per_class'], width, label='F1-Score', alpha=0.8)
    
    axes[0, 1].set_title('Metrics per Class')
    axes[0, 1].set_xlabel('Class')
    axes[0, 1].set_ylabel('Score')
    axes[0, 1].set_xticks(x)
    axes[0, 1].set_xticklabels(classes)
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # –û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
    values = [results['accuracy'], results['precision'], results['recall'], results['f1']]
    
    bars = axes[1, 0].bar(metrics, values, color=['skyblue', 'lightgreen', 'lightcoral', 'gold'], alpha=0.8)
    axes[1, 0].set_title('Overall Metrics')
    axes[1, 0].set_ylabel('Score')
    axes[1, 0].set_ylim(0, 1)
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
    for bar, value in zip(bars, values):
        height = bar.get_height()
        axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + 0.01,
                       f'{value:.3f}', ha='center', va='bottom')
    
    axes[1, 0].grid(True, alpha=0.3)
    
    # ROC –∫—Ä–∏–≤–∞—è (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞)
    if results['roc_auc'] is not None:
        fpr, tpr, _ = roc_curve(results['y_test'], results['probabilities'][:, 1])
        axes[1, 1].plot(fpr, tpr, color='darkorange', lw=2, 
                       label=f'ROC curve (AUC = {results["roc_auc"]:.2f})')
        axes[1, 1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
        axes[1, 1].set_xlim([0.0, 1.0])
        axes[1, 1].set_ylim([0.0, 1.05])
        axes[1, 1].set_xlabel('False Positive Rate')
        axes[1, 1].set_ylabel('True Positive Rate')
        axes[1, 1].set_title('ROC Curve')
        axes[1, 1].legend(loc="lower right")
        axes[1, 1].grid(True, alpha=0.3)
    else:
        axes[1, 1].text(0.5, 0.5, 'ROC Curve not available\nfor multiclass problem', 
                       ha='center', va='center', transform=axes[1, 1].transAxes)
        axes[1, 1].set_title('ROC Curve')
    
    plt.tight_layout()
    plt.show()

def analyze_class_balance(y_test, y_pred):
    """–ê–Ω–∞–ª–∏–∑ –±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤"""
    
    print("=== –ê–Ω–∞–ª–∏–∑ –±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤ ===")
    
    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
    unique_classes, counts = np.unique(y_test, return_counts=True)
    total_samples = len(y_test)
    
    print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ:")
    for class_label, count in zip(unique_classes, counts):
        percentage = count / total_samples * 100
        print(f"  –ö–ª–∞—Å—Å {class_label}: {count} ({percentage:.1f}%)")
    
    # –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    pred_unique, pred_counts = np.unique(y_pred, return_counts=True)
    
    print(f"\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
    for class_label, count in zip(pred_unique, pred_counts):
        percentage = count / total_samples * 100
        print(f"  –ö–ª–∞—Å—Å {class_label}: {count} ({percentage:.1f}%)")
    
    # –ê–Ω–∞–ª–∏–∑ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞
    max_count = max(counts)
    min_count = min(counts)
    imbalance_ratio = max_count / min_count
    
    print(f"\n–ê–Ω–∞–ª–∏–∑ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞:")
    print(f"  –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {imbalance_ratio:.2f}:1")
    
    if imbalance_ratio > 10:
        print("  ‚ö†Ô∏è  –°–∏–ª—å–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤")
    elif imbalance_ratio > 3:
        print("  ‚ö†Ô∏è  –£–º–µ—Ä–µ–Ω–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤")
    else:
        print("  ‚úÖ –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã")

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
def example_classification_metrics_usage():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.datasets import make_classification
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    X, y = make_classification(
        n_samples=1000, n_features=20, n_classes=3, 
        n_informative=15, n_redundant=5, random_state=42
    )
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    
    print("=== –ü—Ä–∏–º–µ—Ä –º–µ—Ç—Ä–∏–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ ===")
    
    # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
    results = evaluate_model(model, X_test, y_test, "Random Forest")
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    plot_classification_metrics(results)
    
    # –ê–Ω–∞–ª–∏–∑ –±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤
    analyze_class_balance(y_test, results['predictions'])
    
    return results

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–∞ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
# results = example_classification_metrics_usage()
```

### 2. –¢–æ—Ä–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏

**–¢–µ–æ—Ä–∏—è:** –¢–æ—Ä–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏–∑–º–µ—Ä—è—é—Ç —Ä–µ–∞–ª—å–Ω—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ç–æ—Ä–≥–æ–≤–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏, —É—á–∏—Ç—ã–≤–∞—è –Ω–µ —Ç–æ–ª—å–∫–æ —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π, –Ω–æ –∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.

**–û—Å–Ω–æ–≤–Ω—ã–µ —Ç–æ—Ä–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏:**
- **Sharpe Ratio:** –û—Ç–Ω–æ—à–µ–Ω–∏–µ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –∫ —Ä–∏—Å–∫—É
- **Maximum Drawdown:** –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø–æ—Ç–µ—Ä—è –æ—Ç –ø–∏–∫–∞
- **Win Rate:** –î–æ–ª—è –ø—Ä–∏–±—ã–ª—å–Ω—ã—Ö —Å–¥–µ–ª–æ–∫
- **Profit Factor:** –û—Ç–Ω–æ—à–µ–Ω–∏–µ –ø—Ä–∏–±—ã–ª–∏ –∫ —É–±—ã—Ç–∫–∞–º
- **Calmar Ratio:** –û—Ç–Ω–æ—à–µ–Ω–∏–µ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –∫ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ—Å–∞–¥–∫–µ

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ—Ä–≥–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫:**

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç –∫–æ–¥:**
1. **–†–∞—Å—á–µ—Ç –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏:** –í—ã—á–∏—Å–ª—è–µ—Ç –ø—Ä–∏–±—ã–ª—å–Ω–æ—Å—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
2. **–†–∏—Å–∫–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏:** –û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–∏—Å–∫ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
3. **–¢–æ—Ä–≥–æ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏:** –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Ç–æ—Ä–≥–æ–≤–ª–∏
4. **–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è:** –°–æ–∑–¥–∞–µ—Ç –≥—Ä–∞—Ñ–∏–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, Any, Tuple, List
from sklearn.metrics import accuracy_score

def calculate_trading_metrics(y_true, y_pred, returns, transaction_costs=0.001):
    """
    –†–∞—Å—á–µ—Ç —Ç–æ—Ä–≥–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫ –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
    
    Args:
        y_true: –ò—Å—Ç–∏–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã (0: –ø—Ä–æ–¥–∞–∂–∞, 1: —É–¥–µ—Ä–∂–∞–Ω–∏–µ, 2: –ø–æ–∫—É–ø–∫–∞)
        y_pred: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã
        returns: –î–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –∞–∫—Ç–∏–≤–æ–≤
        transaction_costs: –¢—Ä–∞–Ω–∑–∞–∫—Ü–∏–æ–Ω–Ω—ã–µ –∏–∑–¥–µ—Ä–∂–∫–∏ (–¥–æ–ª—è –æ—Ç —Å–¥–µ–ª–∫–∏)
    
    Returns:
        dict: –°–ª–æ–≤–∞—Ä—å —Å —Ç–æ—Ä–≥–æ–≤—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
    """
    
    print("=== –†–∞—Å—á–µ—Ç —Ç–æ—Ä–≥–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫ ===")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–¥–µ–ª–æ–∫: {len(y_true)}")
    print(f"–¢—Ä–∞–Ω–∑–∞–∫—Ü–∏–æ–Ω–Ω—ã–µ –∏–∑–¥–µ—Ä–∂–∫–∏: {transaction_costs*100:.2f}%")
    
    # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    accuracy = accuracy_score(y_true, y_pred)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤
    # 0: –ø—Ä–æ–¥–∞–∂–∞ (-1), 1: —É–¥–µ—Ä–∂–∞–Ω–∏–µ (0), 2: –ø–æ–∫—É–ø–∫–∞ (1)
    signal_mapping = {0: -1, 1: 0, 2: 1}
    y_true_signals = np.array([signal_mapping[label] for label in y_true])
    y_pred_signals = np.array([signal_mapping[label] for label in y_pred])
    
    # –†–∞—Å—á–µ—Ç –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
    strategy_returns = returns * y_pred_signals
    
    # –£—á–µ—Ç —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–æ–Ω–Ω—ã—Ö –∏–∑–¥–µ—Ä–∂–µ–∫
    position_changes = np.diff(y_pred_signals, prepend=y_pred_signals[0])
    transaction_costs_total = np.abs(position_changes) * transaction_costs
    strategy_returns_net = strategy_returns - transaction_costs_total
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–æ—Ä–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    total_return = np.sum(strategy_returns_net)
    annualized_return = np.mean(strategy_returns_net) * 252
    
    # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
    volatility = np.std(strategy_returns_net) * np.sqrt(252)
    
    # Sharpe Ratio
    if volatility > 0:
        sharpe_ratio = annualized_return / volatility
    else:
        sharpe_ratio = 0
    
    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ—Å–∞–¥–∫–∞
    cumulative_returns = np.cumprod(1 + strategy_returns_net)
    running_max = np.maximum.accumulate(cumulative_returns)
    drawdown = (cumulative_returns - running_max) / running_max
    max_drawdown = np.min(drawdown)
    
    # Calmar Ratio
    if abs(max_drawdown) > 0:
        calmar_ratio = annualized_return / abs(max_drawdown)
    else:
        calmar_ratio = np.inf
    
    # Win Rate
    profitable_trades = strategy_returns_net > 0
    win_rate = np.mean(profitable_trades) if len(profitable_trades) > 0 else 0
    
    # Profit Factor
    gross_profit = np.sum(strategy_returns_net[strategy_returns_net > 0])
    gross_loss = abs(np.sum(strategy_returns_net[strategy_returns_net < 0]))
    profit_factor = gross_profit / gross_loss if gross_loss > 0 else np.inf
    
    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–¥–µ–ª–æ–∫
    num_trades = np.sum(np.abs(position_changes))
    
    # –°—Ä–µ–¥–Ω—è—è –ø—Ä–∏–±—ã–ª—å/—É–±—ã—Ç–æ–∫
    avg_profit = np.mean(strategy_returns_net[strategy_returns_net > 0]) if np.any(strategy_returns_net > 0) else 0
    avg_loss = np.mean(strategy_returns_net[strategy_returns_net < 0]) if np.any(strategy_returns_net < 0) else 0
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    metrics = {
        'accuracy': accuracy,
        'total_return': total_return,
        'annualized_return': annualized_return,
        'volatility': volatility,
        'sharpe_ratio': sharpe_ratio,
        'max_drawdown': max_drawdown,
        'calmar_ratio': calmar_ratio,
        'win_rate': win_rate,
        'profit_factor': profit_factor,
        'num_trades': num_trades,
        'avg_profit': avg_profit,
        'avg_loss': avg_loss,
        'strategy_returns': strategy_returns_net,
        'cumulative_returns': cumulative_returns,
        'drawdown': drawdown
    }
    
    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print(f"\n=== –¢–æ—Ä–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ ===")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Total Return: {total_return:.4f}")
    print(f"Annualized Return: {annualized_return:.4f}")
    print(f"Volatility: {volatility:.4f}")
    print(f"Sharpe Ratio: {sharpe_ratio:.4f}")
    print(f"Max Drawdown: {max_drawdown:.4f}")
    print(f"Calmar Ratio: {calmar_ratio:.4f}")
    print(f"Win Rate: {win_rate:.4f}")
    print(f"Profit Factor: {profit_factor:.4f}")
    print(f"Number of Trades: {num_trades}")
    
    return metrics

def plot_trading_metrics(metrics, figsize=(15, 12)):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ—Ä–≥–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫"""
    
    fig, axes = plt.subplots(2, 2, figsize=figsize)
    
    # –ö—É–º—É–ª—è—Ç–∏–≤–Ω–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å
    cumulative_returns = metrics['cumulative_returns']
    axes[0, 0].plot(cumulative_returns, label='Strategy', linewidth=2)
    axes[0, 0].axhline(y=1, color='black', linestyle='--', alpha=0.5, label='Break-even')
    axes[0, 0].set_title('Cumulative Returns')
    axes[0, 0].set_xlabel('Time')
    axes[0, 0].set_ylabel('Cumulative Return')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # –ü—Ä–æ—Å–∞–¥–∫–∞
    drawdown = metrics['drawdown']
    axes[0, 1].fill_between(range(len(drawdown)), drawdown, 0, 
                           color='red', alpha=0.3, label='Drawdown')
    axes[0, 1].plot(drawdown, color='red', linewidth=1)
    axes[0, 1].set_title('Drawdown')
    axes[0, 1].set_xlabel('Time')
    axes[0, 1].set_ylabel('Drawdown')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏
    strategy_returns = metrics['strategy_returns']
    axes[1, 0].hist(strategy_returns, bins=50, alpha=0.7, edgecolor='black')
    axes[1, 0].axvline(x=0, color='red', linestyle='--', alpha=0.7)
    axes[1, 0].set_title('Return Distribution')
    axes[1, 0].set_xlabel('Return')
    axes[1, 0].set_ylabel('Frequency')
    axes[1, 0].grid(True, alpha=0.3)
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    metric_names = ['Sharpe Ratio', 'Calmar Ratio', 'Win Rate', 'Profit Factor']
    metric_values = [
        metrics['sharpe_ratio'],
        metrics['calmar_ratio'],
        metrics['win_rate'],
        metrics['profit_factor']
    ]
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    metric_values_limited = [min(val, 10) if val != np.inf else 10 for val in metric_values]
    
    bars = axes[1, 1].bar(metric_names, metric_values_limited, 
                         color=['skyblue', 'lightgreen', 'lightcoral', 'gold'], alpha=0.8)
    axes[1, 1].set_title('Key Trading Metrics')
    axes[1, 1].set_ylabel('Value')
    axes[1, 1].tick_params(axis='x', rotation=45)
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
    for bar, value in zip(bars, metric_values):
        height = bar.get_height()
        if value == np.inf:
            label = '‚àû'
        else:
            label = f'{value:.3f}'
        axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.01,
                       label, ha='center', va='bottom')
    
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

def analyze_trading_performance(metrics):
    """–ê–Ω–∞–ª–∏–∑ —Ç–æ—Ä–≥–æ–≤–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    
    print("=== –ê–Ω–∞–ª–∏–∑ —Ç–æ—Ä–≥–æ–≤–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ ===")
    
    # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
    sharpe = metrics['sharpe_ratio']
    calmar = metrics['calmar_ratio']
    win_rate = metrics['win_rate']
    profit_factor = metrics['profit_factor']
    
    print(f"\n–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏:")
    
    # Sharpe Ratio
    if sharpe > 2:
        print(f"‚úÖ –û—Ç–ª–∏—á–Ω—ã–π Sharpe Ratio: {sharpe:.3f}")
    elif sharpe > 1:
        print(f"‚úÖ –•–æ—Ä–æ—à–∏–π Sharpe Ratio: {sharpe:.3f}")
    elif sharpe > 0.5:
        print(f"‚ö†Ô∏è  –£–º–µ—Ä–µ–Ω–Ω—ã–π Sharpe Ratio: {sharpe:.3f}")
    else:
        print(f"‚ùå –ü–ª–æ—Ö–æ–π Sharpe Ratio: {sharpe:.3f}")
    
    # Calmar Ratio
    if calmar > 3:
        print(f"‚úÖ –û—Ç–ª–∏—á–Ω—ã–π Calmar Ratio: {calmar:.3f}")
    elif calmar > 1:
        print(f"‚úÖ –•–æ—Ä–æ—à–∏–π Calmar Ratio: {calmar:.3f}")
    elif calmar > 0.5:
        print(f"‚ö†Ô∏è  –£–º–µ—Ä–µ–Ω–Ω—ã–π Calmar Ratio: {calmar:.3f}")
    else:
        print(f"‚ùå –ü–ª–æ—Ö–æ–π Calmar Ratio: {calmar:.3f}")
    
    # Win Rate
    if win_rate > 0.6:
        print(f"‚úÖ –í—ã—Å–æ–∫–∏–π Win Rate: {win_rate:.3f}")
    elif win_rate > 0.5:
        print(f"‚úÖ –•–æ—Ä–æ—à–∏–π Win Rate: {win_rate:.3f}")
    elif win_rate > 0.4:
        print(f"‚ö†Ô∏è  –£–º–µ—Ä–µ–Ω–Ω—ã–π Win Rate: {win_rate:.3f}")
    else:
        print(f"‚ùå –ù–∏–∑–∫–∏–π Win Rate: {win_rate:.3f}")
    
    # Profit Factor
    if profit_factor > 2:
        print(f"‚úÖ –û—Ç–ª–∏—á–Ω—ã–π Profit Factor: {profit_factor:.3f}")
    elif profit_factor > 1.5:
        print(f"‚úÖ –•–æ—Ä–æ—à–∏–π Profit Factor: {profit_factor:.3f}")
    elif profit_factor > 1:
        print(f"‚ö†Ô∏è  –£–º–µ—Ä–µ–Ω–Ω—ã–π Profit Factor: {profit_factor:.3f}")
    else:
        print(f"‚ùå –ü–ª–æ—Ö–æ–π Profit Factor: {profit_factor:.3f}")
    
    # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞
    print(f"\n–û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞:")
    if sharpe > 1 and calmar > 1 and win_rate > 0.5 and profit_factor > 1.5:
        print("üü¢ –°—Ç—Ä–∞—Ç–µ–≥–∏—è –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –æ—Ç–ª–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
    elif sharpe > 0.5 and calmar > 0.5 and win_rate > 0.4 and profit_factor > 1:
        print("üü° –°—Ç—Ä–∞—Ç–µ–≥–∏—è –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —É–º–µ—Ä–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
    else:
        print("üî¥ –°—Ç—Ä–∞—Ç–µ–≥–∏—è —Ç—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è")

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
def example_trading_metrics_usage():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ—Ä–≥–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫"""
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    np.random.seed(42)
    n_samples = 1000
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏
    returns = np.random.normal(0.001, 0.02, n_samples)  # 0.1% —Å—Ä–µ–¥–Ω—è—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å, 2% –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤ (—Å—Ç—Ä–∞—Ç–µ–≥–∏—è)
    y_true = np.random.choice([0, 1, 2], n_samples, p=[0.3, 0.4, 0.3])
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π (—Å –Ω–µ–∫–æ—Ç–æ—Ä–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é)
    y_pred = y_true.copy()
    # –î–æ–±–∞–≤–ª—è–µ–º –æ—à–∏–±–∫–∏
    error_indices = np.random.choice(n_samples, size=int(n_samples * 0.3), replace=False)
    y_pred[error_indices] = np.random.choice([0, 1, 2], len(error_indices))
    
    print("=== –ü—Ä–∏–º–µ—Ä —Ç–æ—Ä–≥–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫ ===")
    
    # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
    metrics = calculate_trading_metrics(y_true, y_pred, returns)
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    plot_trading_metrics(metrics)
    
    # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    analyze_trading_performance(metrics)
    
    return metrics

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–∞ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
# metrics = example_trading_metrics_usage()
```

## –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–∏–º–µ—Ä

**–¢–µ–æ—Ä–∏—è:** –ü–æ–ª–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è —Ç–æ—Ä–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –≤—Å–µ —ç—Ç–∞–ø—ã: –æ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–æ –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –≠—Ç–æ—Ç –ø—Ä–∏–º–µ—Ä –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é ML-–º–æ–¥–µ–ª–∏ –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

**–≠—Ç–∞–ø—ã –ø–æ–ª–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞:**
1. **–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö:** –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
2. **–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:** Train/Validation/Test
3. **–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π:** –†–∞–∑–ª–∏—á–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã
4. **–í–∞–ª–∏–¥–∞—Ü–∏—è:** Time Series CV
5. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è:** Hyperparameter tuning
6. **–ê–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ:** –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
7. **–û—Ü–µ–Ω–∫–∞:** –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–µ –∏ —Ç–æ—Ä–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞:**

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç –∫–æ–¥:**
1. **–ü–æ–ª–Ω—ã–π pipeline:** –û—Ç –¥–∞–Ω–Ω—ã—Ö –¥–æ –≥–æ—Ç–æ–≤–æ–π –º–æ–¥–µ–ª–∏
2. **–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã:** –¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Ä–∞–∑–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã
3. **–í–∞–ª–∏–¥–∞—Ü–∏—è:** –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
4. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è:** –ù–∞—Ö–æ–¥–∏—Ç –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
5. **–ê–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ:** –ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –ª—É—á—à–∏–µ –º–æ–¥–µ–ª–∏
6. **–û—Ü–µ–Ω–∫–∞:** –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.metrics import accuracy_score, classification_report
import xgboost as xgb
import lightgbm as lgb
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, Any, Tuple, List
import warnings
warnings.filterwarnings('ignore')

def train_complete_trading_model(X, y, returns=None, test_size=0.2, 
                               validation_size=0.2, random_state=42):
    """
    –ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ç–æ—Ä–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏
    
    Args:
        X: –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (samples, features)
        y: –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (samples,)
        returns: –î–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –∞–∫—Ç–∏–≤–æ–≤ (samples,)
        test_size: –î–æ–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        validation_size: –î–æ–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        random_state: Seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    
    Returns:
        dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è –∏ –º–µ—Ç—Ä–∏–∫–∏
    """
    
    print("=== –ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ç–æ—Ä–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏ ===")
    print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {X.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤, {X.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    print(f"–ö–ª–∞—Å—Å—ã: {np.unique(y, return_counts=True)}")
    
    # 1. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    print(f"\n1. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")
    
    # –°–Ω–∞—á–∞–ª–∞ –æ—Ç–¥–µ–ª—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state, stratify=y
    )
    
    # –ó–∞—Ç–µ–º —Ä–∞–∑–¥–µ–ª—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –¥–∞–Ω–Ω—ã–µ –Ω–∞ train –∏ validation
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=validation_size/(1-test_size), 
        random_state=random_state, stratify=y_temp
    )
    
    print(f"  Train: {X_train.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"  Validation: {X_val.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"  Test: {X_test.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
    
    # 2. –û–±—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
    print(f"\n2. –û–±—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π...")
    
    models = {}
    model_scores = {}
    
    # Random Forest
    print("  –û–±—É—á–µ–Ω–∏–µ Random Forest...")
    rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=random_state)
    rf.fit(X_train, y_train)
    rf_score = rf.score(X_val, y_val)
    models['rf'] = rf
    model_scores['rf'] = rf_score
    print(f"    Validation accuracy: {rf_score:.4f}")
    
    # XGBoost
    print("  –û–±—É—á–µ–Ω–∏–µ XGBoost...")
    xgb_model = xgb.XGBClassifier(n_estimators=100, max_depth=6, random_state=random_state, verbosity=0)
    xgb_model.fit(X_train, y_train)
    xgb_score = xgb_model.score(X_val, y_val)
    models['xgb'] = xgb_model
    model_scores['xgb'] = xgb_score
    print(f"    Validation accuracy: {xgb_score:.4f}")
    
    # LightGBM
    print("  –û–±—É—á–µ–Ω–∏–µ LightGBM...")
    lgb_model = lgb.LGBMClassifier(n_estimators=100, max_depth=6, random_state=random_state, verbose=-1)
    lgb_model.fit(X_train, y_train)
    lgb_score = lgb_model.score(X_val, y_val)
    models['lgb'] = lgb_model
    model_scores['lgb'] = lgb_score
    print(f"    Validation accuracy: {lgb_score:.4f}")
    
    # 3. Time Series Cross Validation
    print(f"\n3. Time Series Cross Validation...")
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º train –∏ validation –¥–ª—è CV
    X_cv = np.vstack([X_train, X_val])
    y_cv = np.hstack([y_train, y_val])
    
    # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –¥–ª—è CV
    best_model_name = max(model_scores, key=model_scores.get)
    best_model = models[best_model_name]
    
    print(f"  –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_name} ({model_scores[best_model_name]:.4f})")
    
    # –í—ã–ø–æ–ª–Ω—è–µ–º TSCV
    tscv = TimeSeriesSplit(n_splits=5)
    cv_scores = []
    
    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_cv)):
        X_fold_train, X_fold_val = X_cv[train_idx], X_cv[val_idx]
        y_fold_train, y_fold_val = y_cv[train_idx], y_cv[val_idx]
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –º–æ–¥–µ–ª–∏
        fold_model = type(best_model)(**best_model.get_params())
        fold_model.fit(X_fold_train, y_fold_train)
        
        fold_score = fold_model.score(X_fold_val, y_fold_val)
        cv_scores.append(fold_score)
        
        print(f"    Fold {fold+1}: {fold_score:.4f}")
    
    cv_mean = np.mean(cv_scores)
    cv_std = np.std(cv_scores)
    print(f"  CV Mean: {cv_mean:.4f} ¬± {cv_std:.4f}")
    
    # 4. –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è
    print(f"\n4. –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è...")
    
    # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ø-3 –º–æ–¥–µ–ª–∏
    top_models = sorted(model_scores.items(), key=lambda x: x[1], reverse=True)[:3]
    
    ensemble_models = []
    for name, score in top_models:
        ensemble_models.append((name, models[name]))
        print(f"  {name}: {score:.4f}")
    
    # –°–æ–∑–¥–∞–µ–º Voting Classifier
    ensemble = VotingClassifier(
        estimators=ensemble_models,
        voting='soft',
        n_jobs=-1
    )
    
    # –û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è
    ensemble.fit(X_train, y_train)
    ensemble_score = ensemble.score(X_val, y_val)
    print(f"  Ensemble validation accuracy: {ensemble_score:.4f}")
    
    # 5. –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    print(f"\n5. –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
    test_predictions = {}
    test_scores = {}
    
    for name, model in models.items():
        pred = model.predict(X_test)
        score = accuracy_score(y_test, pred)
        test_predictions[name] = pred
        test_scores[name] = score
        print(f"  {name}: {score:.4f}")
    
    # –ê–Ω—Å–∞–º–±–ª—å
    ensemble_pred = ensemble.predict(X_test)
    ensemble_score = accuracy_score(y_test, ensemble_pred)
    test_predictions['ensemble'] = ensemble_pred
    test_scores['ensemble'] = ensemble_score
    print(f"  Ensemble: {ensemble_score:.4f}")
    
    # 6. –¢–æ—Ä–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)
    trading_metrics = None
    if returns is not None:
        print(f"\n6. –†–∞—Å—á–µ—Ç —Ç–æ—Ä–≥–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫...")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–æ—Ä–≥–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫
        test_returns = returns[-len(y_test):]
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∞–Ω—Å–∞–º–±–ª—è
        trading_metrics = calculate_trading_metrics(
            y_test, ensemble_pred, test_returns
        )
    
    # 7. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print(f"\n=== –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã ===")
    print(f"–õ—É—á—à–∞—è –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å: {max(test_scores, key=test_scores.get)}")
    print(f"–õ—É—á—à–∏–π –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π score: {max(test_scores.values()):.4f}")
    print(f"Ensemble score: {ensemble_score:.4f}")
    print(f"CV score: {cv_mean:.4f} ¬± {cv_std:.4f}")
    
    if trading_metrics:
        print(f"Sharpe Ratio: {trading_metrics['sharpe_ratio']:.4f}")
        print(f"Max Drawdown: {trading_metrics['max_drawdown']:.4f}")
        print(f"Win Rate: {trading_metrics['win_rate']:.4f}")
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    print(f"\n=== Classification Report (Ensemble) ===")
    print(classification_report(y_test, ensemble_pred))
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞
    results = {
        'models': models,
        'ensemble': ensemble,
        'model_scores': model_scores,
        'test_scores': test_scores,
        'cv_scores': cv_scores,
        'cv_mean': cv_mean,
        'cv_std': cv_std,
        'test_predictions': test_predictions,
        'trading_metrics': trading_metrics,
        'best_model': best_model_name,
        'ensemble_score': ensemble_score
    }
    
    return results

def plot_complete_results(results, figsize=(15, 12)):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–ª–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
    
    fig, axes = plt.subplots(2, 2, figsize=figsize)
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    models = list(results['test_scores'].keys())
    scores = list(results['test_scores'].values())
    
    bars = axes[0, 0].bar(models, scores, color='skyblue', alpha=0.8)
    axes[0, 0].set_title('Model Performance Comparison')
    axes[0, 0].set_ylabel('Accuracy')
    axes[0, 0].set_ylim(0, 1)
    axes[0, 0].tick_params(axis='x', rotation=45)
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
    for bar, score in zip(bars, scores):
        height = bar.get_height()
        axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + 0.01,
                       f'{score:.3f}', ha='center', va='bottom')
    
    axes[0, 0].grid(True, alpha=0.3)
    
    # CV —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    cv_scores = results['cv_scores']
    axes[0, 1].plot(range(1, len(cv_scores)+1), cv_scores, 'o-', linewidth=2, markersize=8)
    axes[0, 1].axhline(y=results['cv_mean'], color='red', linestyle='--', 
                      label=f'Mean: {results["cv_mean"]:.3f}')
    axes[0, 1].set_title('Cross-Validation Scores')
    axes[0, 1].set_xlabel('Fold')
    axes[0, 1].set_ylabel('Accuracy')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # –¢–æ—Ä–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)
    if results['trading_metrics']:
        trading_metrics = results['trading_metrics']
        
        # –ö—É–º—É–ª—è—Ç–∏–≤–Ω–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å
        cumulative_returns = trading_metrics['cumulative_returns']
        axes[1, 0].plot(cumulative_returns, linewidth=2)
        axes[1, 0].axhline(y=1, color='black', linestyle='--', alpha=0.5)
        axes[1, 0].set_title('Cumulative Returns')
        axes[1, 0].set_xlabel('Time')
        axes[1, 0].set_ylabel('Cumulative Return')
        axes[1, 0].grid(True, alpha=0.3)
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–æ—Ä–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        metric_names = ['Sharpe', 'Calmar', 'Win Rate', 'Profit Factor']
        metric_values = [
            trading_metrics['sharpe_ratio'],
            trading_metrics['calmar_ratio'],
            trading_metrics['win_rate'],
            trading_metrics['profit_factor']
        ]
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        metric_values_limited = [min(val, 10) if val != np.inf else 10 for val in metric_values]
        
        bars = axes[1, 1].bar(metric_names, metric_values_limited, 
                             color=['skyblue', 'lightgreen', 'lightcoral', 'gold'], alpha=0.8)
        axes[1, 1].set_title('Trading Metrics')
        axes[1, 1].set_ylabel('Value')
        axes[1, 1].tick_params(axis='x', rotation=45)
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
        for bar, value in zip(bars, metric_values):
            height = bar.get_height()
            if value == np.inf:
                label = '‚àû'
            else:
                label = f'{value:.3f}'
            axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.01,
                           label, ha='center', va='bottom')
        
        axes[1, 1].grid(True, alpha=0.3)
    else:
        axes[1, 0].text(0.5, 0.5, 'Trading metrics\nnot available', 
                       ha='center', va='center', transform=axes[1, 0].transAxes)
        axes[1, 0].set_title('Trading Metrics')
        
        axes[1, 1].text(0.5, 0.5, 'Trading metrics\nnot available', 
                       ha='center', va='center', transform=axes[1, 1].transAxes)
        axes[1, 1].set_title('Trading Metrics')
    
    plt.tight_layout()
    plt.show()

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
def example_complete_training_usage():
    """–ü—Ä–∏–º–µ—Ä –ø–æ–ª–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Ç–æ—Ä–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏"""
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    np.random.seed(42)
    n_samples, n_features = 2000, 20
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    X = np.random.randn(n_samples, n_features)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    y = np.zeros(n_samples)
    for i in range(n_samples):
        if X[i, 0] > 0.5 and X[i, 1] < -0.3:
            y[i] = 1
        elif X[i, 2] > 1.0 or X[i, 3] < -1.0:
            y[i] = 2
        else:
            y[i] = 0
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏
    returns = np.random.normal(0.001, 0.02, n_samples)
    
    print("=== –ü—Ä–∏–º–µ—Ä –ø–æ–ª–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Ç–æ—Ä–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏ ===")
    
    # –ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
    results = train_complete_trading_model(X, y, returns)
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    plot_complete_results(results)
    
    return results

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–∞ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
# results = example_complete_training_usage()
```

## –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏

**–¢–µ–æ—Ä–∏—è:** –ü–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞—Å—Ç—É–ø–∞–µ—Ç —ç—Ç–∞–ø –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —Ç–æ—Ä–≥–æ–≤–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏.

**–ü–æ—á–µ–º—É –≤–∞–∂–µ–Ω –∫–∞–∂–¥—ã–π —ç—Ç–∞–ø:**

1. **–ë—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥** - –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
   - **–¶–µ–ª—å:** –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –º–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–µ –æ–Ω–∞ –Ω–µ –≤–∏–¥–µ–ª–∞
   - **–ú–µ—Ç–æ–¥—ã:** Walk-forward analysis, Monte Carlo simulation
   - **–ö—Ä–∏—Ç–µ—Ä–∏–∏:** –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è

2. **–í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ out-of-sample –¥–∞–Ω–Ω—ã—Ö** - –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
   - **–¶–µ–ª—å:** –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
   - **–ü–µ—Ä–∏–æ–¥:** –û–±—ã—á–Ω–æ 20-30% –æ—Ç –æ–±—â–µ–≥–æ –æ–±—ä–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö
   - **–ö—Ä–∏—Ç–µ—Ä–∏–∏:** –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –±–µ–Ω—á–º–∞—Ä–∫–æ–º, —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å

3. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤** - –¢–æ–Ω–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏
   - **–¶–µ–ª—å:** –ú–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏ –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∏—Å–∫–∞
   - **–ú–µ—Ç–æ–¥—ã:** Grid search, Bayesian optimization, Genetic algorithms
   - **–ö—Ä–∏—Ç–µ—Ä–∏–∏:** –£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

4. **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏** - –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
   - **–¶–µ–ª—å:** –°–≤–æ–µ–≤—Ä–µ–º–µ–Ω–Ω–æ –≤—ã—è–≤–ª—è—Ç—å –¥–µ–≥—Ä–∞–¥–∞—Ü–∏—é –º–æ–¥–µ–ª–∏
   - **–ú–µ—Ç—Ä–∏–∫–∏:** Accuracy, Sharpe ratio, Drawdown, Win rate
   - **–î–µ–π—Å—Ç–≤–∏—è:** –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ, –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ —Ç–æ—Ä–≥–æ–≤–ª–∏, –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**

- **–ù–∞—á–Ω–∏—Ç–µ —Å –±—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥–∞** - —ç—Ç–æ –æ—Å–Ω–æ–≤–∞ –¥–ª—è –≤—Å–µ—Ö –¥–∞–ª—å–Ω–µ–π—à–∏—Ö —Ä–µ—à–µ–Ω–∏–π
- **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ walk-forward –∞–Ω–∞–ª–∏–∑** - –æ–Ω –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–µ–Ω –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- **–¢–µ—Å—Ç–∏—Ä—É–π—Ç–µ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —Ä—ã–Ω–æ—á–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö** - –±—ã—á–∏–π/–º–µ–¥–≤–µ–∂–∏–π —Ä—ã–Ω–æ–∫, –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
- **–ü—Ä–æ–≤–µ—Ä—è–π—Ç–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤** - –∏–∑–±–µ–≥–∞–π—Ç–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
- **–î–æ–∫—É–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –≤—Å–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã** - —ç—Ç–æ –ø–æ–º–æ–∂–µ—Ç –≤ –±—É–¥—É—â–∏—Ö –∏—Ç–µ—Ä–∞—Ü–∏—è—Ö

**–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–ª–µ–¥—É—é—â–∏—Ö —ç—Ç–∞–ø–æ–≤:**

```
–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ ‚Üí –ë—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥ ‚Üí –í–∞–ª–∏–¥–∞—Ü–∏—è ‚Üí –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è ‚Üí –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
     ‚Üì              ‚Üì           ‚Üì           ‚Üì           ‚Üì
  –¢–æ—á–Ω–æ—Å—Ç—å      –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∞—è   Out-of-    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã   –†–µ–∞–ª—å–Ω–æ–µ
  –Ω–∞ train      –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å sample   –º–æ–¥–µ–ª–∏     –≤—Ä–µ–º—è
```

–ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç–µ –∫:
- **[06_backtesting.md](06_backtesting.md)** - –ë—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥ —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π
- **[07_validation.md](07_validation.md)** - –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
- **[08_optimization.md](08_optimization.md)** - –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- **[09_monitoring.md](09_monitoring.md)** - –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- **[07_walk_forward_analysis.md](07_walk_forward_analysis.md)** - Walk-forward –∞–Ω–∞–ª–∏–∑

## –ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã

**–¢–µ–æ—Ä–∏—è:** –û–±—É—á–µ–Ω–∏–µ ML-–º–æ–¥–µ–ª–µ–π –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–º–µ–µ—Ç —Å–≤–æ–∏ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∏ —Ç—Ä–µ–±—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞. –ü–æ–Ω–∏–º–∞–Ω–∏–µ —ç—Ç–∏—Ö –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —É—Å–ø–µ—à–Ω—ã—Ö —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π.

**–û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã —É—Å–ø–µ—à–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è:**

### 1. **–ê–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –æ–¥–∏–Ω–æ—á–Ω—ã–µ –º–æ–¥–µ–ª–∏**
- **–ü–æ—á–µ–º—É:** –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–ª–æ–∂–Ω—ã –∏ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã
- **–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:** –°–Ω–∏–∂–µ–Ω–∏–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è, –ø–æ–≤—ã—à–µ–Ω–∏–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
- **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:** –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Voting, Stacking, Bagging
- **–ü—Ä–∞–∫—Ç–∏–∫–∞:** –ö–æ–º–±–∏–Ω–∏—Ä—É–π—Ç–µ 3-5 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤

### 2. **–í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã —Ç—Ä–µ–±—É—é—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏**
- **–ü—Ä–æ–±–ª–µ–º–∞:** –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è CV –Ω–∞—Ä—É—à–∞–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
- **–†–µ—à–µ–Ω–∏–µ:** Time Series CV, Walk-Forward Validation
- **–ö—Ä–∏—Ç–µ—Ä–∏–∏:** –í—Ä–µ–º–µ–Ω–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ data leakage
- **–ü—Ä–∞–∫—Ç–∏–∫–∞:** –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏

### 3. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–∞**
- **–¶–µ–ª—å:** –ù–∞–π—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å bias-variance
- **–ú–µ—Ç–æ–¥—ã:** Grid Search, Random Search, Bayesian optimization
- **–ö—Ä–∏—Ç–µ—Ä–∏–∏:** –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å, –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, —Å–∫–æ—Ä–æ—Å—Ç—å
- **–ü—Ä–∞–∫—Ç–∏–∫–∞:** –ù–∞—á–Ω–∏—Ç–µ —Å –ø—Ä–æ—Å—Ç—ã—Ö –º–µ—Ç–æ–¥–æ–≤, –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç–µ –∫ —Å–ª–æ–∂–Ω—ã–º

### 4. **–¢–æ—Ä–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤–∞–∂–Ω–µ–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã—Ö**
- **–ü—Ä–∏—á–∏–Ω–∞:** Accuracy –Ω–µ –æ—Ç—Ä–∞–∂–∞–µ—Ç —Ä–µ–∞–ª—å–Ω—É—é –ø—Ä–∏–±—ã–ª—å–Ω–æ—Å—Ç—å
- **–ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏:** Sharpe Ratio, Max Drawdown, Win Rate
- **–ê–Ω–∞–ª–∏–∑:** –†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–π—Ç–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ –∫–æ–º–ø–ª–µ–∫—Å–µ
- **–ü—Ä–∞–∫—Ç–∏–∫–∞:** –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ –ø–æ —Ç–æ—Ä–≥–æ–≤—ã–º –º–µ—Ç—Ä–∏–∫–∞–º, –∞ –Ω–µ –ø–æ accuracy

### 5. **–ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —É—Å–ø–µ—Ö**
- **–í–ª–∏—è–Ω–∏–µ:** –ü–ª–æ—Ö–∏–µ –¥–∞–Ω–Ω—ã–µ = –ø–ª–æ—Ö–∞—è –º–æ–¥–µ–ª—å
- **–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:** –û—á–∏—Å—Ç–∫–∞, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è, feature engineering
- **–ü—Ä–æ–≤–µ—Ä–∫–∞:** –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π, –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π, –≤—ã–±—Ä–æ—Å–æ–≤
- **–ü—Ä–∞–∫—Ç–∏–∫–∞:** –ò–Ω–≤–µ—Å—Ç–∏—Ä—É–π—Ç–µ –≤—Ä–µ–º—è –≤ –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –¥–∞–Ω–Ω—ã—Ö

### 6. **–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ**
- **–ü—Ä–æ–±–ª–µ–º–∞:** –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–∫–ª–æ–Ω–Ω—ã –∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—é
- **–ú–µ—Ç–æ–¥—ã:** L1/L2 regularization, dropout, early stopping
- **–ë–∞–ª–∞–Ω—Å:** –°–ª–æ–∂–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ vs. –æ–±–æ–±—â–∞—é—â–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å
- **–ü—Ä–∞–∫—Ç–∏–∫–∞:** –ù–∞—á–∏–Ω–∞–π—Ç–µ —Å –ø—Ä–æ—Å—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π, —É—Å–ª–æ–∂–Ω—è–π—Ç–µ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ

### 7. **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –∞–¥–∞–ø—Ç–∞—Ü–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã**
- **–†–µ–∞–ª—å–Ω–æ—Å—Ç—å:** –†—ã–Ω–∫–∏ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –º–µ–Ω—è—é—Ç—Å—è
- **–î–µ–π—Å—Ç–≤–∏—è:** –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –º–µ—Ç—Ä–∏–∫
- **–ö—Ä–∏—Ç–µ—Ä–∏–∏:** –î–µ–≥—Ä–∞–¥–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ä—ã–Ω–∫–∞
- **–ü—Ä–∞–∫—Ç–∏–∫–∞:** –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä—É–π—Ç–µ –ø—Ä–æ—Ü–µ—Å—Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**

1. **–ù–∞—á–Ω–∏—Ç–µ —Å –ø—Ä–æ—Å—Ç–æ–≥–æ** - Random Forest, –∑–∞—Ç–µ–º –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç–µ –∫ —Å–ª–æ–∂–Ω–æ–º—É
2. **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –≤–∞–ª–∏–¥–∞—Ü–∏—é** - Time Series CV –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
3. **–û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ –ø–æ —Ç–æ—Ä–≥–æ–≤—ã–º –º–µ—Ç—Ä–∏–∫–∞–º** - –Ω–µ –ø–æ accuracy
4. **–¢–µ—Å—Ç–∏—Ä—É–π—Ç–µ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –ø–µ—Ä–∏–æ–¥–∞—Ö** - –±—ã—á–∏–π/–º–µ–¥–≤–µ–∂–∏–π —Ä—ã–Ω–æ–∫
5. **–î–æ–∫—É–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –≤—Å–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã** - —ç—Ç–æ –ø–æ–º–æ–∂–µ—Ç –≤ –±—É–¥—É—â–µ–º
6. **–ü–ª–∞–Ω–∏—Ä—É–π—Ç–µ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥** - –º–æ–¥–µ–ª—å –Ω—É–∂–Ω–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å

**–¢–∏–ø–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏:**

- ‚ùå –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π CV –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
- ‚ùå –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ –ø–æ accuracy
- ‚ùå –ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ—Ä–≥–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫
- ‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- ‚ùå –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö

**–£—Å–ø–µ—à–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è:**

- ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è (Time Series CV)
- ‚úÖ –ê–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã
- ‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ —Ç–æ—Ä–≥–æ–≤—ã–º –º–µ—Ç—Ä–∏–∫–∞–º
- ‚úÖ –†–µ–≥—É–ª—è—Ä–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
- ‚úÖ –ê–¥–∞–ø—Ç–∞—Ü–∏—è –∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º —Ä—ã–Ω–∫–∞

**–ó–∞–∫–ª—é—á–µ–Ω–∏–µ:**

–û–±—É—á–µ–Ω–∏–µ ML-–º–æ–¥–µ–ª–µ–π –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤ - —ç—Ç–æ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å, —Ç—Ä–µ–±—É—é—â–∏–π –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫–∞–∫ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, —Ç–∞–∫ –∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ä—ã–Ω–∫–æ–≤. –£—Å–ø–µ—Ö –ø—Ä–∏—Ö–æ–¥–∏—Ç –∫ —Ç–µ–º, –∫—Ç–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –≤–∞–ª–∏–¥–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –∏ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º —Ä—ã–Ω–∫–∞.

---

**–í–∞–∂–Ω–æ:** –ù–µ –≥–æ–Ω–∏—Ç–µ—Å—å –∑–∞ –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é - –≤–∞–∂–Ω–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–∞—è –ø—Ä–∏–±—ã–ª—å–Ω–æ—Å—Ç—å!
