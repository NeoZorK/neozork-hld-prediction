# 05. ü§ñ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π

**–¶–µ–ª—å:** –ù–∞—É—á–∏—Ç—å—Å—è –æ–±—É—á–∞—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ ML-–º–æ–¥–µ–ª–∏ –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

## –í—ã–±–æ—Ä –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏

### –ü–æ—á–µ–º—É –Ω–µ –≤—Å–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –ø–æ–¥—Ö–æ–¥—è—Ç?

**–§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–º–µ—é—Ç –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:**
- **–ù–µ—Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ—Å—Ç—å** - —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–µ–Ω—è—é—Ç—Å—è –≤–æ –≤—Ä–µ–º–µ–Ω–∏
- **–í—ã—Å–æ–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å** - –º–Ω–æ–≥–æ —à—É–º–∞
- **–ù–µ—Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç—å** - —Ä–µ–¥–∫–∏–µ, –Ω–æ –≤–∞–∂–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è
- **–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏** - –ø—Ä–∏–∑–Ω–∞–∫–∏ —á–∞—Å—Ç–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω—ã

### –õ—É—á—à–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤

1. **–ê–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã** - Random Forest, XGBoost, LightGBM
2. **–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏** - LSTM, GRU, Transformer
3. **SVM** - –¥–ª—è –Ω–µ–ª–∏–Ω–µ–π–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
4. **Logistic Regression** - –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏

## –ê–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã

### 1. Random Forest
```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

def train_random_forest(X, y):
    """–û–±—É—á–µ–Ω–∏–µ Random Forest"""
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    rf = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42,
        n_jobs=-1
    )
    
    # –û–±—É—á–µ–Ω–∏–µ
    rf.fit(X_train, y_train)
    
    # –û—Ü–µ–Ω–∫–∞
    train_score = rf.score(X_train, y_train)
    test_score = rf.score(X_test, y_test)
    
    print(f"Train accuracy: {train_score:.4f}")
    print(f"Test accuracy: {test_score:.4f}")
    
    return rf
```

### 2. XGBoost
```python
import xgboost as xgb
from sklearn.metrics import classification_report

def train_xgboost(X, y):
    """–û–±—É—á–µ–Ω–∏–µ XGBoost"""
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã XGBoost
    params = {
        'objective': 'multi:softprob',
        'num_class': 3,
        'max_depth': 6,
        'learning_rate': 0.1,
        'n_estimators': 100,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'random_state': 42
    }
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    xgb_model = xgb.XGBClassifier(**params)
    
    # –û–±—É—á–µ–Ω–∏–µ
    xgb_model.fit(
        X_train, y_train,
        eval_set=[(X_test, y_test)],
        early_stopping_rounds=10,
        verbose=False
    )
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    y_pred = xgb_model.predict(X_test)
    
    # –û—Ç—á–µ—Ç
    print(classification_report(y_test, y_pred))
    
    return xgb_model
```

### 3. LightGBM
```python
import lightgbm as lgb

def train_lightgbm(X, y):
    """–û–±—É—á–µ–Ω–∏–µ LightGBM"""
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã LightGBM
    params = {
        'objective': 'multiclass',
        'num_class': 3,
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'learning_rate': 0.05,
        'feature_fraction': 0.9,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': -1
    }
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    train_data = lgb.Dataset(X_train, label=y_train)
    test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)
    
    # –û–±—É—á–µ–Ω–∏–µ
    model = lgb.train(
        params,
        train_data,
        valid_sets=[test_data],
        num_boost_round=100,
        callbacks=[lgb.early_stopping(10)]
    )
    
    return model
```

## –ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏

### 1. –ü—Ä–æ—Å—Ç–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å
```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

class TradingNN(nn.Module):
    def __init__(self, input_size, hidden_size=128, num_classes=3):
        super(TradingNN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, hidden_size)
        self.fc4 = nn.Linear(hidden_size, num_classes)
        self.dropout = nn.Dropout(0.2)
        self.relu = nn.ReLU()
        
    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.relu(self.fc3(x))
        x = self.dropout(x)
        x = self.fc4(x)
        return x

def train_neural_network(X, y, epochs=100):
    """–û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏"""
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ç–µ–Ω–∑–æ—Ä—ã
    X_tensor = torch.FloatTensor(X)
    y_tensor = torch.LongTensor(y)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
    dataset = TensorDataset(X_tensor, y_tensor)
    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model = TradingNN(X.shape[1])
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    # –û–±—É—á–µ–Ω–∏–µ
    for epoch in range(epochs):
        for batch_X, batch_y in dataloader:
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
        
        if epoch % 10 == 0:
            print(f'Epoch {epoch}, Loss: {loss.item():.4f}')
    
    return model
```

### 2. LSTM –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
```python
class LSTMTradingModel(nn.Module):
    def __init__(self, input_size, hidden_size=64, num_layers=2, num_classes=3):
        super(LSTMTradingModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, 
                           batch_first=True, dropout=0.2)
        self.fc = nn.Linear(hidden_size, num_classes)
        self.dropout = nn.Dropout(0.2)
        
    def forward(self, x):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)
        
        # LSTM
        out, _ = self.lstm(x, (h0, c0))
        
        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—ã—Ö–æ–¥
        out = self.dropout(out[:, -1, :])
        out = self.fc(out)
        
        return out

def train_lstm_model(X, y, sequence_length=10):
    """–û–±—É—á–µ–Ω–∏–µ LSTM –º–æ–¥–µ–ª–∏"""
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
    X_seq, y_seq = create_sequences(X, y, sequence_length)
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ç–µ–Ω–∑–æ—Ä—ã
    X_tensor = torch.FloatTensor(X_seq)
    y_tensor = torch.LongTensor(y_seq)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model = LSTMTradingModel(X.shape[1])
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    # –û–±—É—á–µ–Ω–∏–µ
    for epoch in range(100):
        optimizer.zero_grad()
        outputs = model(X_tensor)
        loss = criterion(outputs, y_tensor)
        loss.backward()
        optimizer.step()
        
        if epoch % 10 == 0:
            print(f'Epoch {epoch}, Loss: {loss.item():.4f}')
    
    return model

def create_sequences(X, y, sequence_length):
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è LSTM"""
    X_seq, y_seq = [], []
    
    for i in range(sequence_length, len(X)):
        X_seq.append(X[i-sequence_length:i])
        y_seq.append(y[i])
    
    return np.array(X_seq), np.array(y_seq)
```

## –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π

### 1. Time Series Cross Validation
```python
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

def time_series_cv(model, X, y, n_splits=5):
    """–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤"""
    
    tscv = TimeSeriesSplit(n_splits=n_splits)
    scores = []
    
    for train_idx, test_idx in tscv.split(X):
        X_train, X_test = X[train_idx], X[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]
        
        # –û–±—É—á–µ–Ω–∏–µ
        model.fit(X_train, y_train)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_pred = model.predict(X_test)
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted')
        recall = recall_score(y_test, y_pred, average='weighted')
        f1 = f1_score(y_test, y_pred, average='weighted')
        
        scores.append({
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1
        })
    
    return scores
```

### 2. Walk-Forward Validation
```python
def walk_forward_validation(model, X, y, train_size=1000, step_size=100):
    """Walk-Forward –≤–∞–ª–∏–¥–∞—Ü–∏—è"""
    
    results = []
    
    for i in range(train_size, len(X), step_size):
        # –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞
        X_train = X[i-train_size:i]
        y_train = y[i-train_size:i]
        
        # –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞
        X_test = X[i:i+step_size]
        y_test = y[i:i+step_size]
        
        # –û–±—É—á–µ–Ω–∏–µ
        model.fit(X_train, y_train)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_pred = model.predict(X_test)
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        accuracy = accuracy_score(y_test, y_pred)
        results.append(accuracy)
    
    return results
```

## –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

### 1. Grid Search
```python
from sklearn.model_selection import GridSearchCV

def optimize_random_forest(X, y):
    """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è Random Forest"""
    
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [5, 10, 15],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }
    
    rf = RandomForestClassifier(random_state=42)
    
    grid_search = GridSearchCV(
        rf, param_grid, cv=5, 
        scoring='accuracy', n_jobs=-1
    )
    
    grid_search.fit(X, y)
    
    return grid_search.best_estimator_, grid_search.best_params_
```

### 2. Optuna –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
```python
import optuna

def optimize_xgboost_optuna(X, y):
    """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è XGBoost —Å Optuna"""
    
    def objective(trial):
        params = {
            'objective': 'multi:softprob',
            'num_class': 3,
            'max_depth': trial.suggest_int('max_depth', 3, 10),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
            'n_estimators': trial.suggest_int('n_estimators', 50, 200),
            'subsample': trial.suggest_float('subsample', 0.6, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0)
        }
        
        model = xgb.XGBClassifier(**params)
        scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
        return scores.mean()
    
    study = optuna.create_study(direction='maximize')
    study.optimize(objective, n_trials=100)
    
    return study.best_params_
```

## –ê–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã

### 1. Voting Classifier
```python
from sklearn.ensemble import VotingClassifier

def create_ensemble_model(X, y):
    """–°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏"""
    
    # –ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42)
    lgb_model = lgb.LGBMClassifier(n_estimators=100, random_state=42, verbose=-1)
    
    # –ê–Ω—Å–∞–º–±–ª—å
    ensemble = VotingClassifier(
        estimators=[
            ('rf', rf),
            ('xgb', xgb_model),
            ('lgb', lgb_model)
        ],
        voting='soft'
    )
    
    # –û–±—É—á–µ–Ω–∏–µ
    ensemble.fit(X, y)
    
    return ensemble
```

### 2. Stacking
```python
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression

def create_stacking_model(X, y):
    """–°–æ–∑–¥–∞–Ω–∏–µ Stacking –º–æ–¥–µ–ª–∏"""
    
    # –ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏
    base_models = [
        ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),
        ('xgb', xgb.XGBClassifier(n_estimators=100, random_state=42)),
        ('lgb', lgb.LGBMClassifier(n_estimators=100, random_state=42, verbose=-1))
    ]
    
    # –ú–µ—Ç–∞-–º–æ–¥–µ–ª—å
    meta_model = LogisticRegression()
    
    # Stacking
    stacking_model = StackingClassifier(
        estimators=base_models,
        final_estimator=meta_model,
        cv=5
    )
    
    # –û–±—É—á–µ–Ω–∏–µ
    stacking_model.fit(X, y)
    
    return stacking_model
```

## –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### 1. –ú–µ—Ç—Ä–∏–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
```python
from sklearn.metrics import confusion_matrix, classification_report

def evaluate_model(model, X_test, y_test):
    """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏"""
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    
    # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
    cm = confusion_matrix(y_test, y_pred)
    
    # –û—Ç—á–µ—Ç
    report = classification_report(y_test, y_pred)
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'confusion_matrix': cm,
        'classification_report': report
    }
```

### 2. –¢–æ—Ä–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
```python
def calculate_trading_metrics(y_true, y_pred, returns):
    """–†–∞—Å—á–µ—Ç —Ç–æ—Ä–≥–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫"""
    
    # –¢–æ—á–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    accuracy = accuracy_score(y_true, y_pred)
    
    # –ü—Ä–∏–±—ã–ª—å–Ω–æ—Å—Ç—å
    correct_predictions = (y_true == y_pred)
    total_return = (returns * correct_predictions).sum()
    
    # Sharpe Ratio
    if returns.std() > 0:
        sharpe_ratio = returns.mean() / returns.std() * np.sqrt(252)
    else:
        sharpe_ratio = 0
    
    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ—Å–∞–¥–∫–∞
    cumulative_returns = (1 + returns).cumprod()
    running_max = cumulative_returns.expanding().max()
    drawdown = (cumulative_returns - running_max) / running_max
    max_drawdown = drawdown.min()
    
    return {
        'accuracy': accuracy,
        'total_return': total_return,
        'sharpe_ratio': sharpe_ratio,
        'max_drawdown': max_drawdown
    }
```

## –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–∏–º–µ—Ä

```python
def train_complete_trading_model(X, y):
    """–ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ç–æ—Ä–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏"""
    
    # 1. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # 2. –û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è
    ensemble = create_ensemble_model(X_train, y_train)
    
    # 3. –í–∞–ª–∏–¥–∞—Ü–∏—è
    cv_scores = time_series_cv(ensemble, X_train, y_train)
    
    # 4. –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–µ
    test_metrics = evaluate_model(ensemble, X_test, y_test)
    
    # 5. –¢–æ—Ä–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    # (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ —É –Ω–∞—Å –µ—Å—Ç—å returns)
    # trading_metrics = calculate_trading_metrics(y_test, y_pred, returns)
    
    print("=== –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è ===")
    print(f"CV Accuracy: {np.mean([s['accuracy'] for s in cv_scores]):.4f}")
    print(f"Test Accuracy: {test_metrics['accuracy']:.4f}")
    print(f"Test F1: {test_metrics['f1']:.4f}")
    
    return ensemble, test_metrics
```

## –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏

–ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç–µ –∫:
- **[06_backtesting.md](06_backtesting.md)** - –ë—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥
- **[07_walk_forward_analysis.md](07_walk_forward_analysis.md)** - Walk-forward –∞–Ω–∞–ª–∏–∑

## –ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã

1. **–ê–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã** —Ä–∞–±–æ—Ç–∞—é—Ç –ª—É—á—à–µ –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤
2. **–í–∞–ª–∏–¥–∞—Ü–∏—è** –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–∞
3. **–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã** —Å–∏–ª—å–Ω–æ –≤–ª–∏—è—é—Ç –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
4. **–¢–æ—Ä–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏** –≤–∞–∂–Ω–µ–µ —Ç–æ—á–Ω–æ—Å—Ç–∏
5. **–ê–Ω—Å–∞–º–±–ª–∏** –æ–±—ã—á–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏

---

**–í–∞–∂–Ω–æ:** –ù–µ –≥–æ–Ω–∏—Ç–µ—Å—å –∑–∞ –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é - –≤–∞–∂–Ω–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–∞—è –ø—Ä–∏–±—ã–ª—å–Ω–æ—Å—Ç—å!
