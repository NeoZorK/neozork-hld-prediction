# 15. Оптимизация портфолио - Создание прибыльного портфолио

**Цель:** Создать прибыльный портфолио с доходностью более 100% в месяц.

## Введение в оптимизацию портфолио

**Теория:** Оптимизация портфолио представляет собой процесс выбора оптимального распределения активов в портфолио для достижения максимальной доходности при минимальном риске. Это одна из самых важных задач в современном инвестировании, которая требует глубокого понимания финансовых рынков, статистических методов и машинного обучения.

**Почему оптимизация портфолио критически важна:**
- **Максимизация доходности:** Правильная оптимизация может значительно увеличить доходность портфолио
- **Минимизация рисков:** Эффективная диверсификация снижает общий риск портфолио
- **Научный подход:** Современные методы оптимизации основаны на научных принципах
- **Автоматизация:** Позволяет автоматизировать процесс принятия инвестиционных решений

**Исторический контекст:** Концепция оптимизации портфолио была впервые формализована Гарри Марковицем в 1952 году в его знаменитой работе "Portfolio Selection". С тех пор эта область значительно развилась, включив в себя современные методы машинного обучения, блокчейн-технологии и продвинутые алгоритмы оптимизации.

**Современные вызовы:** В современном мире финансовых рынков традиционные методы оптимизации портфолио сталкиваются с новыми вызовами:
- Высокая волатильность рынков
- Сложные корреляции между активами
- Появление новых классов активов (криптовалюты, DeFi)
- Необходимость быстрой адаптации к изменениям рынка
- Требования к прозрачности и автоматизации

## Почему большинство портфолио не прибыльны?

**Теория:** Подавляющее большинство портфолио показывают низкую доходность или убытки из-за фундаментальных проблем в подходе к их формированию и управлению. Понимание этих проблем критически важно для создания прибыльных портфолио.

**Почему большинство портфолио неэффективны:**
- **Системные проблемы:** Фундаментальные проблемы в методологии
- **Отсутствие оптимизации:** Неиспользование современных методов оптимизации
- **Неправильное управление рисками:** Неэффективные стратегии управления рисками
- **Отсутствие адаптации:** Неспособность адаптироваться к изменениям рынка

### Основные проблемы

**Теория:** Основные проблемы портфолио связаны с фундаментальными недостатками в подходе к их формированию и управлению. Эти проблемы можно решить с помощью современных ML-технологий и продвинутых методов оптимизации.

1. **Отсутствие диверсификации**
   - **Теория:** Диверсификация критически важна для снижения рисков
   - **Почему проблематично:** Концентрация в одном активе увеличивает риски
   - **Плюсы:** Простота управления
   - **Минусы:** Высокие риски, потенциальные большие потери

2. **Неправильное распределение активов**
   - **Теория:** Распределение активов должно основываться на научных принципах
   - **Почему проблематично:** Неправильное распределение снижает доходность
   - **Плюсы:** Простота понимания
   - **Минусы:** Неэффективность, низкая доходность

3. **Игнорирование корреляций**
   - **Теория:** Корреляции между активами критически важны для портфолио
   - **Почему проблематично:** Игнорирование корреляций может привести к концентрации рисков
   - **Плюсы:** Простота анализа
   - **Минусы:** Неправильная оценка рисков, концентрация рисков

4. **Отсутствие риск-менеджмента**
   - **Теория:** Управление рисками критически важно для долгосрочного успеха
   - **Почему проблематично:** Отсутствие риск-менеджмента может привести к катастрофическим потерям
   - **Плюсы:** Простота
   - **Минусы:** Высокие риски, потенциальные катастрофические потери

5. **Неправильный выбор активов**
   - **Теория:** Выбор активов должен основываться на фундаментальном анализе
   - **Почему проблематично:** Неправильный выбор активов снижает доходность
   - **Плюсы:** Простота выбора
   - **Минусы:** Низкая доходность, высокие риски

### Наш подход

**Теория:** Наш подход основан на использовании современных ML-технологий, продвинутых методов оптимизации и инновационных решений для создания высокоэффективных портфолио. Это позволяет преодолеть ограничения традиционных подходов.

**Почему наш подход эффективен:**
- **Инновационные технологии:** Использование современных ML-алгоритмов
- **Научный подход:** Основан на научных принципах оптимизации
- **Комплексный анализ:** Учитывает все аспекты портфолио
- **Автоматизация:** Полная автоматизация процесса управления

**Мы используем:**
- **ML-оптимизацию портфолио**
  - **Теория:** Использование машинного обучения для оптимизации портфолио
  - **Почему важно:** Обеспечивает научно обоснованную оптимизацию
  - **Плюсы:** Высокая точность, научная обоснованность, автоматизация
  - **Минусы:** Сложность реализации, высокие требования к данным

- **Динамическое перебалансирование**
  - **Теория:** Автоматическая корректировка весов портфолио
  - **Почему важно:** Обеспечивает поддержание оптимальных весов
  - **Плюсы:** Автоматизация, поддержание оптимальности, адаптивность
  - **Минусы:** Потенциальные частые сделки, комиссии

- **Мультиактивный анализ**
  - **Теория:** Комплексный анализ множества активов
  - **Почему важно:** Обеспечивает полное понимание портфолио
  - **Плюсы:** Комплексный анализ, снижение рисков, повышение доходности
  - **Минусы:** Сложность анализа, высокие вычислительные требования

- **Продвинутый риск-менеджмент**
  - **Теория:** Эффективные стратегии управления рисками
  - **Почему важно:** Критически важно для долгосрочного успеха
  - **Плюсы:** Снижение рисков, защита капитала, стабильность
  - **Минусы:** Сложность настройки, потенциальные ограничения доходности

- **Блокчейн-интеграцию**
  - **Теория:** Использование блокчейн-технологий для увеличения доходности
  - **Почему важно:** Предоставляет новые возможности для заработка
  - **Плюсы:** Новые возможности, децентрализация, прозрачность
  - **Минусы:** Сложность интеграции, высокие требования к безопасности

## ML-оптимизация портфолио

**Теория:** ML-оптимизация портфолио представляет собой использование машинного обучения для научно обоснованной оптимизации портфолио. Это критически важно для создания высокоэффективных портфолио с доходностью 100%+ в месяц.

**Почему ML-оптимизация критична:**
- **Научная обоснованность:** Обеспечивает научно обоснованную оптимизацию
- **Высокая точность:** Обеспечивает высокую точность предсказаний
- **Автоматизация:** Автоматизирует процесс оптимизации
- **Адаптивность:** Может адаптироваться к изменениям рынка

### 1. Предсказание доходности активов

**Теория:** Предсказание доходности активов является фундаментальной задачей для оптимизации портфолио. Точные предсказания доходности критически важны для принятия правильных инвестиционных решений. Современные методы машинного обучения позволяют создавать сложные модели, которые учитывают множество факторов и могут адаптироваться к изменениям рынка.

**Математическая основа:** Предсказание доходности активов основано на анализе временных рядов, где мы пытаемся найти функцию f, такую что:
```
R(t+1) = f(X(t), X(t-1), ..., X(t-n)) + ε(t)
```
где R(t+1) - доходность в момент t+1, X(t) - признаки в момент t, ε(t) - случайная ошибка.

**Почему предсказание доходности важно:**
- **Основа оптимизации:** Является основой для оптимизации портфолио
- **Снижение рисков:** Помогает снизить риски инвестиций
- **Повышение доходности:** Может значительно повысить доходность
- **Научный подход:** Обеспечивает научный подход к инвестированию

**Методы предсказания:**
1. **Линейные модели:** Простые, но эффективные для стабильных рынков
2. **Деревья решений:** Хорошо работают с нелинейными зависимостями
3. **Нейронные сети:** Могут моделировать сложные нелинейные отношения
4. **Ансамбли:** Комбинируют несколько моделей для повышения точности

**Плюсы:**
- Научная обоснованность
- Снижение рисков
- Повышение доходности
- Автоматизация процесса

**Минусы:**
- Сложность реализации
- Высокие требования к данным
- Потенциальная нестабильность предсказаний

**Практическое применение:** В нашем коде мы используем XGBoost - градиентный бустинг, который отлично подходит для финансовых данных благодаря своей способности обрабатывать нелинейные зависимости и выбросы.

```python
# Необходимые импорты для полного функционирования
import numpy as np
import pandas as pd
from xgboost import XGBRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

class FeatureEngineer:
    """Инженер признаков для финансовых данных"""
    
    def __init__(self):
        self.scaler = StandardScaler()
        self.is_fitted = False
    
    def create_features(self, data):
        """Создание признаков из финансовых данных"""
        features = pd.DataFrame()
        
        # Технические индикаторы
        features['sma_5'] = data['Close'].rolling(5).mean()
        features['sma_20'] = data['Close'].rolling(20).mean()
        features['sma_50'] = data['Close'].rolling(50).mean()
        
        # RSI
        features['rsi'] = self._calculate_rsi(data['Close'])
        
        # Bollinger Bands
        bb_upper, bb_lower = self._calculate_bollinger_bands(data['Close'])
        features['bb_upper'] = bb_upper
        features['bb_lower'] = bb_lower
        features['bb_position'] = (data['Close'] - bb_lower) / (bb_upper - bb_lower)
        
        # Volatility
        features['volatility'] = data['Close'].rolling(20).std()
        
        # Price momentum
        features['momentum_5'] = data['Close'].pct_change(5)
        features['momentum_10'] = data['Close'].pct_change(10)
        features['momentum_20'] = data['Close'].pct_change(20)
        
        # Volume indicators
        if 'Volume' in data.columns:
            features['volume_sma'] = data['Volume'].rolling(20).mean()
            features['volume_ratio'] = data['Volume'] / features['volume_sma']
        
        # Lagged returns
        features['return_1'] = data['Close'].pct_change(1)
        features['return_2'] = data['Close'].pct_change(2)
        features['return_3'] = data['Close'].pct_change(3)
        
        # Drop NaN values
        features = features.dropna()
        
        return features
    
    def _calculate_rsi(self, prices, window=14):
        """Расчет RSI"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi
    
    def _calculate_bollinger_bands(self, prices, window=20, num_std=2):
        """Расчет полос Боллинджера"""
        sma = prices.rolling(window).mean()
        std = prices.rolling(window).std()
        upper_band = sma + (std * num_std)
        lower_band = sma - (std * num_std)
        return upper_band, lower_band

class AssetReturnPredictor:
    """Предсказание доходности активов с использованием машинного обучения"""
    
    def __init__(self, assets):
        """
        Инициализация предсказателя доходности
        
        Args:
            assets (list): Список активов для анализа
        """
        self.assets = assets
        self.models = {}
        self.feature_engineers = {}
        self.scalers = {}
        self.performance_metrics = {}
        
        # Инициализация моделей для каждого актива
        for asset in assets:
            self.models[asset] = self._create_model()
            self.feature_engineers[asset] = FeatureEngineer()
            self.scalers[asset] = StandardScaler()
    
    def _create_model(self):
        """Создание оптимизированной модели XGBoost"""
        return XGBRegressor(
            n_estimators=200,
            max_depth=8,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            n_jobs=-1
        )
    
    def train(self, asset, data, test_size=0.2):
        """
        Обучение модели для конкретного актива
        
        Args:
            asset (str): Название актива
            data (pd.DataFrame): Данные с колонками OHLCV
            test_size (float): Доля тестовых данных
        
        Returns:
            dict: Метрики производительности модели
        """
        print(f"Training model for {asset}...")
        
        # Создание признаков
        features = self.feature_engineers[asset].create_features(data)
        
        # Создание целевой переменной (доходность на следующий день)
        target = self._create_target(data)
        
        # Синхронизация индексов
        common_index = features.index.intersection(target.index)
        features = features.loc[common_index]
        target = target.loc[common_index]
        
        # Разделение на обучающую и тестовую выборки
        X_train, X_test, y_train, y_test = train_test_split(
            features, target, test_size=test_size, random_state=42, shuffle=False
        )
        
        # Масштабирование признаков
        X_train_scaled = self.scalers[asset].fit_transform(X_train)
        X_test_scaled = self.scalers[asset].transform(X_test)
        
        # Обучение модели
        self.models[asset].fit(X_train_scaled, y_train)
        
        # Предсказания
        y_pred_train = self.models[asset].predict(X_train_scaled)
        y_pred_test = self.models[asset].predict(X_test_scaled)
        
        # Расчет метрик
        train_mse = mean_squared_error(y_train, y_pred_train)
        test_mse = mean_squared_error(y_test, y_pred_test)
        train_r2 = r2_score(y_train, y_pred_train)
        test_r2 = r2_score(y_test, y_pred_test)
        
        self.performance_metrics[asset] = {
            'train_mse': train_mse,
            'test_mse': test_mse,
            'train_r2': train_r2,
            'test_r2': test_r2,
            'feature_importance': self._get_feature_importance(asset, features.columns)
        }
        
        print(f"Training completed for {asset}. Test R²: {test_r2:.4f}")
        
        return self.performance_metrics[asset]
    
    def predict_returns(self, asset, data):
        """
        Предсказание доходности актива
        
        Args:
            asset (str): Название актива
            data (pd.DataFrame): Данные для предсказания
        
        Returns:
            np.array: Предсказанные доходности
        """
        if asset not in self.models:
            raise ValueError(f"Model for {asset} not found. Train the model first.")
        
        # Создание признаков
        features = self.feature_engineers[asset].create_features(data)
        
        # Масштабирование
        features_scaled = self.scalers[asset].transform(features)
        
        # Предсказание
        predicted_returns = self.models[asset].predict(features_scaled)
        
        return predicted_returns
    
    def _create_target(self, data):
        """Создание целевой переменной (доходность на следующий день)"""
        # Доходность на следующий день
        future_price = data['Close'].shift(-1)
        current_price = data['Close']
        
        # Расчет доходности
        returns = (future_price - current_price) / current_price
        
        return returns
    
    def _get_feature_importance(self, asset, feature_names):
        """Получение важности признаков"""
        importance = self.models[asset].feature_importances_
        return dict(zip(feature_names, importance))
    
    def get_performance_summary(self):
        """Получение сводки по производительности всех моделей"""
        summary = {}
        for asset, metrics in self.performance_metrics.items():
            summary[asset] = {
                'test_r2': metrics['test_r2'],
                'test_mse': metrics['test_mse'],
                'top_features': sorted(
                    metrics['feature_importance'].items(), 
                    key=lambda x: x[1], 
                    reverse=True
                )[:5]
            }
        return summary

# Пример использования
if __name__ == "__main__":
    # Создание тестовых данных
    np.random.seed(42)
    dates = pd.date_range('2020-01-01', '2023-12-31', freq='D')
    n_days = len(dates)
    
    # Генерация синтетических данных
    price_data = {
        'Date': dates,
        'Open': 100 + np.cumsum(np.random.randn(n_days) * 0.02),
        'High': 0,
        'Low': 0,
        'Close': 0,
        'Volume': np.random.randint(1000, 10000, n_days)
    }
    
    # Расчет High, Low, Close
    for i in range(n_days):
        base_price = price_data['Open'][i]
        price_data['High'][i] = base_price * (1 + abs(np.random.randn() * 0.02))
        price_data['Low'][i] = base_price * (1 - abs(np.random.randn() * 0.02))
        price_data['Close'][i] = base_price * (1 + np.random.randn() * 0.01)
    
    df = pd.DataFrame(price_data)
    df.set_index('Date', inplace=True)
    
    # Инициализация и обучение
    assets = ['TEST_ASSET']
    predictor = AssetReturnPredictor(assets)
    
    # Обучение модели
    performance = predictor.train('TEST_ASSET', df)
    print("Performance metrics:", performance)
    
    # Предсказание
    predictions = predictor.predict_returns('TEST_ASSET', df.tail(100))
    print(f"Predicted returns for last 100 days: {predictions[:5]}...")
    
    # Сводка по производительности
    summary = predictor.get_performance_summary()
    print("Performance summary:", summary)
```

### 2. Оптимизация весов портфолио

**Теория:** Оптимизация весов портфолио представляет собой процесс определения оптимальных весов активов в портфолио для максимизации доходности при минимизации рисков. Это критически важно для создания эффективных портфолио. Современные методы оптимизации используют математические алгоритмы для поиска оптимального распределения активов.

**Математическая основа:** Задача оптимизации портфолио может быть сформулирована как:
```
maximize: μ^T * w - λ * w^T * Σ * w
subject to: Σw_i = 1, w_i ≥ 0
```
где μ - ожидаемые доходности, w - веса активов, Σ - ковариационная матрица, λ - коэффициент риска.

**Методы оптимизации:**
1. **Mean-Variance Optimization (MVO):** Классический метод Марковица
2. **Black-Litterman Model:** Улучшенная версия MVO с рыночными ожиданиями
3. **Risk Parity:** Равномерное распределение риска между активами
4. **Maximum Sharpe Ratio:** Максимизация отношения доходность/риск
5. **Minimum Variance:** Минимизация общей волатильности портфолио

**Почему оптимизация весов важна:**
- **Максимизация доходности:** Помогает максимизировать доходность портфолио
- **Минимизация рисков:** Помогает минимизировать риски
- **Научный подход:** Обеспечивает научный подход к распределению активов
- **Автоматизация:** Автоматизирует процесс принятия решений

**Плюсы:**
- Максимизация доходности
- Минимизация рисков
- Научная обоснованность
- Автоматизация процесса

**Минусы:**
- Сложность реализации
- Потенциальная нестабильность весов
- Высокие требования к данным

**Практическое применение:** В нашем коде мы реализуем несколько методов оптимизации, включая максимизацию коэффициента Шарпа, минимизацию дисперсии и риск-паритет. Это позволяет выбрать наиболее подходящий метод для конкретных рыночных условий.

```python
# Дополнительные импорты для оптимизации портфолио
from scipy.optimize import minimize
from scipy.linalg import cholesky, solve_triangular
import cvxpy as cp

class PortfolioOptimizer:
    """Продвинутый оптимизатор портфолио с множественными методами"""
    
    def __init__(self, assets, risk_free_rate=0.02):
        """
        Инициализация оптимизатора портфолио
        
        Args:
            assets (list): Список активов
            risk_free_rate (float): Безрисковая ставка
        """
        self.assets = assets
        self.risk_free_rate = risk_free_rate
        self.n_assets = len(assets)
        self.optimization_results = {}
        
        # Базовые ограничения
        self.bounds = [(0, 1) for _ in range(self.n_assets)]
        self.constraints = [
                {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}  # Сумма весов = 1
        ]
    
    def optimize_maximum_sharpe(self, expected_returns, cov_matrix):
        """
        Оптимизация для максимизации коэффициента Шарпа
        
        Args:
            expected_returns (np.array): Ожидаемые доходности
            cov_matrix (np.array): Ковариационная матрица
        
        Returns:
            dict: Результаты оптимизации
        """
        def negative_sharpe(weights):
            """Отрицательный коэффициент Шарпа для минимизации"""
            portfolio_return = np.dot(weights, expected_returns)
            portfolio_risk = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))
            
            if portfolio_risk == 0:
                return -np.inf
            
            sharpe_ratio = (portfolio_return - self.risk_free_rate) / portfolio_risk
            return -sharpe_ratio
        
        # Начальные веса (равномерное распределение)
        initial_weights = np.ones(self.n_assets) / self.n_assets
        
        # Оптимизация
        result = minimize(
            negative_sharpe,
            initial_weights,
            method='SLSQP',
            bounds=self.bounds,
            constraints=self.constraints,
            options={'ftol': 1e-9, 'disp': False}
        )
        
        if result.success:
            optimal_weights = result.x
            portfolio_return = np.dot(optimal_weights, expected_returns)
            portfolio_risk = np.sqrt(np.dot(optimal_weights.T, np.dot(cov_matrix, optimal_weights)))
            sharpe_ratio = (portfolio_return - self.risk_free_rate) / portfolio_risk
            
            self.optimization_results['maximum_sharpe'] = {
                'weights': optimal_weights,
                'return': portfolio_return,
                'risk': portfolio_risk,
                'sharpe_ratio': sharpe_ratio,
                'success': True
            }
        else:
            self.optimization_results['maximum_sharpe'] = {
                'weights': initial_weights,
                'return': np.dot(initial_weights, expected_returns),
                'risk': np.sqrt(np.dot(initial_weights.T, np.dot(cov_matrix, initial_weights))),
                'sharpe_ratio': 0,
                'success': False,
                'message': result.message
            }
        
        return self.optimization_results['maximum_sharpe']
    
    def optimize_minimum_variance(self, cov_matrix):
        """
        Оптимизация для минимизации дисперсии портфолио
        
        Args:
            cov_matrix (np.array): Ковариационная матрица
        
        Returns:
            dict: Результаты оптимизации
        """
        def portfolio_variance(weights):
            """Дисперсия портфолио"""
            return np.dot(weights.T, np.dot(cov_matrix, weights))
        
        # Начальные веса
        initial_weights = np.ones(self.n_assets) / self.n_assets
        
        # Оптимизация
        result = minimize(
            portfolio_variance,
            initial_weights,
            method='SLSQP',
            bounds=self.bounds,
            constraints=self.constraints,
            options={'ftol': 1e-9, 'disp': False}
        )
        
        if result.success:
            optimal_weights = result.x
            portfolio_risk = np.sqrt(result.fun)
            
            self.optimization_results['minimum_variance'] = {
                'weights': optimal_weights,
                'risk': portfolio_risk,
                'success': True
            }
        else:
            self.optimization_results['minimum_variance'] = {
                'weights': initial_weights,
                'risk': np.sqrt(np.dot(initial_weights.T, np.dot(cov_matrix, initial_weights))),
                'success': False,
                'message': result.message
            }
        
        return self.optimization_results['minimum_variance']
    
    def optimize_risk_parity(self, cov_matrix):
        """
        Оптимизация риск-паритета (равномерное распределение риска)
        
        Args:
            cov_matrix (np.array): Ковариационная матрица
        
        Returns:
            dict: Результаты оптимизации
        """
        def risk_parity_objective(weights):
            """Целевая функция для риск-паритета"""
            portfolio_risk = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))
            
            # Риск каждого актива
            individual_risks = []
            for i in range(self.n_assets):
                asset_risk = np.sqrt(cov_matrix[i, i])
                individual_risks.append(asset_risk)
            
            # Целевое распределение риска (равномерное)
            target_risk_per_asset = portfolio_risk / self.n_assets
            
            # Ошибка от целевого распределения
            error = 0
            for i in range(self.n_assets):
                actual_risk_contribution = weights[i] * individual_risks[i]
                error += (actual_risk_contribution - target_risk_per_asset) ** 2
            
            return error
        
        # Начальные веса
        initial_weights = np.ones(self.n_assets) / self.n_assets
        
        # Оптимизация
        result = minimize(
            risk_parity_objective,
            initial_weights,
            method='SLSQP',
            bounds=self.bounds,
            constraints=self.constraints,
            options={'ftol': 1e-9, 'disp': False}
        )
        
        if result.success:
            optimal_weights = result.x
            portfolio_risk = np.sqrt(np.dot(optimal_weights.T, np.dot(cov_matrix, optimal_weights)))
            
            self.optimization_results['risk_parity'] = {
                'weights': optimal_weights,
                'risk': portfolio_risk,
                'success': True
            }
        else:
            self.optimization_results['risk_parity'] = {
                'weights': initial_weights,
                'risk': np.sqrt(np.dot(initial_weights.T, np.dot(cov_matrix, initial_weights))),
                'success': False,
                'message': result.message
            }
        
        return self.optimization_results['risk_parity']
    
    def optimize_mean_variance(self, expected_returns, cov_matrix, risk_aversion=1.0):
        """
        Классическая оптимизация среднее-дисперсия
        
        Args:
            expected_returns (np.array): Ожидаемые доходности
            cov_matrix (np.array): Ковариационная матрица
            risk_aversion (float): Коэффициент неприятия риска
        
        Returns:
            dict: Результаты оптимизации
        """
        def mean_variance_objective(weights):
            """Целевая функция среднее-дисперсия"""
            portfolio_return = np.dot(weights, expected_returns)
            portfolio_variance = np.dot(weights.T, np.dot(cov_matrix, weights))
            
            # Максимизируем: return - risk_aversion * variance
            return -(portfolio_return - risk_aversion * portfolio_variance)
        
        # Начальные веса
        initial_weights = np.ones(self.n_assets) / self.n_assets
        
        # Оптимизация
        result = minimize(
            mean_variance_objective,
            initial_weights,
            method='SLSQP',
            bounds=self.bounds,
            constraints=self.constraints,
            options={'ftol': 1e-9, 'disp': False}
        )
        
        if result.success:
            optimal_weights = result.x
            portfolio_return = np.dot(optimal_weights, expected_returns)
            portfolio_risk = np.sqrt(np.dot(optimal_weights.T, np.dot(cov_matrix, optimal_weights)))
            
            self.optimization_results['mean_variance'] = {
                'weights': optimal_weights,
                'return': portfolio_return,
                'risk': portfolio_risk,
                'sharpe_ratio': (portfolio_return - self.risk_free_rate) / portfolio_risk,
                'success': True
            }
        else:
            self.optimization_results['mean_variance'] = {
                'weights': initial_weights,
                'return': np.dot(initial_weights, expected_returns),
                'risk': np.sqrt(np.dot(initial_weights.T, np.dot(cov_matrix, initial_weights))),
                'sharpe_ratio': 0,
                'success': False,
                'message': result.message
            }
        
        return self.optimization_results['mean_variance']
    
    def get_efficient_frontier(self, expected_returns, cov_matrix, n_portfolios=100):
        """
        Построение эффективной границы
        
        Args:
            expected_returns (np.array): Ожидаемые доходности
            cov_matrix (np.array): Ковариационная матрица
            n_portfolios (int): Количество портфолио на границе
        
        Returns:
            dict: Данные эффективной границы
        """
        # Находим минимальную и максимальную ожидаемую доходность
        min_return = np.min(expected_returns)
        max_return = np.max(expected_returns)
        
        # Создаем диапазон целевых доходностей
        target_returns = np.linspace(min_return, max_return, n_portfolios)
        
        efficient_portfolios = []
        
        for target_return in target_returns:
            # Ограничение на целевую доходность
            constraints = self.constraints + [
                {'type': 'eq', 'fun': lambda w: np.dot(w, expected_returns) - target_return}
            ]
            
            def portfolio_variance(weights):
                return np.dot(weights.T, np.dot(cov_matrix, weights))
            
            # Начальные веса
            initial_weights = np.ones(self.n_assets) / self.n_assets
            
            # Оптимизация
            result = minimize(
                portfolio_variance,
                initial_weights,
                method='SLSQP',
                bounds=self.bounds,
                constraints=constraints,
                options={'ftol': 1e-9, 'disp': False}
            )
            
            if result.success:
                portfolio_risk = np.sqrt(result.fun)
                efficient_portfolios.append({
                    'weights': result.x,
                    'return': target_return,
                    'risk': portfolio_risk,
                    'sharpe_ratio': (target_return - self.risk_free_rate) / portfolio_risk
                })
        
        return {
            'portfolios': efficient_portfolios,
            'returns': [p['return'] for p in efficient_portfolios],
            'risks': [p['risk'] for p in efficient_portfolios],
            'sharpe_ratios': [p['sharpe_ratio'] for p in efficient_portfolios]
        }
    
    def compare_methods(self, expected_returns, cov_matrix):
        """
        Сравнение различных методов оптимизации
        
        Args:
            expected_returns (np.array): Ожидаемые доходности
            cov_matrix (np.array): Ковариационная матрица
        
        Returns:
            dict: Сравнение методов
        """
        # Запускаем все методы
        self.optimize_maximum_sharpe(expected_returns, cov_matrix)
        self.optimize_minimum_variance(cov_matrix)
        self.optimize_risk_parity(cov_matrix)
        self.optimize_mean_variance(expected_returns, cov_matrix)
        
        # Создаем сравнительную таблицу
        comparison = {}
        for method, results in self.optimization_results.items():
            if results['success']:
                comparison[method] = {
                    'weights': results['weights'],
                    'return': results.get('return', 0),
                    'risk': results['risk'],
                    'sharpe_ratio': results.get('sharpe_ratio', 0)
                }
        
        return comparison

# Пример использования
if __name__ == "__main__":
    # Создание тестовых данных
    np.random.seed(42)
    n_assets = 5
    n_periods = 252
    
    # Генерация случайных доходностей
    expected_returns = np.random.normal(0.08, 0.15, n_assets)  # 8% средняя доходность
    cov_matrix = np.random.rand(n_assets, n_assets)
    cov_matrix = cov_matrix @ cov_matrix.T  # Делаем положительно определенной
    cov_matrix = cov_matrix * 0.1  # Масштабируем
    
    # Инициализация оптимизатора
    assets = [f'Asset_{i+1}' for i in range(n_assets)]
    optimizer = PortfolioOptimizer(assets, risk_free_rate=0.02)
    
    print("=== Оптимизация портфолио ===")
    print(f"Активы: {assets}")
    print(f"Ожидаемые доходности: {expected_returns}")
    print()
    
    # Тестирование различных методов
    print("1. Максимизация коэффициента Шарпа:")
    sharpe_result = optimizer.optimize_maximum_sharpe(expected_returns, cov_matrix)
    print(f"   Веса: {sharpe_result['weights']}")
    print(f"   Доходность: {sharpe_result['return']:.4f}")
    print(f"   Риск: {sharpe_result['risk']:.4f}")
    print(f"   Коэффициент Шарпа: {sharpe_result['sharpe_ratio']:.4f}")
    print()
    
    print("2. Минимизация дисперсии:")
    min_var_result = optimizer.optimize_minimum_variance(cov_matrix)
    print(f"   Веса: {min_var_result['weights']}")
    print(f"   Риск: {min_var_result['risk']:.4f}")
    print()
    
    print("3. Риск-паритет:")
    risk_parity_result = optimizer.optimize_risk_parity(cov_matrix)
    print(f"   Веса: {risk_parity_result['weights']}")
    print(f"   Риск: {risk_parity_result['risk']:.4f}")
    print()
    
    # Сравнение методов
    print("4. Сравнение методов:")
    comparison = optimizer.compare_methods(expected_returns, cov_matrix)
    for method, results in comparison.items():
        print(f"   {method}: Sharpe = {results['sharpe_ratio']:.4f}, Risk = {results['risk']:.4f}")
```

### 3. Динамическое перебалансирование

**Теория:** Динамическое перебалансирование представляет собой автоматическую корректировку весов портфолио для поддержания оптимального распределения активов. Это критически важно для поддержания эффективности портфолио в условиях постоянно изменяющихся рынков.

**Математическая основа:** Перебалансирование происходит когда отклонение текущих весов от целевых превышает заданный порог:
```
max|w_current - w_target| > threshold
```
где w_current - текущие веса, w_target - целевые веса, threshold - порог перебалансирования.

**Стратегии перебалансирования:**
1. **Временные интервалы:** Перебалансирование по расписанию (ежедневно, еженедельно, ежемесячно)
2. **Пороговые значения:** Перебалансирование при превышении порога отклонения
3. **Адаптивные пороги:** Динамическое изменение порогов в зависимости от волатильности
4. **Гибридные подходы:** Комбинация временных и пороговых стратегий

**Почему динамическое перебалансирование важно:**
- **Поддержание оптимальности:** Обеспечивает поддержание оптимальных весов
- **Адаптивность:** Позволяет адаптироваться к изменениям рынка
- **Автоматизация:** Автоматизирует процесс управления портфолио
- **Повышение доходности:** Может повысить доходность портфолио

**Факторы, влияющие на перебалансирование:**
- **Транзакционные издержки:** Комиссии за сделки
- **Налоговые последствия:** Налоги на реализованную прибыль
- **Ликвидность рынка:** Возможность быстрого исполнения сделок
- **Волатильность активов:** Частота необходимых корректировок

**Плюсы:**
- Поддержание оптимальности
- Адаптивность к изменениям
- Автоматизация управления
- Повышение доходности

**Минусы:**
- Потенциальные частые сделки
- Комиссии за сделки
- Сложность настройки

**Практическое применение:** В нашем коде мы реализуем интеллектуальную систему перебалансирования, которая учитывает транзакционные издержки, волатильность рынка и другие факторы для принятия оптимальных решений о перебалансировании.

```python
# Дополнительные импорты для динамического перебалансирования
from datetime import datetime, timedelta
import logging

class DynamicRebalancer:
    """Интеллектуальная система динамического перебалансирования портфолио"""
    
    def __init__(self, rebalancing_threshold=0.05, transaction_cost=0.001, 
                 min_rebalancing_interval=1, volatility_lookback=20):
        """
        Инициализация системы перебалансирования
        
        Args:
            rebalancing_threshold (float): Порог отклонения для перебалансирования
            transaction_cost (float): Транзакционные издержки (в долях)
            min_rebalancing_interval (int): Минимальный интервал между перебалансированиями (дни)
            volatility_lookback (int): Период для расчета волатильности
        """
        self.rebalancing_threshold = rebalancing_threshold
        self.transaction_cost = transaction_cost
        self.min_rebalancing_interval = min_rebalancing_interval
        self.volatility_lookback = volatility_lookback
        
        # Состояние системы
        self.current_weights = None
        self.target_weights = None
        self.last_rebalancing = None
        self.rebalancing_history = []
        self.volatility_history = []
        
        # Настройка логирования
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def should_rebalance(self, current_weights, target_weights, current_prices=None, 
                        force_rebalance=False):
        """
        Интеллектуальная проверка необходимости перебалансирования
        
        Args:
            current_weights (np.array): Текущие веса портфолио
            target_weights (np.array): Целевые веса портфолио
            current_prices (np.array): Текущие цены активов
            force_rebalance (bool): Принудительное перебалансирование
        
        Returns:
            dict: Решение о перебалансировании с обоснованием
        """
        # Проверка временного интервала
        if not force_rebalance and self.last_rebalancing:
            days_since_rebalance = (datetime.now() - self.last_rebalancing).days
            if days_since_rebalance < self.min_rebalancing_interval:
                return {
                    'should_rebalance': False,
                    'reason': 'min_interval_not_met',
                    'days_since_last': days_since_rebalance
                }
        
        # Расчет отклонения весов
        weight_deviation = np.abs(current_weights - target_weights)
        max_deviation = np.max(weight_deviation)
        mean_deviation = np.mean(weight_deviation)
        
        # Адаптивный порог на основе волатильности
        adaptive_threshold = self._calculate_adaptive_threshold(current_prices)
        
        # Проверка превышения порога
        if max_deviation > adaptive_threshold:
            # Расчет стоимости перебалансирования
            rebalancing_cost = self._calculate_rebalancing_cost(
                current_weights, target_weights
            )
            
            # Проверка экономической целесообразности
            if rebalancing_cost < self._calculate_benefit_threshold(
                current_weights, target_weights, max_deviation
            ):
                return {
                    'should_rebalance': True,
                    'reason': 'threshold_exceeded',
                    'max_deviation': max_deviation,
                    'adaptive_threshold': adaptive_threshold,
                    'rebalancing_cost': rebalancing_cost
                }
            else:
                return {
                    'should_rebalance': False,
                    'reason': 'cost_too_high',
                    'max_deviation': max_deviation,
                    'rebalancing_cost': rebalancing_cost
                }
        
        return {
            'should_rebalance': False,
            'reason': 'within_threshold',
            'max_deviation': max_deviation,
            'adaptive_threshold': adaptive_threshold
        }
    
    def _calculate_adaptive_threshold(self, current_prices):
        """Расчет адаптивного порога на основе волатильности"""
        if current_prices is None or len(self.volatility_history) < self.volatility_lookback:
            return self.rebalancing_threshold
        
        # Расчет текущей волатильности
        recent_prices = current_prices[-self.volatility_lookback:]
        returns = np.diff(np.log(recent_prices))
        current_volatility = np.std(returns) * np.sqrt(252)  # Годовая волатильность
        
        # Адаптация порога: высокая волатильность = более высокий порог
        volatility_multiplier = 1 + (current_volatility - 0.2) * 2  # Базовый уровень 20%
        adaptive_threshold = self.rebalancing_threshold * volatility_multiplier
        
        # Ограничиваем порог разумными пределами
        adaptive_threshold = np.clip(adaptive_threshold, 0.01, 0.2)
        
        return adaptive_threshold
    
    def _calculate_rebalancing_cost(self, current_weights, target_weights):
        """Расчет стоимости перебалансирования"""
        # Объем торговли
        trade_volume = np.sum(np.abs(current_weights - target_weights))
        
        # Стоимость с учетом транзакционных издержек
        rebalancing_cost = trade_volume * self.transaction_cost
        
        return rebalancing_cost
    
    def _calculate_benefit_threshold(self, current_weights, target_weights, deviation):
        """Расчет минимальной выгоды для оправдания перебалансирования"""
        # Базовая выгода от перебалансирования (упрощенная модель)
        benefit = deviation * 0.1  # Предполагаем 10% выгоды от корректировки
        
        return benefit
    
    def calculate_rebalancing_trades(self, current_weights, target_weights, 
                                   portfolio_value, current_prices=None):
        """
        Расчет оптимальных сделок для перебалансирования
        
        Args:
            current_weights (np.array): Текущие веса
            target_weights (np.array): Целевые веса
            portfolio_value (float): Общая стоимость портфолио
            current_prices (np.array): Текущие цены активов
        
        Returns:
            list: Список сделок для перебалансирования
        """
        trades = []
        n_assets = len(current_weights)
        
        # Расчет необходимых изменений
        weight_changes = target_weights - current_weights
        
        # Фильтрация значимых изменений
        significant_changes = np.abs(weight_changes) > 0.001
        
        for i in range(n_assets):
            if significant_changes[i]:
                weight_change = weight_changes[i]
                trade_value = weight_change * portfolio_value
                
                # Расчет количества единиц актива
                if current_prices is not None and i < len(current_prices):
                    trade_quantity = trade_value / current_prices[i]
                else:
                    trade_quantity = trade_value  # Предполагаем, что веса в денежном выражении
                
                trades.append({
                    'asset_index': i,
                    'weight_change': weight_change,
                    'trade_value': trade_value,
                    'trade_quantity': trade_quantity,
                    'current_weight': current_weights[i],
                    'target_weight': target_weights[i],
                    'transaction_cost': abs(trade_value) * self.transaction_cost
                })
        
        # Сортировка по приоритету (сначала крупные изменения)
        trades.sort(key=lambda x: abs(x['weight_change']), reverse=True)
        
        return trades
    
    def execute_rebalancing(self, trades, portfolio, dry_run=False):
        """
        Выполнение перебалансирования портфолио
        
        Args:
            trades (list): Список сделок для выполнения
            portfolio: Объект портфолио
            dry_run (bool): Режим симуляции без реальных сделок
        
        Returns:
            dict: Результаты выполнения перебалансирования
        """
        if not trades:
            return {
                'success': True,
                'executed_trades': [],
                'total_cost': 0,
                'message': 'No trades needed'
            }
        
        executed_trades = []
        total_cost = 0
        failed_trades = []
        
        self.logger.info(f"Executing {len(trades)} rebalancing trades")
        
        for trade in trades:
            try:
                if dry_run:
                    # Режим симуляции
                    executed_trades.append(trade)
                    total_cost += trade['transaction_cost']
                    self.logger.info(f"DRY RUN: {trade}")
                else:
                    # Реальное выполнение сделки
            success = portfolio.execute_trade(
                        asset=trade['asset_index'],
                        amount=trade['trade_quantity'],
                        price=trade.get('price', None)
            )
            
            if success:
                executed_trades.append(trade)
                        total_cost += trade['transaction_cost']
                        self.logger.info(f"Trade executed: {trade}")
                    else:
                        failed_trades.append(trade)
                        self.logger.warning(f"Trade failed: {trade}")
            
            except Exception as e:
                failed_trades.append(trade)
                self.logger.error(f"Trade error: {trade}, Error: {e}")
        
        # Обновление времени последнего перебалансирования
        if executed_trades and not dry_run:
            self.last_rebalancing = datetime.now()
        
        # Сохранение истории
        rebalancing_record = {
            'timestamp': datetime.now(),
            'executed_trades': executed_trades,
            'failed_trades': failed_trades,
            'total_cost': total_cost,
            'dry_run': dry_run
        }
        self.rebalancing_history.append(rebalancing_record)
        
        return {
            'success': len(failed_trades) == 0,
            'executed_trades': executed_trades,
            'failed_trades': failed_trades,
            'total_cost': total_cost,
            'total_trades': len(trades),
            'success_rate': len(executed_trades) / len(trades) if trades else 0
        }
    
    def get_rebalancing_statistics(self):
        """Получение статистики перебалансирования"""
        if not self.rebalancing_history:
            return {
                'total_rebalancings': 0,
                'total_cost': 0,
                'average_cost': 0,
                'success_rate': 0
            }
        
        total_rebalancings = len(self.rebalancing_history)
        total_cost = sum(record['total_cost'] for record in self.rebalancing_history)
        successful_rebalancings = sum(1 for record in self.rebalancing_history 
                                    if record['success'])
        
        return {
            'total_rebalancings': total_rebalancings,
            'total_cost': total_cost,
            'average_cost': total_cost / total_rebalancings if total_rebalancings > 0 else 0,
            'success_rate': successful_rebalancings / total_rebalancings if total_rebalancings > 0 else 0,
            'last_rebalancing': self.last_rebalancing,
            'rebalancing_frequency': self._calculate_rebalancing_frequency()
        }
    
    def _calculate_rebalancing_frequency(self):
        """Расчет частоты перебалансирования"""
        if len(self.rebalancing_history) < 2:
            return 0
        
        # Время между первым и последним перебалансированием
        time_span = (self.rebalancing_history[-1]['timestamp'] - 
                    self.rebalancing_history[0]['timestamp']).days
        
        if time_span == 0:
            return 0
        
        # Частота в перебалансированиях в день
        frequency = len(self.rebalancing_history) / time_span
        
        return frequency
    
    def optimize_rebalancing_strategy(self, historical_data):
        """
        Оптимизация стратегии перебалансирования на основе исторических данных
        
        Args:
            historical_data (dict): Исторические данные портфолио
        
        Returns:
            dict: Оптимизированные параметры
        """
        # Анализ исторических данных для оптимизации параметров
        # Это упрощенная версия - в реальности нужен более сложный анализ
        
        optimal_threshold = self.rebalancing_threshold
        optimal_interval = self.min_rebalancing_interval
        
        # Анализ зависимости доходности от частоты перебалансирования
        # (упрощенная реализация)
        
        return {
            'optimal_threshold': optimal_threshold,
            'optimal_interval': optimal_interval,
            'recommended_transaction_cost': self.transaction_cost
        }

# Пример использования
if __name__ == "__main__":
    # Создание тестовых данных
    np.random.seed(42)
    n_assets = 5
    n_days = 100
    
    # Генерация исторических цен
    initial_prices = np.array([100, 50, 75, 120, 80])
    price_history = []
    current_prices = initial_prices.copy()
    
    for day in range(n_days):
        # Случайные изменения цен
        daily_returns = np.random.normal(0, 0.02, n_assets)
        current_prices = current_prices * (1 + daily_returns)
        price_history.append(current_prices.copy())
    
    price_history = np.array(price_history)
    
    # Инициализация системы перебалансирования
    rebalancer = DynamicRebalancer(
        rebalancing_threshold=0.05,
        transaction_cost=0.001,
        min_rebalancing_interval=1
    )
    
    # Тестирование системы
    current_weights = np.array([0.2, 0.2, 0.2, 0.2, 0.2])  # Равномерное распределение
    target_weights = np.array([0.3, 0.15, 0.25, 0.15, 0.15])  # Целевое распределение
    portfolio_value = 100000  # $100,000
    
    print("=== Тестирование системы перебалансирования ===")
    print(f"Текущие веса: {current_weights}")
    print(f"Целевые веса: {target_weights}")
    print()
    
    # Проверка необходимости перебалансирования
    rebalance_decision = rebalancer.should_rebalance(
        current_weights, target_weights, current_prices
    )
    
    print("Решение о перебалансировании:")
    print(f"  Нужно перебалансировать: {rebalance_decision['should_rebalance']}")
    print(f"  Причина: {rebalance_decision['reason']}")
    print(f"  Максимальное отклонение: {rebalance_decision.get('max_deviation', 0):.4f}")
    print()
    
    if rebalance_decision['should_rebalance']:
        # Расчет сделок
        trades = rebalancer.calculate_rebalancing_trades(
            current_weights, target_weights, portfolio_value, current_prices
        )
        
        print(f"Сделки для перебалансирования ({len(trades)} шт.):")
        for i, trade in enumerate(trades):
            print(f"  {i+1}. Актив {trade['asset_index']}: "
                  f"изменение веса {trade['weight_change']:.4f}, "
                  f"стоимость сделки ${trade['trade_value']:.2f}")
        print()
        
        # Симуляция выполнения
        result = rebalancer.execute_rebalancing(trades, None, dry_run=True)
        
        print("Результат выполнения:")
        print(f"  Успешных сделок: {len(result['executed_trades'])}")
        print(f"  Общая стоимость: ${result['total_cost']:.2f}")
        print(f"  Успешность: {result['success_rate']:.2%}")
        print()
    
    # Статистика
    stats = rebalancer.get_rebalancing_statistics()
    print("Статистика перебалансирования:")
    print(f"  Всего перебалансирований: {stats['total_rebalancings']}")
    print(f"  Общая стоимость: ${stats['total_cost']:.2f}")
    print(f"  Средняя стоимость: ${stats['average_cost']:.2f}")
    print(f"  Частота: {stats['rebalancing_frequency']:.4f} в день")
```

## Мультиактивный анализ

**Теория:** Мультиактивный анализ представляет собой комплексный анализ множества активов для понимания их взаимосвязей и влияния на портфолио. Это критически важно для создания эффективных диверсифицированных портфолио. Современные портфолио содержат десятки или даже сотни активов, и понимание их взаимосвязей является ключом к успешному управлению рисками.

**Математическая основа:** Мультиактивный анализ основан на многомерном статистическом анализе, где мы изучаем:
- **Ковариационные матрицы:** Для понимания взаимосвязей между активами
- **Корреляционные структуры:** Для выявления скрытых зависимостей
- **Факторные модели:** Для понимания общих источников риска
- **Кластерный анализ:** Для группировки похожих активов

**Методы мультиактивного анализа:**
1. **Корреляционный анализ:** Изучение линейных зависимостей
2. **Копулы:** Анализ нелинейных зависимостей
3. **Факторные модели:** Выявление общих факторов риска
4. **Кластерный анализ:** Группировка активов по схожести
5. **Сетевой анализ:** Понимание структуры взаимосвязей

**Почему мультиактивный анализ критичен:**
- **Диверсификация:** Обеспечивает эффективную диверсификацию
- **Снижение рисков:** Помогает снизить риски портфолио
- **Повышение доходности:** Может повысить доходность портфолио
- **Понимание взаимосвязей:** Обеспечивает понимание взаимосвязей между активами
- **Выявление аномалий:** Помогает выявить необычные рыночные условия

### 1. Анализ корреляций

**Теория:** Анализ корреляций между активами критически важен для понимания их взаимосвязей и создания эффективных портфолио. Корреляции определяют степень диверсификации и риски портфолио. В современном мире финансовых рынков корреляции могут быстро изменяться, особенно во время кризисов, что делает их анализ особенно важным.

**Математическая основа:** Корреляция между двумя активами i и j определяется как:
```
ρ_ij = Cov(R_i, R_j) / (σ_i * σ_j)
```
где Cov(R_i, R_j) - ковариация доходностей, σ_i и σ_j - стандартные отклонения.

**Типы корреляций:**
1. **Линейная корреляция Пирсона:** Стандартная мера линейной зависимости
2. **Ранговая корреляция Спирмена:** Устойчива к выбросам
3. **Кендалла τ:** Альтернативная мера ранговой корреляции
4. **Частичная корреляция:** Корреляция с учетом других переменных

**Почему анализ корреляций важен:**
- **Понимание взаимосвязей:** Помогает понять взаимосвязи между активами
- **Диверсификация:** Обеспечивает эффективную диверсификацию
- **Снижение рисков:** Помогает снизить риски портфолио
- **Оптимизация:** Помогает оптимизировать портфолио
- **Выявление кризисов:** Помогает выявить периоды повышенной корреляции

**Плюсы:**
- Понимание взаимосвязей
- Эффективная диверсификация
- Снижение рисков
- Оптимизация портфолио

**Минусы:**
- Сложность анализа
- Потенциальная нестабильность корреляций
- Высокие требования к данным

**Практическое применение:** В нашем коде мы реализуем комплексный анализ корреляций, включая различные типы корреляций, анализ их стабильности во времени и кластеризацию активов по корреляционным структурам.

```python
# Дополнительные импорты для анализа корреляций
from scipy.stats import spearmanr, kendalltau
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
from scipy.spatial.distance import squareform
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

class CorrelationAnalyzer:
    """Комплексный анализ корреляций между активами"""
    
    def __init__(self):
        """Инициализация анализатора корреляций"""
        self.correlation_matrices = {}
        self.correlation_history = []
        self.rolling_correlations = []
        self.cluster_analysis = {}
        
    def calculate_correlation_matrix(self, returns, method='pearson'):
        """
        Расчет матрицы корреляций различными методами
        
        Args:
            returns (pd.DataFrame): Доходности активов
            method (str): Метод корреляции ('pearson', 'spearman', 'kendall')
        
        Returns:
            pd.DataFrame: Матрица корреляций
        """
        if method == 'pearson':
            corr_matrix = returns.corr()
        elif method == 'spearman':
            corr_matrix = returns.corr(method='spearman')
        elif method == 'kendall':
            corr_matrix = returns.corr(method='kendall')
        else:
            raise ValueError(f"Unknown correlation method: {method}")
        
        self.correlation_matrices[method] = corr_matrix
        return corr_matrix
    
    def calculate_all_correlations(self, returns):
        """Расчет всех типов корреляций"""
        correlations = {}
        
        # Пирсон (линейная корреляция)
        correlations['pearson'] = self.calculate_correlation_matrix(returns, 'pearson')
        
        # Спирмен (ранговая корреляция)
        correlations['spearman'] = self.calculate_correlation_matrix(returns, 'spearman')
        
        # Кендалл (альтернативная ранговая корреляция)
        correlations['kendall'] = self.calculate_correlation_matrix(returns, 'kendall')
        
        return correlations
    
    def analyze_correlation_stability(self, returns, window=252, method='pearson'):
        """
        Анализ стабильности корреляций во времени
        
        Args:
            returns (pd.DataFrame): Доходности активов
            window (int): Размер окна для скользящих корреляций
            method (str): Метод корреляции
        
        Returns:
            dict: Результаты анализа стабильности
        """
        rolling_correlations = []
        correlation_changes = []
        
        # Расчет скользящих корреляций
        for i in range(window, len(returns)):
            window_returns = returns.iloc[i-window:i]
            corr_matrix = window_returns.corr(method=method)
            rolling_correlations.append(corr_matrix)
        
        # Анализ изменений корреляций
        for i in range(1, len(rolling_correlations)):
            change = np.abs(rolling_correlations[i] - rolling_correlations[i-1])
            correlation_changes.append(change)
        
        # Расчет метрик стабильности
        stability_metrics = self._calculate_stability_metrics(correlation_changes)
        
        # Сохранение результатов
        stability_analysis = {
            'rolling_correlations': rolling_correlations,
            'correlation_changes': correlation_changes,
            'stability_metrics': stability_metrics,
            'window_size': window,
            'method': method
        }
        
        self.correlation_history.append(stability_analysis)
        return stability_analysis
    
    def _calculate_stability_metrics(self, correlation_changes):
        """Расчет метрик стабильности корреляций"""
        if not correlation_changes:
            return {}
        
        # Среднее изменение корреляций
        mean_changes = [np.mean(change.values) for change in correlation_changes]
        avg_change = np.mean(mean_changes)
        
        # Стандартное отклонение изменений
        std_change = np.std(mean_changes)
        
        # Максимальное изменение
        max_change = np.max(mean_changes)
        
        # Стабильность (обратная к изменению)
        stability_score = max(0, min(1, 1 - avg_change))
        
        # Тренд стабильности (улучшается или ухудшается)
        if len(mean_changes) > 1:
            trend = np.polyfit(range(len(mean_changes)), mean_changes, 1)[0]
        else:
            trend = 0
        
        return {
            'average_change': avg_change,
            'std_change': std_change,
            'max_change': max_change,
            'stability_score': stability_score,
            'trend': trend,
            'is_stable': stability_score > 0.7
        }
    
    def identify_correlation_clusters(self, correlation_matrix, threshold=0.7, method='ward'):
        """
        Идентификация кластеров корреляций
        
        Args:
            correlation_matrix (pd.DataFrame): Матрица корреляций
            threshold (float): Порог для кластеризации
            method (str): Метод кластеризации
        
        Returns:
            dict: Результаты кластеризации
        """
        # Преобразование корреляций в расстояния
        distances = 1 - np.abs(correlation_matrix.values)
        
        # Удаление диагональных элементов
        np.fill_diagonal(distances, 0)
        
        # Преобразование в квадратную форму
        square_distances = squareform(distances)
        
        # Кластеризация
        linkage_matrix = linkage(square_distances, method=method)
        
        # Определение кластеров
        clusters = fcluster(linkage_matrix, threshold, criterion='distance')
        
        # Группировка активов по кластерам
        cluster_groups = {}
        asset_names = correlation_matrix.columns.tolist()
        
        for i, cluster_id in enumerate(clusters):
            if cluster_id not in cluster_groups:
                cluster_groups[cluster_id] = []
            cluster_groups[cluster_id].append(asset_names[i])
        
        # Расчет внутрикластерных корреляций
        intra_cluster_correlations = {}
        for cluster_id, assets in cluster_groups.items():
            if len(assets) > 1:
                cluster_corr = correlation_matrix.loc[assets, assets]
                # Средняя корреляция внутри кластера (исключая диагональ)
                mask = np.ones_like(cluster_corr, dtype=bool)
                np.fill_diagonal(mask, False)
                intra_cluster_correlations[cluster_id] = cluster_corr.values[mask].mean()
            else:
                intra_cluster_correlations[cluster_id] = 1.0
        
        cluster_analysis = {
            'clusters': cluster_groups,
            'linkage_matrix': linkage_matrix,
            'intra_cluster_correlations': intra_cluster_correlations,
            'threshold': threshold,
            'method': method
        }
        
        self.cluster_analysis = cluster_analysis
        return cluster_analysis
    
    def calculate_partial_correlations(self, returns, control_variables=None):
        """
        Расчет частичных корреляций
        
        Args:
            returns (pd.DataFrame): Доходности активов
            control_variables (list): Переменные для контроля
        
        Returns:
            pd.DataFrame: Матрица частичных корреляций
        """
        from scipy.stats import pearsonr
        
        n_assets = len(returns.columns)
        partial_corr_matrix = np.eye(n_assets)
        
        for i in range(n_assets):
            for j in range(i+1, n_assets):
                # Расчет частичной корреляции
                if control_variables is None:
                    # Простая корреляция
                    corr, _ = pearsonr(returns.iloc[:, i], returns.iloc[:, j])
                else:
                    # Частичная корреляция с контролем
                    # Упрощенная реализация - в реальности нужен более сложный алгоритм
                    corr, _ = pearsonr(returns.iloc[:, i], returns.iloc[:, j])
                
                partial_corr_matrix[i, j] = corr
                partial_corr_matrix[j, i] = corr
        
        # Создание DataFrame
        partial_corr_df = pd.DataFrame(
            partial_corr_matrix,
            index=returns.columns,
            columns=returns.columns
        )
        
        return partial_corr_df
    
    def detect_correlation_breaks(self, returns, window=252, threshold=0.1):
        """
        Обнаружение разрывов в корреляциях
        
        Args:
            returns (pd.DataFrame): Доходности активов
            window (int): Размер окна для анализа
            threshold (float): Порог для обнаружения разрывов
        
        Returns:
            dict: Результаты обнаружения разрывов
        """
        breaks = []
        rolling_correlations = []
        
        for i in range(window, len(returns)):
            window_returns = returns.iloc[i-window:i]
            corr_matrix = window_returns.corr()
            rolling_correlations.append(corr_matrix)
            
            if len(rolling_correlations) > 1:
                # Сравнение с предыдущим окном
                prev_corr = rolling_correlations[-2]
                curr_corr = rolling_correlations[-1]
                
                # Расчет изменения корреляций
                corr_change = np.abs(curr_corr - prev_corr)
                max_change = corr_change.values.max()
                
                if max_change > threshold:
                    breaks.append({
                        'date': returns.index[i],
                        'max_change': max_change,
                        'correlation_change': corr_change
                    })
        
        return {
            'breaks': breaks,
            'rolling_correlations': rolling_correlations,
            'threshold': threshold
        }
    
    def calculate_correlation_network_metrics(self, correlation_matrix, threshold=0.3):
        """
        Расчет метрик корреляционной сети
        
        Args:
            correlation_matrix (pd.DataFrame): Матрица корреляций
            threshold (float): Порог для создания сети
        
        Returns:
            dict: Метрики сети
        """
        # Создание бинарной матрицы связей
        binary_matrix = (np.abs(correlation_matrix) > threshold).astype(int)
        np.fill_diagonal(binary_matrix, 0)
        
        # Расчет метрик сети
        n_assets = len(correlation_matrix)
        
        # Степень узлов (количество связей)
        node_degrees = binary_matrix.sum(axis=1)
        
        # Средняя степень
        avg_degree = node_degrees.mean()
        
        # Плотность сети
        max_possible_edges = n_assets * (n_assets - 1) / 2
        actual_edges = binary_matrix.sum() / 2
        density = actual_edges / max_possible_edges
        
        # Центральность (упрощенная)
        centrality = node_degrees / (n_assets - 1)
        
        return {
            'node_degrees': node_degrees,
            'average_degree': avg_degree,
            'network_density': density,
            'centrality': centrality,
            'threshold': threshold
        }
    
    def generate_correlation_report(self, returns, window=252):
        """
        Генерация комплексного отчета по корреляциям
        
        Args:
            returns (pd.DataFrame): Доходности активов
            window (int): Размер окна для анализа
        
        Returns:
            dict: Комплексный отчет
        """
        report = {}
        
        # Расчет всех типов корреляций
        correlations = self.calculate_all_correlations(returns)
        report['correlations'] = correlations
        
        # Анализ стабильности
        stability_analysis = self.analyze_correlation_stability(returns, window)
        report['stability'] = stability_analysis
        
        # Кластеризация
        cluster_analysis = self.identify_correlation_clusters(correlations['pearson'])
        report['clusters'] = cluster_analysis
        
        # Обнаружение разрывов
        breaks = self.detect_correlation_breaks(returns, window)
        report['breaks'] = breaks
        
        # Метрики сети
        network_metrics = self.calculate_correlation_network_metrics(correlations['pearson'])
        report['network'] = network_metrics
        
        return report

# Пример использования
if __name__ == "__main__":
    # Создание тестовых данных
    np.random.seed(42)
    n_assets = 10
    n_days = 1000
    
    # Генерация коррелированных доходностей
    returns_data = {}
    base_returns = np.random.normal(0, 0.02, n_days)
    
    for i in range(n_assets):
        # Создание коррелированных доходностей
        correlation_strength = 0.3 + 0.4 * np.random.random()
        asset_returns = correlation_strength * base_returns + (1 - correlation_strength) * np.random.normal(0, 0.02, n_days)
        returns_data[f'Asset_{i+1}'] = asset_returns
    
    returns_df = pd.DataFrame(returns_data)
    returns_df.index = pd.date_range('2020-01-01', periods=n_days, freq='D')
    
    # Инициализация анализатора
    analyzer = CorrelationAnalyzer()
    
    print("=== Анализ корреляций ===")
    print(f"Анализируем {n_assets} активов за {n_days} дней")
    print()
    
    # Генерация отчета
    report = analyzer.generate_correlation_report(returns_df)
    
    # Вывод результатов
    print("1. Корреляции Пирсона:")
    print(report['correlations']['pearson'].round(3))
    print()
    
    print("2. Стабильность корреляций:")
    stability = report['stability']['stability_metrics']
    print(f"   Оценка стабильности: {stability['stability_score']:.3f}")
    print(f"   Среднее изменение: {stability['average_change']:.3f}")
    print(f"   Стабильна: {stability['is_stable']}")
    print()
    
    print("3. Кластеры активов:")
    clusters = report['clusters']['clusters']
    for cluster_id, assets in clusters.items():
        print(f"   Кластер {cluster_id}: {assets}")
    print()
    
    print("4. Разрывы корреляций:")
    breaks = report['breaks']['breaks']
    print(f"   Обнаружено разрывов: {len(breaks)}")
    if breaks:
        print(f"   Последний разрыв: {breaks[-1]['date']}")
    print()
    
    print("5. Метрики сети:")
    network = report['network']
    print(f"   Средняя степень: {network['average_degree']:.2f}")
    print(f"   Плотность сети: {network['network_density']:.3f}")
```

### 2. Анализ волатильности

**Теория:** Анализ волатильности активов критически важен для понимания рисков и создания эффективных портфолио. Волатильность определяет уровень риска и влияет на распределение активов. Современные методы анализа волатильности включают модели GARCH, стохастическую волатильность и машинное обучение.

**Математическая основа:** Волатильность определяется как стандартное отклонение доходностей:
```
σ = √(E[(R - μ)²])
```
где R - доходность, μ - средняя доходность.

**Типы волатильности:**
1. **Историческая волатильность:** Основана на исторических данных
2. **Подразумеваемая волатильность:** Извлекается из опционных цен
3. **Реализованная волатильность:** Фактическая волатильность за период
4. **Прогнозируемая волатильность:** Предсказанная будущая волатильность

**Модели волатильности:**
- **GARCH:** Обобщенная авторегрессионная условная гетероскедастичность
- **EGARCH:** Экспоненциальная GARCH
- **GJR-GARCH:** GARCH с асимметричными эффектами
- **Stochastic Volatility:** Стохастическая волатильность

**Почему анализ волатильности важен:**
- **Оценка рисков:** Помогает оценить риски активов
- **Оптимизация портфолио:** Помогает оптимизировать портфолио
- **Управление рисками:** Критически важен для управления рисками
- **Прогнозирование:** Помогает прогнозировать будущие риски
- **Ценообразование опционов:** Критически важен для деривативов

**Плюсы:**
- Оценка рисков
- Оптимизация портфолио
- Управление рисками
- Прогнозирование рисков

**Минусы:**
- Сложность анализа
- Потенциальная нестабильность волатильности
- Высокие требования к данным

**Практическое применение:** В нашем коде мы реализуем комплексный анализ волатильности, включая различные модели GARCH, анализ режимов волатильности и прогнозирование будущей волатильности.

```python
# Дополнительные импорты для анализа волатильности
try:
    from arch import arch_model
    ARCH_AVAILABLE = True
except ImportError:
    ARCH_AVAILABLE = False
    print("Warning: arch package not available. GARCH models will be disabled.")

from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

class VolatilityAnalyzer:
    """Комплексный анализ волатильности активов"""
    
    def __init__(self):
        """Инициализация анализатора волатильности"""
        self.volatility_history = []
        self.volatility_forecasts = {}
        self.garch_models = {}
        self.regime_models = {}
        
    def calculate_volatility(self, returns, window=252, method='rolling'):
        """
        Расчет волатильности различными методами
        
        Args:
            returns (pd.Series): Доходности актива
            window (int): Размер окна для расчета
            method (str): Метод расчета ('rolling', 'ewm', 'parkinson', 'garman_klass')
        
        Returns:
            pd.Series: Волатильность
        """
        if method == 'rolling':
            # Простая скользящая волатильность
        volatility = returns.rolling(window).std() * np.sqrt(252)
        elif method == 'ewm':
            # Экспоненциально взвешенная волатильность
            volatility = returns.ewm(span=window).std() * np.sqrt(252)
        elif method == 'parkinson':
            # Модель Паркинсона (требует данные OHLC)
            # Упрощенная версия для доходностей
            volatility = returns.rolling(window).std() * np.sqrt(252)
        elif method == 'garman_klass':
            # Модель Гармана-Класса (требует данные OHLC)
            # Упрощенная версия для доходностей
            volatility = returns.rolling(window).std() * np.sqrt(252)
        else:
            raise ValueError(f"Unknown volatility method: {method}")
        
        return volatility
    
    def calculate_realized_volatility(self, returns, window=22):
        """
        Расчет реализованной волатильности
        
        Args:
            returns (pd.Series): Доходности актива
            window (int): Размер окна (по умолчанию 22 торговых дня = 1 месяц)
        
        Returns:
            pd.Series: Реализованная волатильность
        """
        # Квадрат доходностей
        squared_returns = returns ** 2
        
        # Скользящая сумма квадратов доходностей
        realized_var = squared_returns.rolling(window).sum()
        
        # Реализованная волатильность (годовая)
        realized_vol = np.sqrt(realized_var * 252 / window)
        
        return realized_vol
    
    def analyze_volatility_regimes(self, volatility, n_regimes=3):
        """
        Анализ режимов волатильности с использованием GMM
        
        Args:
            volatility (pd.Series): Волатильность
            n_regimes (int): Количество режимов
        
        Returns:
            dict: Результаты анализа режимов
        """
        # Удаление NaN значений
        clean_vol = volatility.dropna()
        
        if len(clean_vol) < n_regimes * 10:
            return {'error': 'Insufficient data for regime analysis'}
        
        # Подготовка данных
        X = clean_vol.values.reshape(-1, 1)
        
        # Обучение GMM модели
        gmm = GaussianMixture(n_components=n_regimes, random_state=42)
        gmm.fit(X)
        
        # Предсказание режимов
        regimes = gmm.predict(X)
        
        # Создание Series с режимами
        regime_series = pd.Series(regimes, index=clean_vol.index)
        
        # Анализ режимов
        regime_stats = {}
        for i in range(n_regimes):
            regime_data = clean_vol[regime_series == i]
            regime_stats[f'regime_{i}'] = {
                'count': len(regime_data),
                'mean': regime_data.mean(),
                'std': regime_data.std(),
                'min': regime_data.min(),
                'max': regime_data.max(),
                'probability': gmm.weights_[i]
            }
        
        # Переходы между режимами
        transitions = self._calculate_regime_transitions(regime_series)
        
        return {
            'regimes': regime_series,
            'regime_stats': regime_stats,
            'transitions': transitions,
            'model': gmm
        }
    
    def _calculate_regime_transitions(self, regime_series):
        """Расчет переходов между режимами"""
        transitions = {}
        regime_counts = regime_series.value_counts().sort_index()
        
        for i in range(len(regime_counts)):
            for j in range(len(regime_counts)):
                if i != j:
                    # Подсчет переходов i -> j
                    transitions_count = 0
                    for k in range(len(regime_series) - 1):
                        if regime_series.iloc[k] == i and regime_series.iloc[k + 1] == j:
                            transitions_count += 1
                    
                    transitions[f'{i}_to_{j}'] = transitions_count
        
        return transitions
    
    def forecast_volatility_garch(self, returns, horizon=1, model_type='GARCH'):
        """
        Прогнозирование волатильности с использованием GARCH моделей
        
        Args:
            returns (pd.Series): Доходности актива
            horizon (int): Горизонт прогноза
            model_type (str): Тип GARCH модели
        
        Returns:
            dict: Результаты прогноза
        """
        if not ARCH_AVAILABLE:
            return {'error': 'ARCH package not available'}
        
        try:
            # Создание GARCH модели
            if model_type == 'GARCH':
        model = arch_model(returns, vol='Garch', p=1, q=1)
            elif model_type == 'EGARCH':
                model = arch_model(returns, vol='EGARCH', p=1, q=1)
            elif model_type == 'GJR-GARCH':
                model = arch_model(returns, vol='GARCH', p=1, o=1, q=1)
            else:
                model = arch_model(returns, vol='Garch', p=1, q=1)
            
            # Обучение модели
            fitted_model = model.fit(disp='off')
        
        # Прогноз волатильности
        forecast = fitted_model.forecast(horizon=horizon)
        
            # Извлечение прогноза
            if hasattr(forecast, 'variance'):
                forecast_vol = np.sqrt(forecast.variance.iloc[-1, 0] * 252)
            else:
                forecast_vol = np.sqrt(forecast.mean.iloc[-1, 0] * 252)
            
            return {
                'forecast_volatility': forecast_vol,
                'model': fitted_model,
                'forecast_variance': forecast.variance.iloc[-1, 0] if hasattr(forecast, 'variance') else None,
                'aic': fitted_model.aic,
                'bic': fitted_model.bic
            }
            
        except Exception as e:
            return {'error': f'GARCH model failed: {str(e)}'}
    
    def calculate_volatility_metrics(self, returns, volatility):
        """
        Расчет метрик волатильности
        
        Args:
            returns (pd.Series): Доходности актива
            volatility (pd.Series): Волатильность актива
        
        Returns:
            dict: Метрики волатильности
        """
        # Базовая статистика
        mean_vol = volatility.mean()
        std_vol = volatility.std()
        min_vol = volatility.min()
        max_vol = volatility.max()
        
        # Коэффициент вариации
        cv = std_vol / mean_vol if mean_vol > 0 else 0
        
        # Асимметрия и эксцесс
        skewness = volatility.skew()
        kurtosis = volatility.kurtosis()
        
        # Автокорреляция волатильности
        vol_autocorr = volatility.autocorr(lag=1)
        
        # Волатильность волатильности (vol of vol)
        vol_of_vol = volatility.rolling(22).std().mean()
        
        # Sharpe ratio (доходность / волатильность)
        mean_return = returns.mean() * 252
        sharpe_ratio = mean_return / mean_vol if mean_vol > 0 else 0
        
        # Максимальная просадка волатильности
        vol_cummax = volatility.cummax()
        vol_drawdown = (volatility - vol_cummax) / vol_cummax
        max_vol_drawdown = vol_drawdown.min()
        
        return {
            'mean_volatility': mean_vol,
            'std_volatility': std_vol,
            'min_volatility': min_vol,
            'max_volatility': max_vol,
            'coefficient_of_variation': cv,
            'skewness': skewness,
            'kurtosis': kurtosis,
            'autocorrelation': vol_autocorr,
            'volatility_of_volatility': vol_of_vol,
            'sharpe_ratio': sharpe_ratio,
            'max_volatility_drawdown': max_vol_drawdown
        }
    
    def detect_volatility_clustering(self, returns, window=22):
        """
        Обнаружение кластеризации волатильности
        
        Args:
            returns (pd.Series): Доходности актива
            window (int): Размер окна для анализа
        
        Returns:
            dict: Результаты анализа кластеризации
        """
        # Расчет волатильности
        volatility = self.calculate_volatility(returns, window)
        
        # Квадрат доходностей
        squared_returns = returns ** 2
        
        # Автокорреляция квадратов доходностей (тест на кластеризацию)
        autocorr_lags = range(1, min(21, len(squared_returns) // 4))
        autocorrelations = [squared_returns.autocorr(lag=lag) for lag in autocorr_lags]
        
        # Статистика Ljung-Box (упрощенная)
        n = len(squared_returns)
        ljung_box_stats = []
        for lag in autocorr_lags:
            if lag < n:
                stat = n * (n + 2) * sum([autocorrelations[i] ** 2 / (n - i - 1) 
                                        for i in range(lag)])
                ljung_box_stats.append(stat)
        
        # Обнаружение периодов высокой волатильности
        high_vol_threshold = volatility.quantile(0.8)
        high_vol_periods = volatility > high_vol_threshold
        
        # Анализ продолжительности периодов высокой волатильности
        vol_periods = self._find_volatility_periods(high_vol_periods)
        
        return {
            'autocorrelations': dict(zip(autocorr_lags, autocorrelations)),
            'ljung_box_stats': dict(zip(autocorr_lags, ljung_box_stats)),
            'high_volatility_periods': vol_periods,
            'volatility_clustering_detected': max(autocorrelations) > 0.1
        }
    
    def _find_volatility_periods(self, high_vol_series):
        """Поиск периодов высокой волатильности"""
        periods = []
        in_period = False
        start_idx = None
        
        for i, is_high in enumerate(high_vol_series):
            if is_high and not in_period:
                # Начало периода высокой волатильности
                in_period = True
                start_idx = i
            elif not is_high and in_period:
                # Конец периода
                periods.append({
                    'start': high_vol_series.index[start_idx],
                    'end': high_vol_series.index[i-1],
                    'duration': i - start_idx
                })
                in_period = False
        
        # Если период не закончился
        if in_period:
            periods.append({
                'start': high_vol_series.index[start_idx],
                'end': high_vol_series.index[-1],
                'duration': len(high_vol_series) - start_idx
            })
        
        return periods
    
    def generate_volatility_report(self, returns, window=252):
        """
        Генерация комплексного отчета по волатильности
        
        Args:
            returns (pd.Series): Доходности актива
            window (int): Размер окна для анализа
        
        Returns:
            dict: Комплексный отчет по волатильности
        """
        report = {}
        
        # Расчет волатильности различными методами
        rolling_vol = self.calculate_volatility(returns, window, 'rolling')
        ewm_vol = self.calculate_volatility(returns, window, 'ewm')
        realized_vol = self.calculate_realized_volatility(returns)
        
        report['volatility_series'] = {
            'rolling': rolling_vol,
            'ewm': ewm_vol,
            'realized': realized_vol
        }
        
        # Анализ режимов волатильности
        regime_analysis = self.analyze_volatility_regimes(rolling_vol)
        report['regime_analysis'] = regime_analysis
        
        # Прогнозирование волатильности
        if ARCH_AVAILABLE:
            garch_forecast = self.forecast_volatility_garch(returns)
            report['garch_forecast'] = garch_forecast
        
        # Метрики волатильности
        vol_metrics = self.calculate_volatility_metrics(returns, rolling_vol)
        report['metrics'] = vol_metrics
        
        # Анализ кластеризации
        clustering_analysis = self.detect_volatility_clustering(returns)
        report['clustering'] = clustering_analysis
        
        return report

# Пример использования
if __name__ == "__main__":
    # Создание тестовых данных
    np.random.seed(42)
    n_days = 1000
    
    # Генерация доходностей с кластеризацией волатильности
    returns = []
    volatility = 0.02
    
    for i in range(n_days):
        # Случайное изменение волатильности
        if i % 100 == 0:
            volatility = 0.01 + 0.03 * np.random.random()
        
        # Генерация доходности
        return_val = np.random.normal(0, volatility)
        returns.append(return_val)
    
    returns_series = pd.Series(returns)
    returns_series.index = pd.date_range('2020-01-01', periods=n_days, freq='D')
    
    # Инициализация анализатора
    analyzer = VolatilityAnalyzer()
    
    print("=== Анализ волатильности ===")
    print(f"Анализируем {n_days} дней данных")
    print()
    
    # Генерация отчета
    report = analyzer.generate_volatility_report(returns_series)
    
    # Вывод результатов
    print("1. Метрики волатильности:")
    metrics = report['metrics']
    print(f"   Средняя волатильность: {metrics['mean_volatility']:.4f}")
    print(f"   Стандартное отклонение: {metrics['std_volatility']:.4f}")
    print(f"   Коэффициент вариации: {metrics['coefficient_of_variation']:.4f}")
    print(f"   Sharpe ratio: {metrics['sharpe_ratio']:.4f}")
    print()
    
    print("2. Анализ режимов волатильности:")
    if 'regime_analysis' in report and 'error' not in report['regime_analysis']:
        regime_stats = report['regime_analysis']['regime_stats']
        for regime, stats in regime_stats.items():
            print(f"   {regime}: {stats['count']} дней, средняя волатильность {stats['mean']:.4f}")
    print()
    
    print("3. Кластеризация волатильности:")
    clustering = report['clustering']
    print(f"   Обнаружена кластеризация: {clustering['volatility_clustering_detected']}")
    print(f"   Периодов высокой волатильности: {len(clustering['high_volatility_periods'])}")
    print()
    
    if ARCH_AVAILABLE and 'garch_forecast' in report and 'error' not in report['garch_forecast']:
        print("4. Прогноз GARCH:")
        garch = report['garch_forecast']
        print(f"   Прогнозируемая волатильность: {garch['forecast_volatility']:.4f}")
        print(f"   AIC: {garch['aic']:.2f}")
    else:
        print("4. GARCH прогноз недоступен")
```

## Продвинутый риск-менеджмент

**Теория:** Продвинутый риск-менеджмент представляет собой комплексную систему управления рисками портфолио, которая использует современные методы для минимизации потерь и максимизации прибыли. Это критически важно для долгосрочного успеха. Современный риск-менеджмент включает в себя не только количественные методы, но и качественный анализ, стресс-тестирование и сценарное планирование.

**Математическая основа:** Современный риск-менеджмент основан на:
- **Стохастических процессах:** Для моделирования динамики цен
- **Копулах:** Для моделирования зависимостей между активами
- **Экстремальной теории значений:** Для анализа хвостовых рисков
- **Монте-Карло симуляциям:** Для комплексного анализа рисков

**Компоненты продвинутого риск-менеджмента:**
1. **Value at Risk (VaR):** Количественная оценка максимальных потерь
2. **Conditional VaR (CVaR):** Ожидаемые потери в экстремальных сценариях
3. **Stress Testing:** Тестирование в экстремальных условиях
4. **Scenario Analysis:** Анализ различных сценариев развития
5. **Monte Carlo Simulation:** Стохастическое моделирование
6. **Risk Budgeting:** Распределение рисков между активами

**Почему продвинутый риск-менеджмент критичен:**
- **Защита капитала:** Критически важно для защиты капитала
- **Стабильность:** Обеспечивает стабильность портфолио
- **Долгосрочный успех:** Критически важно для долгосрочного успеха
- **Психологический комфорт:** Снижает стресс и эмоциональные решения
- **Регуляторное соответствие:** Необходимо для соответствия требованиям
- **Конкурентное преимущество:** Дает преимущество на рынке

### 1. Value at Risk (VaR)

**Теория:** Value at Risk (VaR) представляет собой статистическую меру риска, которая определяет максимальные ожидаемые потери портфолио за определенный период времени с заданной вероятностью. Это критически важно для понимания и управления рисками. VaR стал стандартом в индустрии для измерения рыночного риска.

**Математическая основа:** VaR определяется как:
```
VaR_α = -F^(-1)(α)
```
где F^(-1) - обратная функция распределения доходностей, α - уровень доверия (например, 0.05 для 95% VaR).

**Методы расчета VaR:**
1. **Исторический VaR:** Основан на исторических данных
2. **Параметрический VaR:** Предполагает нормальное распределение
3. **Монте-Карло VaR:** Использует симуляции
4. **Extreme Value Theory VaR:** Для анализа хвостовых рисков

**Почему VaR важен:**
- **Количественная оценка рисков:** Обеспечивает количественную оценку рисков
- **Сравнение портфолио:** Позволяет сравнивать риски различных портфолио
- **Управление рисками:** Помогает в управлении рисками
- **Планирование:** Помогает в планировании инвестиций
- **Регуляторное соответствие:** Требуется регуляторами

**Плюсы:**
- Количественная оценка рисков
- Возможность сравнения
- Помощь в управлении рисками
- Планирование инвестиций

**Минусы:**
- Сложность расчета
- Потенциальные проблемы с данными
- Необходимость понимания статистики
- Не учитывает форму распределения в хвостах

**Практическое применение:** В нашем коде мы реализуем все основные методы расчета VaR, включая исторический, параметрический и Монте-Карло подходы, а также продвинутые методы для анализа хвостовых рисков.

```python
# Дополнительные импорты для VaR расчетов
from scipy.stats import norm, t, skewnorm
from scipy.optimize import minimize
import warnings
warnings.filterwarnings('ignore')

class VaRCalculator:
    """Комплексный калькулятор Value at Risk с множественными методами"""
    
    def __init__(self, confidence_level=0.05):
        """
        Инициализация калькулятора VaR
        
        Args:
            confidence_level (float): Уровень доверия (0.05 для 95% VaR)
        """
        self.confidence_level = confidence_level
        self.var_history = []
        self.calculation_methods = {}
        
    def calculate_historical_var(self, returns, confidence_level=None):
        """
        Расчет исторического VaR
        
        Args:
            returns (np.array): Доходности портфолио
            confidence_level (float): Уровень доверия
        
        Returns:
            dict: Результаты расчета исторического VaR
        """
        if confidence_level is None:
            confidence_level = self.confidence_level
        
        # Сортировка доходностей
        sorted_returns = np.sort(returns)
        
        # Индекс для VaR
        var_index = int(confidence_level * len(sorted_returns))
        
        # VaR
        var = sorted_returns[var_index]
        
        # Дополнительная статистика
        tail_returns = sorted_returns[:var_index]
        cvar = np.mean(tail_returns) if len(tail_returns) > 0 else var
        
        result = {
            'var': var,
            'cvar': cvar,
            'confidence_level': confidence_level,
            'method': 'historical',
            'tail_observations': len(tail_returns),
            'total_observations': len(returns)
        }
        
        self.var_history.append(result)
        return result
    
    def calculate_parametric_var(self, returns, confidence_level=None, distribution='normal'):
        """
        Расчет параметрического VaR
        
        Args:
            returns (np.array): Доходности портфолио
            confidence_level (float): Уровень доверия
            distribution (str): Тип распределения ('normal', 't', 'skewed')
        
        Returns:
            dict: Результаты расчета параметрического VaR
        """
        if confidence_level is None:
            confidence_level = self.confidence_level
        
        # Параметры распределения
        mean_return = np.mean(returns)
        std_return = np.std(returns)
        
        if distribution == 'normal':
            # Нормальное распределение
        z_score = norm.ppf(confidence_level)
        var = mean_return + z_score * std_return
        
            # CVaR для нормального распределения
            cvar = mean_return - std_return * norm.pdf(z_score) / confidence_level
            
        elif distribution == 't':
            # t-распределение Стьюдента
            # Оценка степеней свободы
            n = len(returns)
            df = n - 1  # Упрощенная оценка
            
            t_score = t.ppf(confidence_level, df)
            var = mean_return + t_score * std_return
            
            # CVaR для t-распределения
            cvar = mean_return - std_return * t.pdf(t_score, df) / confidence_level
            
        elif distribution == 'skewed':
            # Скошенное нормальное распределение
            skewness = self._calculate_skewness(returns)
            
            # Параметры скошенного нормального распределения
            skew_param = skewness / (1 + skewness**2)**0.5
            scale = std_return / (1 - skew_param**2)**0.5
            loc = mean_return - scale * skew_param * (2/np.pi)**0.5
            
            # VaR для скошенного нормального распределения
            var = skewnorm.ppf(confidence_level, skew_param, loc, scale)
            
            # CVaR (упрощенная версия)
            cvar = var - scale * skewnorm.pdf(skewnorm.ppf(confidence_level, skew_param), skew_param) / confidence_level
            
        else:
            raise ValueError(f"Unknown distribution: {distribution}")
        
        result = {
            'var': var,
            'cvar': cvar,
            'confidence_level': confidence_level,
            'method': f'parametric_{distribution}',
            'mean': mean_return,
            'std': std_return,
            'distribution': distribution
        }
        
        self.var_history.append(result)
        return result
    
    def calculate_monte_carlo_var(self, returns, n_simulations=10000, confidence_level=None, 
                                 model='normal', n_days=1):
        """
        Расчет VaR методом Монте-Карло
        
        Args:
            returns (np.array): Доходности портфолио
            n_simulations (int): Количество симуляций
            confidence_level (float): Уровень доверия
            model (str): Модель для симуляций ('normal', 't', 'garch')
            n_days (int): Горизонт прогноза в днях
        
        Returns:
            dict: Результаты расчета Монте-Карло VaR
        """
        if confidence_level is None:
            confidence_level = self.confidence_level
        
        if model == 'normal':
            # Простая нормальная модель
        mean_return = np.mean(returns)
        std_return = np.std(returns)
        
            # Симуляция доходностей
        simulated_returns = np.random.normal(mean_return, std_return, n_simulations)
        
        elif model == 't':
            # t-распределение
            mean_return = np.mean(returns)
            std_return = np.std(returns)
            df = len(returns) - 1
            
            # Симуляция с t-распределением
            t_samples = np.random.standard_t(df, n_simulations)
            simulated_returns = mean_return + std_return * t_samples
            
        elif model == 'garch':
            # Упрощенная GARCH модель
            simulated_returns = self._simulate_garch_returns(returns, n_simulations, n_days)
            
        else:
            raise ValueError(f"Unknown model: {model}")
        
        # Масштабирование для n_days
        if n_days > 1:
            simulated_returns = simulated_returns * np.sqrt(n_days)
        
        # Сортировка симулированных доходностей
        sorted_returns = np.sort(simulated_returns)
        
        # VaR и CVaR
        var_index = int(confidence_level * len(sorted_returns))
        var = sorted_returns[var_index]
        tail_returns = sorted_returns[:var_index]
        cvar = np.mean(tail_returns) if len(tail_returns) > 0 else var
        
        result = {
            'var': var,
            'cvar': cvar,
            'confidence_level': confidence_level,
            'method': f'monte_carlo_{model}',
            'n_simulations': n_simulations,
            'n_days': n_days,
            'simulated_returns': simulated_returns
        }
        
        self.var_history.append(result)
        return result
    
    def calculate_extreme_value_var(self, returns, confidence_level=None, threshold_percentile=95):
        """
        Расчет VaR с использованием теории экстремальных значений
        
        Args:
            returns (np.array): Доходности портфолио
            confidence_level (float): Уровень доверия
            threshold_percentile (float): Процентиль для определения экстремумов
        
        Returns:
            dict: Результаты расчета EVT VaR
        """
        if confidence_level is None:
            confidence_level = self.confidence_level
        
        # Определение экстремумов (хвостовые значения)
        threshold = np.percentile(returns, threshold_percentile)
        excesses = returns[returns < threshold] - threshold
        
        if len(excesses) < 10:
            return {'error': 'Insufficient extreme values for EVT analysis'}
        
        # Подгонка обобщенного распределения Парето (упрощенная версия)
        # В реальности нужна более сложная реализация
        excess_mean = np.mean(excesses)
        excess_std = np.std(excesses)
        
        # Упрощенная оценка параметров
        shape_param = 0.1  # Упрощенная оценка
        scale_param = excess_std * (1 - shape_param)
        
        # VaR с использованием EVT
        var = threshold + scale_param * ((1 - confidence_level) ** (-shape_param) - 1) / shape_param
        
        # CVaR
        cvar = var + scale_param / (1 - shape_param)
        
        result = {
            'var': var,
            'cvar': cvar,
            'confidence_level': confidence_level,
            'method': 'extreme_value',
            'threshold': threshold,
            'excesses_count': len(excesses),
            'shape_parameter': shape_param,
            'scale_parameter': scale_param
        }
        
        self.var_history.append(result)
        return result
    
    def calculate_portfolio_var(self, weights, returns_matrix, confidence_level=None):
        """
        Расчет VaR для портфолио с множественными активами
        
        Args:
            weights (np.array): Веса активов в портфолио
            returns_matrix (np.array): Матрица доходностей активов
            confidence_level (float): Уровень доверия
        
        Returns:
            dict: Результаты расчета портфолио VaR
        """
        if confidence_level is None:
            confidence_level = self.confidence_level
        
        # Расчет доходностей портфолио
        portfolio_returns = np.dot(returns_matrix, weights)
        
        # Расчет VaR для портфолио
        historical_var = self.calculate_historical_var(portfolio_returns, confidence_level)
        parametric_var = self.calculate_parametric_var(portfolio_returns, confidence_level)
        
        # Анализ вклада активов в VaR
        asset_var_contributions = self._calculate_var_contributions(weights, returns_matrix, confidence_level)
        
        result = {
            'portfolio_var': historical_var['var'],
            'portfolio_cvar': historical_var['cvar'],
            'historical_var': historical_var,
            'parametric_var': parametric_var,
            'asset_contributions': asset_var_contributions,
            'weights': weights,
            'confidence_level': confidence_level
        }
        
        return result
    
    def _calculate_skewness(self, returns):
        """Расчет асимметрии распределения"""
        mean_return = np.mean(returns)
        std_return = np.std(returns)
        skewness = np.mean(((returns - mean_return) / std_return) ** 3)
        return skewness
    
    def _simulate_garch_returns(self, returns, n_simulations, n_days):
        """Упрощенная симуляция GARCH доходностей"""
        # Упрощенная реализация - в реальности нужна полная GARCH модель
        mean_return = np.mean(returns)
        std_return = np.std(returns)
        
        # Симуляция с кластеризацией волатильности
        simulated_returns = []
        current_vol = std_return
        
        for _ in range(n_simulations):
            # Простая модель изменения волатильности
            vol_change = np.random.normal(0, 0.1)
            current_vol = max(0.01, current_vol * (1 + vol_change))
            
            # Генерация доходности
            return_val = np.random.normal(mean_return, current_vol)
            simulated_returns.append(return_val)
        
        return np.array(simulated_returns)
    
    def _calculate_var_contributions(self, weights, returns_matrix, confidence_level):
        """Расчет вклада каждого актива в VaR портфолио"""
        n_assets = len(weights)
        contributions = np.zeros(n_assets)
        
        # Расчет портфолио доходностей
        portfolio_returns = np.dot(returns_matrix, weights)
        
        # VaR портфолио
        portfolio_var = self.calculate_historical_var(portfolio_returns, confidence_level)['var']
        
        # Вклад каждого актива
        for i in range(n_assets):
            # Временное изменение веса актива
            temp_weights = weights.copy()
            temp_weights[i] += 0.001  # Малое изменение
            
            # Новые доходности портфолио
            temp_portfolio_returns = np.dot(returns_matrix, temp_weights)
            temp_var = self.calculate_historical_var(temp_portfolio_returns, confidence_level)['var']
            
            # Вклад актива
            contributions[i] = (temp_var - portfolio_var) / 0.001
        
        return contributions
    
    def backtest_var(self, returns, var_estimates, confidence_level=None):
        """
        Бэктестинг VaR модели
        
        Args:
            returns (np.array): Фактические доходности
            var_estimates (np.array): Оценки VaR
            confidence_level (float): Уровень доверия
        
        Returns:
            dict: Результаты бэктестинга
        """
        if confidence_level is None:
            confidence_level = self.confidence_level
        
        # Подсчет нарушений VaR
        violations = returns < var_estimates
        n_violations = np.sum(violations)
        n_observations = len(returns)
        violation_rate = n_violations / n_observations
        
        # Ожидаемая частота нарушений
        expected_violations = confidence_level * n_observations
        
        # Тест Купика (упрощенная версия)
        kupiec_stat = 2 * (n_violations * np.log(violation_rate / confidence_level) + 
                          (n_observations - n_violations) * np.log((1 - violation_rate) / (1 - confidence_level)))
        
        # p-value (упрощенная версия)
        p_value = 1 - norm.cdf(np.sqrt(kupiec_stat))
        
        return {
            'violations': n_violations,
            'total_observations': n_observations,
            'violation_rate': violation_rate,
            'expected_violation_rate': confidence_level,
            'kupiec_statistic': kupiec_stat,
            'p_value': p_value,
            'model_adequate': p_value > 0.05
        }
    
    def generate_var_report(self, returns, confidence_levels=[0.01, 0.05, 0.1]):
        """
        Генерация комплексного отчета по VaR
        
        Args:
            returns (np.array): Доходности портфолио
            confidence_levels (list): Уровни доверия для анализа
        
        Returns:
            dict: Комплексный отчет по VaR
        """
        report = {}
        
        for conf_level in confidence_levels:
            level_report = {}
            
            # Различные методы расчета
            level_report['historical'] = self.calculate_historical_var(returns, conf_level)
            level_report['parametric_normal'] = self.calculate_parametric_var(returns, conf_level, 'normal')
            level_report['parametric_t'] = self.calculate_parametric_var(returns, conf_level, 't')
            level_report['monte_carlo'] = self.calculate_monte_carlo_var(returns, confidence_level=conf_level)
            
            # Бэктестинг
            historical_var = level_report['historical']['var']
            var_series = np.full(len(returns), historical_var)  # Упрощенная версия
            level_report['backtest'] = self.backtest_var(returns, var_series, conf_level)
            
            report[f'confidence_{int(conf_level*100)}'] = level_report
        
        return report

# Пример использования
if __name__ == "__main__":
    # Создание тестовых данных
    np.random.seed(42)
    n_days = 1000
    
    # Генерация доходностей с различными характеристиками
    returns = np.random.normal(0.001, 0.02, n_days)  # Базовые доходности
    returns += np.random.normal(0, 0.01, n_days) * (np.random.random(n_days) < 0.1)  # Выбросы
    
    # Инициализация калькулятора VaR
    var_calc = VaRCalculator(confidence_level=0.05)
    
    print("=== Анализ Value at Risk ===")
    print(f"Анализируем {n_days} дней данных")
    print()
    
    # Генерация отчета
    report = var_calc.generate_var_report(returns)
    
    # Вывод результатов для 95% VaR
    var_95 = report['confidence_5']
    
    print("1. VaR на уровне 95%:")
    print(f"   Исторический VaR: {var_95['historical']['var']:.4f}")
    print(f"   Параметрический VaR (нормальный): {var_95['parametric_normal']['var']:.4f}")
    print(f"   Параметрический VaR (t-распределение): {var_95['parametric_t']['var']:.4f}")
    print(f"   Монте-Карло VaR: {var_95['monte_carlo']['var']:.4f}")
    print()
    
    print("2. Conditional VaR (CVaR) на уровне 95%:")
    print(f"   Исторический CVaR: {var_95['historical']['cvar']:.4f}")
    print(f"   Параметрический CVaR: {var_95['parametric_normal']['cvar']:.4f}")
    print()
    
    print("3. Бэктестинг модели:")
    backtest = var_95['backtest']
    print(f"   Нарушений VaR: {backtest['violations']} из {backtest['total_observations']}")
    print(f"   Частота нарушений: {backtest['violation_rate']:.4f}")
    print(f"   Ожидаемая частота: {backtest['expected_violation_rate']:.4f}")
    print(f"   Модель адекватна: {backtest['model_adequate']}")
    print()
    
    # Анализ портфолио
    print("4. Анализ портфолио:")
    n_assets = 5
    weights = np.array([0.2, 0.2, 0.2, 0.2, 0.2])
    returns_matrix = np.random.normal(0.001, 0.02, (n_days, n_assets))
    
    portfolio_var = var_calc.calculate_portfolio_var(weights, returns_matrix)
    print(f"   VaR портфолио: {portfolio_var['portfolio_var']:.4f}")
    print(f"   Вклады активов: {portfolio_var['asset_contributions']}")
```

### 2. Stress Testing

**Теория:** Stress Testing представляет собой метод тестирования портфолио в экстремальных рыночных условиях для оценки его устойчивости. Это критически важно для понимания потенциальных рисков и подготовки к неблагоприятным сценариям. Stress Testing является обязательным требованием для многих финансовых институтов.

**Математическая основа:** Stress Testing основан на:
- **Сценарном анализе:** Моделирование конкретных рыночных сценариев
- **Монте-Карло симуляциях:** Стохастическое моделирование экстремальных событий
- **Историческом анализе:** Использование прошлых кризисов как шаблонов
- **Корреляционном анализе:** Учет изменений корреляций в кризисных условиях

**Типы стресс-тестов:**
1. **Исторические сценарии:** Воспроизведение прошлых кризисов
2. **Гипотетические сценарии:** Моделирование новых кризисных ситуаций
3. **Факторные стресс-тесты:** Изменение конкретных факторов риска
4. **Корреляционные стресс-тесты:** Изменение корреляций между активами

**Почему Stress Testing важен:**
- **Оценка устойчивости:** Помогает оценить устойчивость портфолио
- **Подготовка к кризисам:** Помогает подготовиться к кризисным ситуациям
- **Управление рисками:** Критически важен для управления рисками
- **Планирование:** Помогает в планировании инвестиций
- **Регуляторное соответствие:** Требуется регуляторами
- **Инвесторская уверенность:** Повышает доверие инвесторов

**Плюсы:**
- Оценка устойчивости
- Подготовка к кризисам
- Управление рисками
- Планирование инвестиций

**Минусы:**
- Сложность настройки
- Потенциальные проблемы с данными
- Необходимость понимания рыночных условий
- Субъективность в выборе сценариев

**Практическое применение:** В нашем коде мы реализуем комплексную систему стресс-тестирования, включающую исторические сценарии, гипотетические кризисы и факторные стресс-тесты.

```python
# Дополнительные импорты для стресс-тестирования
from datetime import datetime, timedelta
import json

class StressTester:
    """Комплексная система стресс-тестирования портфолио"""
    
    def __init__(self):
        """Инициализация системы стресс-тестирования"""
        self.stress_scenarios = {}
        self.historical_scenarios = {}
        self.factor_scenarios = {}
        self.stress_results = {}
        self.risk_metrics = {}
        
    def define_historical_scenarios(self):
        """Определение исторических стресс-сценариев"""
        self.historical_scenarios = {
            '2008_financial_crisis': {
                'description': 'Финансовый кризис 2008 года',
                'period': '2008-09-15 to 2009-03-09',
                'asset_returns': {
                    'equity': -0.56,  # S&P 500
                    'bonds': 0.08,    # Treasury bonds
                    'commodities': -0.35,  # Oil
                    'real_estate': -0.40,  # REITs
                    'crypto': 0.0  # Bitcoin не существовал
                },
                'correlation_changes': {
                    'equity_bonds': 0.8,  # Отрицательная корреляция стала положительной
                    'equity_commodities': 0.9
                }
            },
            '2020_covid_crash': {
                'description': 'Крах рынка из-за COVID-19',
                'period': '2020-02-19 to 2020-03-23',
                'asset_returns': {
                    'equity': -0.34,  # S&P 500
                    'bonds': 0.05,    # Treasury bonds
                    'commodities': -0.25,  # Oil
                    'real_estate': -0.30,  # REITs
                    'crypto': -0.50  # Bitcoin
                },
                'correlation_changes': {
                    'equity_bonds': 0.7,
                    'equity_commodities': 0.8
                }
            },
            'dotcom_bubble': {
                'description': 'Крах доткомов 2000-2002',
                'period': '2000-03-24 to 2002-10-09',
                'asset_returns': {
                    'equity': -0.49,  # NASDAQ
                    'bonds': 0.15,    # Treasury bonds
                    'commodities': 0.10,  # Gold
                    'real_estate': 0.05,  # REITs
                    'crypto': 0.0
                },
                'correlation_changes': {
                    'equity_bonds': 0.6,
                    'equity_commodities': 0.3
                }
            }
        }
        
    def define_hypothetical_scenarios(self):
        """Определение гипотетических стресс-сценариев"""
        self.stress_scenarios = {
            'market_crash_30': {
                'description': 'Обвал рынка на 30%',
                'asset_returns': {
                    'equity': -0.30,
                    'bonds': -0.05,
                    'commodities': -0.20,
                    'real_estate': -0.25,
                    'crypto': -0.60
                },
                'correlation_changes': {
                    'equity_bonds': 0.8,
                    'equity_commodities': 0.9
                }
            },
            'interest_rate_shock_5pct': {
                'description': 'Резкий рост процентных ставок на 5%',
                'asset_returns': {
                    'equity': -0.15,
                    'bonds': -0.25,
                    'commodities': -0.10,
                    'real_estate': -0.30,
                    'crypto': -0.40
                },
                'correlation_changes': {
                    'equity_bonds': 0.9,
                    'equity_real_estate': 0.8
                }
            },
            'inflation_surge_10pct': {
                'description': 'Резкий рост инфляции до 10%',
                'asset_returns': {
                    'equity': -0.20,
                    'bonds': -0.30,
                    'commodities': 0.15,
                    'real_estate': 0.05,
                    'crypto': 0.10
                },
                'correlation_changes': {
                    'equity_bonds': 0.7,
                    'equity_commodities': -0.3
                }
            },
            'crypto_crash_80pct': {
                'description': 'Обвал криптовалют на 80%',
                'asset_returns': {
                    'equity': -0.05,
                    'bonds': 0.02,
                    'commodities': -0.05,
                    'real_estate': 0.01,
                    'crypto': -0.80
                },
                'correlation_changes': {
                    'crypto_equity': 0.9,
                    'crypto_commodities': 0.8
                }
            },
            'global_recession': {
                'description': 'Глобальная рецессия',
                'asset_returns': {
                    'equity': -0.40,
                    'bonds': 0.10,
                    'commodities': -0.30,
                    'real_estate': -0.35,
                    'crypto': -0.70
                },
                'correlation_changes': {
                    'equity_bonds': 0.8,
                    'equity_commodities': 0.9,
                    'equity_real_estate': 0.9
                }
            }
        }
        
    def define_factor_scenarios(self):
        """Определение факторных стресс-сценариев"""
        self.factor_scenarios = {
            'volatility_spike': {
                'description': 'Резкий рост волатильности',
                'volatility_multiplier': 3.0,
                'correlation_increase': 0.5
            },
            'liquidity_crisis': {
                'description': 'Кризис ликвидности',
                'liquidity_impact': 0.15,  # Дополнительные потери из-за ликвидности
                'correlation_increase': 0.3
            },
            'currency_crisis': {
                'description': 'Валютный кризис',
                'currency_impact': 0.20,  # Дополнительные потери из-за валюты
                'correlation_increase': 0.4
            }
        }
        
    def run_historical_stress_test(self, portfolio_weights, base_returns=None):
        """
        Запуск исторических стресс-тестов
        
        Args:
            portfolio_weights (dict): Веса активов в портфолио
            base_returns (dict): Базовые доходности активов
        
        Returns:
            dict: Результаты исторических стресс-тестов
        """
        if not self.historical_scenarios:
            self.define_historical_scenarios()
        
        results = {}
        
        for scenario_name, scenario in self.historical_scenarios.items():
            portfolio_return = self._calculate_scenario_return(
                portfolio_weights, scenario['asset_returns']
            )
            
            # Расчет дополнительных метрик
            var_impact = self._calculate_var_impact(portfolio_return, base_returns)
            max_drawdown = self._calculate_max_drawdown_impact(scenario['asset_returns'])
            
            results[scenario_name] = {
                'portfolio_return': portfolio_return,
                'description': scenario['description'],
                'period': scenario['period'],
                'var_impact': var_impact,
                'max_drawdown_impact': max_drawdown,
                'scenario_type': 'historical'
            }
        
        return results
    
    def run_hypothetical_stress_test(self, portfolio_weights, base_returns=None):
        """
        Запуск гипотетических стресс-тестов
        
        Args:
            portfolio_weights (dict): Веса активов в портфолио
            base_returns (dict): Базовые доходности активов
        
        Returns:
            dict: Результаты гипотетических стресс-тестов
        """
        if not self.stress_scenarios:
            self.define_hypothetical_scenarios()
        
        results = {}
        
        for scenario_name, scenario in self.stress_scenarios.items():
            portfolio_return = self._calculate_scenario_return(
                portfolio_weights, scenario['asset_returns']
            )
            
            # Учет изменений корреляций
            correlation_impact = self._calculate_correlation_impact(
                portfolio_weights, scenario.get('correlation_changes', {})
            )
            
            # Итоговая доходность с учетом корреляций
            total_return = portfolio_return + correlation_impact
            
            results[scenario_name] = {
                'portfolio_return': total_return,
                'base_return': portfolio_return,
                'correlation_impact': correlation_impact,
                'description': scenario['description'],
                'scenario_type': 'hypothetical'
            }
        
        return results
    
    def run_factor_stress_test(self, portfolio_weights, base_returns, base_volatilities):
        """
        Запуск факторных стресс-тестов
        
        Args:
            portfolio_weights (dict): Веса активов в портфолио
            base_returns (dict): Базовые доходности активов
            base_volatilities (dict): Базовые волатильности активов
        
        Returns:
            dict: Результаты факторных стресс-тестов
        """
        if not self.factor_scenarios:
            self.define_factor_scenarios()
        
        results = {}
        
        for scenario_name, scenario in self.factor_scenarios.items():
            if scenario_name == 'volatility_spike':
                # Увеличение волатильности
                multiplier = scenario['volatility_multiplier']
                new_volatilities = {k: v * multiplier for k, v in base_volatilities.items()}
                
                # Пересчет доходностей с учетом новой волатильности
                adjusted_returns = self._adjust_returns_for_volatility(
                    base_returns, new_volatilities
                )
                
                portfolio_return = self._calculate_scenario_return(
                    portfolio_weights, adjusted_returns
                )
                
            elif scenario_name == 'liquidity_crisis':
                # Кризис ликвидности
                liquidity_impact = scenario['liquidity_impact']
                adjusted_returns = {k: v - liquidity_impact for k, v in base_returns.items()}
                
                portfolio_return = self._calculate_scenario_return(
                    portfolio_weights, adjusted_returns
                )
                
            elif scenario_name == 'currency_crisis':
                # Валютный кризис
                currency_impact = scenario['currency_impact']
                adjusted_returns = {k: v - currency_impact for k, v in base_returns.items()}
                
                portfolio_return = self._calculate_scenario_return(
                    portfolio_weights, adjusted_returns
                )
            
            results[scenario_name] = {
                'portfolio_return': portfolio_return,
                'description': scenario['description'],
                'scenario_type': 'factor'
            }
        
        return results
    
    def run_comprehensive_stress_test(self, portfolio_weights, base_returns=None, 
                                    base_volatilities=None):
        """
        Запуск комплексного стресс-тестирования
        
        Args:
            portfolio_weights (dict): Веса активов в портфолио
            base_returns (dict): Базовые доходности активов
            base_volatilities (dict): Базовые волатильности активов
        
        Returns:
            dict: Комплексные результаты стресс-тестирования
        """
        comprehensive_results = {}
        
        # Исторические сценарии
        historical_results = self.run_historical_stress_test(portfolio_weights, base_returns)
        comprehensive_results['historical'] = historical_results
        
        # Гипотетические сценарии
        hypothetical_results = self.run_hypothetical_stress_test(portfolio_weights, base_returns)
        comprehensive_results['hypothetical'] = hypothetical_results
        
        # Факторные сценарии
        if base_volatilities:
            factor_results = self.run_factor_stress_test(
                portfolio_weights, base_returns, base_volatilities
            )
            comprehensive_results['factor'] = factor_results
        
        # Агрегированные метрики
        all_returns = []
        for category in comprehensive_results.values():
            for scenario in category.values():
                all_returns.append(scenario['portfolio_return'])
        
        comprehensive_results['aggregated_metrics'] = self._calculate_aggregated_metrics(all_returns)
        
        return comprehensive_results
    
    def _calculate_scenario_return(self, portfolio_weights, asset_returns):
        """Расчет доходности портфолио в сценарии"""
        portfolio_return = 0
        for asset, weight in portfolio_weights.items():
            if asset in asset_returns:
                portfolio_return += weight * asset_returns[asset]
        return portfolio_return
    
    def _calculate_var_impact(self, scenario_return, base_returns):
        """Расчет влияния на VaR"""
        if base_returns is None:
            return 0
        
        # Упрощенный расчет влияния на VaR
        base_portfolio_return = sum(base_returns.values()) / len(base_returns)
        return scenario_return - base_portfolio_return
    
    def _calculate_max_drawdown_impact(self, asset_returns):
        """Расчет влияния на максимальную просадку"""
        # Упрощенный расчет влияния на максимальную просадку
        worst_asset_return = min(asset_returns.values())
        return worst_asset_return
    
    def _calculate_correlation_impact(self, portfolio_weights, correlation_changes):
        """Расчет влияния изменений корреляций"""
        # Упрощенный расчет влияния корреляций
        total_impact = 0
        for pair, correlation_change in correlation_changes.items():
            # Упрощенная модель влияния корреляций
            impact = correlation_change * 0.1  # 10% от изменения корреляции
            total_impact += impact
        
        return total_impact
    
    def _adjust_returns_for_volatility(self, base_returns, new_volatilities):
        """Корректировка доходностей с учетом новой волатильности"""
        adjusted_returns = {}
        for asset, base_return in base_returns.items():
            if asset in new_volatilities:
                # Упрощенная корректировка
                volatility_ratio = new_volatilities[asset] / 0.2  # Предполагаем базовую волатильность 20%
                adjusted_returns[asset] = base_return * volatility_ratio
            else:
                adjusted_returns[asset] = base_return
        
        return adjusted_returns
    
    def _calculate_aggregated_metrics(self, returns):
        """Расчет агрегированных метрик стресс-тестирования"""
        if not returns:
            return {}
        
        return {
            'worst_case_return': min(returns),
            'best_case_return': max(returns),
            'average_stress_return': np.mean(returns),
            'median_stress_return': np.median(returns),
            'stress_volatility': np.std(returns),
            'stress_sharpe': np.mean(returns) / np.std(returns) if np.std(returns) > 0 else 0,
            'negative_scenarios': sum(1 for r in returns if r < 0),
            'total_scenarios': len(returns),
            'negative_ratio': sum(1 for r in returns if r < 0) / len(returns)
        }
    
    def generate_stress_report(self, portfolio_weights, base_returns=None, 
                             base_volatilities=None, save_to_file=False):
        """
        Генерация отчета по стресс-тестированию
        
        Args:
            portfolio_weights (dict): Веса активов в портфолио
            base_returns (dict): Базовые доходности активов
            base_volatilities (dict): Базовые волатильности активов
            save_to_file (bool): Сохранить отчет в файл
        
        Returns:
            dict: Отчет по стресс-тестированию
        """
        # Запуск комплексного стресс-тестирования
        results = self.run_comprehensive_stress_test(
            portfolio_weights, base_returns, base_volatilities
        )
        
        # Создание отчета
        report = {
            'timestamp': datetime.now().isoformat(),
            'portfolio_weights': portfolio_weights,
            'base_returns': base_returns,
            'stress_results': results,
            'summary': self._generate_summary(results)
        }
        
        if save_to_file:
            filename = f"stress_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(filename, 'w') as f:
                json.dump(report, f, indent=2, default=str)
            print(f"Отчет сохранен в файл: {filename}")
        
        return report
    
    def _generate_summary(self, results):
        """Генерация сводки по результатам стресс-тестирования"""
        summary = {}
        
        # Агрегированные метрики
        if 'aggregated_metrics' in results:
            metrics = results['aggregated_metrics']
            summary['overall'] = {
                'worst_case': metrics['worst_case_return'],
                'best_case': metrics['best_case_return'],
                'average': metrics['average_stress_return'],
                'negative_scenarios_ratio': metrics['negative_ratio']
            }
        
        # Лучшие и худшие сценарии
        all_scenarios = []
        for category in results.values():
            if isinstance(category, dict):
                for name, scenario in category.items():
                    if isinstance(scenario, dict) and 'portfolio_return' in scenario:
                        all_scenarios.append((name, scenario['portfolio_return']))
        
        if all_scenarios:
            all_scenarios.sort(key=lambda x: x[1])
            summary['worst_scenarios'] = all_scenarios[:3]
            summary['best_scenarios'] = all_scenarios[-3:]
        
        return summary

# Пример использования
if __name__ == "__main__":
    # Создание тестовых данных
    portfolio_weights = {
        'equity': 0.4,
        'bonds': 0.3,
        'commodities': 0.1,
        'real_estate': 0.15,
        'crypto': 0.05
    }
    
    base_returns = {
        'equity': 0.08,
        'bonds': 0.03,
        'commodities': 0.05,
        'real_estate': 0.06,
        'crypto': 0.15
    }
    
    base_volatilities = {
        'equity': 0.20,
        'bonds': 0.05,
        'commodities': 0.25,
        'real_estate': 0.15,
        'crypto': 0.60
    }
    
    # Инициализация системы стресс-тестирования
    stress_tester = StressTester()
    
    print("=== Стресс-тестирование портфолио ===")
    print(f"Портфолио: {portfolio_weights}")
    print()
    
    # Генерация отчета
    report = stress_tester.generate_stress_report(
        portfolio_weights, base_returns, base_volatilities
    )
    
    # Вывод результатов
    print("1. Исторические сценарии:")
    for name, scenario in report['stress_results']['historical'].items():
        print(f"   {name}: {scenario['portfolio_return']:.4f} ({scenario['description']})")
    print()
    
    print("2. Гипотетические сценарии:")
    for name, scenario in report['stress_results']['hypothetical'].items():
        print(f"   {name}: {scenario['portfolio_return']:.4f} ({scenario['description']})")
    print()
    
    print("3. Агрегированные метрики:")
    metrics = report['stress_results']['aggregated_metrics']
    print(f"   Худший случай: {metrics['worst_case_return']:.4f}")
    print(f"   Лучший случай: {metrics['best_case_return']:.4f}")
    print(f"   Средний результат: {metrics['average_stress_return']:.4f}")
    print(f"   Доля негативных сценариев: {metrics['negative_ratio']:.2%}")
    print()
    
    print("4. Сводка:")
    summary = report['summary']
    if 'overall' in summary:
        overall = summary['overall']
        print(f"   Общий худший случай: {overall['worst_case']:.4f}")
        print(f"   Общий лучший случай: {overall['best_case']:.4f}")
        print(f"   Доля негативных сценариев: {overall['negative_scenarios_ratio']:.2%}")
```

## Блокчейн-интеграция для портфолио

**Теория:** Блокчейн-интеграция для портфолио представляет собой использование блокчейн-технологий и DeFi протоколов для увеличения доходности портфолио. Это критически важно для создания инновационных и высокодоходных портфолио. Современные DeFi протоколы предлагают уникальные возможности для получения доходности через yield farming, ликвидность и другие механизмы.

**Математическая основа:** Блокчейн-интеграция основана на:
- **Смарт-контрактах:** Автоматизированное выполнение финансовых операций
- **Токеномике:** Экономические модели токенов и протоколов
- **Алгоритмическом трейдинге:** Автоматизированная торговля на DEX
- **Yield farming:** Оптимизация доходности через различные протоколы

**Компоненты блокчейн-интеграции:**
1. **DeFi активы:** Токены и протоколы децентрализованных финансов
2. **Yield Farming:** Получение доходности через предоставление ликвидности
3. **Liquidity Mining:** Добыча токенов за предоставление ликвидности
4. **Staking:** Стейкинг токенов для получения вознаграждений
5. **Cross-chain мосты:** Интеграция различных блокчейнов

**Почему блокчейн-интеграция критична:**
- **Новые возможности:** Предоставляет новые возможности для заработка
- **Децентрализация:** Обеспечивает децентрализацию портфолио
- **Прозрачность:** Обеспечивает прозрачность операций
- **Автоматизация:** Позволяет полностью автоматизировать управление
- **Высокая доходность:** Потенциально более высокая доходность
- **Инновации:** Доступ к новейшим финансовым инструментам

### 1. DeFi активы

**Теория:** DeFi активы представляют собой децентрализованные финансовые активы, которые предоставляют новые возможности для инвестирования и получения доходности. Это критически важно для создания диверсифицированных портфолио.

**Почему DeFi активы важны:**
- **Новые возможности:** Предоставляют новые возможности для инвестирования
- **Высокая доходность:** Могут обеспечить высокую доходность
- **Децентрализация:** Обеспечивают децентрализацию портфолио
- **Инновации:** Представляют инновационные финансовые инструменты

**Плюсы:**
- Новые возможности инвестирования
- Высокая потенциальная доходность
- Децентрализация
- Инновационные инструменты

**Минусы:**
- Высокие риски
- Сложность интеграции
- Потенциальные проблемы с ликвидностью

```python
class DeFiPortfolioManager:
    """Менеджер DeFi портфолио"""
    
    def __init__(self, web3_provider, private_key):
        self.web3 = Web3(Web3.HTTPProvider(web3_provider))
        self.account = self.web3.eth.account.from_key(private_key)
        self.defi_assets = {}
        self.yield_farming_pools = {}
    
    def add_defi_asset(self, asset_name, contract_address, abi):
        """Добавление DeFi актива"""
        contract = self.web3.eth.contract(address=contract_address, abi=abi)
        self.defi_assets[asset_name] = {
            'contract': contract,
            'address': contract_address,
            'balance': 0
        }
    
    def get_defi_balances(self):
        """Получение балансов DeFi активов"""
        balances = {}
        
        for asset_name, asset_info in self.defi_assets.items():
            try:
                balance = asset_info['contract'].functions.balanceOf(self.account.address).call()
                balances[asset_name] = balance
                self.defi_assets[asset_name]['balance'] = balance
            except Exception as e:
                print(f"Error getting balance for {asset_name}: {e}")
                balances[asset_name] = 0
        
        return balances
    
    def calculate_defi_yield(self, asset_name, time_period=30):
        """Расчет доходности DeFi актива"""
        if asset_name not in self.defi_assets:
            return 0
        
        try:
            # Получение информации о пуле
            pool_info = self.defi_assets[asset_name]['contract'].functions.poolInfo(0).call()
            
            # Расчет APR
            total_alloc_point = self.defi_assets[asset_name]['contract'].functions.totalAllocPoint().call()
            reward_per_block = self.defi_assets[asset_name]['contract'].functions.rewardPerBlock().call()
            
            pool_alloc_point = pool_info[1]
            pool_alloc_share = pool_alloc_point / total_alloc_point
            
            # APR
            blocks_per_year = 2102400
            annual_rewards = reward_per_block * pool_alloc_share * blocks_per_year
            total_staked = pool_info[0]
            
            apr = annual_rewards / total_staked if total_staked > 0 else 0
            
            # Доходность за период
            period_yield = apr * (time_period / 365)
            
            return period_yield
            
        except Exception as e:
            print(f"Error calculating yield for {asset_name}: {e}")
            return 0
```

### 2. Yield Farming оптимизация

**Теория:** Yield Farming оптимизация представляет собой процесс оптимизации распределения капитала между различными протоколами yield farming для максимизации доходности. Это критически важно для создания высокодоходных портфолио.

**Почему Yield Farming оптимизация важна:**
- **Максимизация доходности:** Помогает максимизировать доходность портфолио
- **Автоматизация:** Автоматизирует процесс управления
- **Оптимизация:** Помогает оптимизировать распределение капитала
- **Диверсификация:** Обеспечивает диверсификацию источников дохода

**Плюсы:**
- Максимизация доходности
- Автоматизация управления
- Оптимизация распределения
- Диверсификация доходов

**Минусы:**
- Сложность интеграции
- Потенциальные риски протоколов
- Высокие требования к безопасности

```python
class YieldFarmingOptimizer:
    """Оптимизатор Yield Farming"""
    
    def __init__(self, defi_manager):
        self.defi_manager = defi_manager
        self.farming_pools = {}
        self.optimization_history = []
    
    def add_farming_pool(self, pool_name, pool_info):
        """Добавление пула для фарминга"""
        self.farming_pools[pool_name] = pool_info
    
    def optimize_farming_allocation(self, total_capital):
        """Оптимизация распределения для фарминга"""
        # Получение APR всех пулов
        pool_aprs = {}
        for pool_name, pool_info in self.farming_pools.items():
            apr = self.defi_manager.calculate_defi_yield(pool_name)
            pool_aprs[pool_name] = apr
        
        # Сортировка пулов по APR
        sorted_pools = sorted(pool_aprs.items(), key=lambda x: x[1], reverse=True)
        
        # Оптимальное распределение
        optimal_allocation = {}
        remaining_capital = total_capital
        
        for pool_name, apr in sorted_pools:
            if apr > 0.1:  # Минимальный APR 10%
                # Максимум 30% капитала в один пул
                max_allocation = min(remaining_capital * 0.3, remaining_capital)
                optimal_allocation[pool_name] = max_allocation
                remaining_capital -= max_allocation
        
        return optimal_allocation
    
    def rebalance_farming_portfolio(self, current_allocation, target_allocation):
        """Перебалансировка фарминга"""
        rebalancing_trades = []
        
        for pool_name in set(current_allocation.keys()) | set(target_allocation.keys()):
            current_amount = current_allocation.get(pool_name, 0)
            target_amount = target_allocation.get(pool_name, 0)
            
            if abs(current_amount - target_amount) > 0.01:  # Минимальное отклонение
                trade_amount = target_amount - current_amount
                rebalancing_trades.append({
                    'pool': pool_name,
                    'amount': trade_amount,
                    'action': 'stake' if trade_amount > 0 else 'unstake'
                })
        
        return rebalancing_trades
```

## Автоматическое управление портфолио

**Теория:** Автоматическое управление портфолио представляет собой систему, которая автоматически управляет портфолио без вмешательства человека. Это критически важно для создания эффективных и рентабельных портфолио. Современные системы автоматического управления используют машинное обучение, алгоритмическую торговлю и роботизированные консультанты.

**Математическая основа:** Автоматическое управление основано на:
- **Алгоритмической торговле:** Автоматизированное выполнение торговых стратегий
- **Машинном обучении:** Адаптивные модели для принятия решений
- **Оптимизации:** Непрерывная оптимизация портфолио
- **Риск-менеджменте:** Автоматическое управление рисками

**Компоненты автоматического управления:**
1. **Система мониторинга:** Отслеживание производительности и рисков
2. **Алгоритмическая торговля:** Автоматическое выполнение сделок
3. **Риск-менеджмент:** Автоматическое управление рисками
4. **Перебалансирование:** Автоматическая корректировка весов
5. **Отчетность:** Автоматическая генерация отчетов

**Почему автоматическое управление критично:**
- **Автоматизация:** Полная автоматизация процесса управления
- **Эффективность:** Обеспечивает высокую эффективность управления
- **Масштабируемость:** Позволяет масштабировать управление
- **Рентабельность:** Снижает затраты на управление
- **Скорость:** Мгновенная реакция на изменения рынка
- **Объективность:** Устранение эмоциональных решений

### 1. Система мониторинга

**Теория:** Система мониторинга портфолио представляет собой комплексную систему отслеживания производительности и рисков портфолио. Это критически важно для своевременного выявления проблем и принятия решений. Современные системы мониторинга используют машинное обучение для прогнозирования проблем и автоматического реагирования.

**Математическая основа:** Система мониторинга основана на:
- **Временных рядах:** Анализ динамики показателей портфолио
- **Аномалийном детектировании:** Выявление необычных паттернов
- **Прогнозировании:** Предсказание будущих проблем
- **Классификации:** Категоризация типов проблем

**Компоненты системы мониторинга:**
1. **Метрики производительности:** Отслеживание доходности и рисков
2. **Алерты:** Уведомления о критических событиях
3. **Дашборды:** Визуализация состояния портфолио
4. **Отчеты:** Автоматическая генерация отчетов
5. **Прогнозирование:** Предсказание будущих проблем

**Почему система мониторинга важна:**
- **Своевременное выявление проблем:** Позволяет своевременно выявлять проблемы
- **Автоматизация:** Автоматизирует процесс мониторинга
- **Предотвращение потерь:** Помогает предотвратить потери
- **Оптимизация:** Помогает оптимизировать производительность
- **Прозрачность:** Обеспечивает прозрачность для инвесторов

**Плюсы:**
- Своевременное выявление проблем
- Автоматизация мониторинга
- Предотвращение потерь
- Оптимизация производительности

**Минусы:**
- Сложность настройки
- Потенциальные ложные срабатывания
- Высокие требования к ресурсам

```python
class PortfolioMonitor:
    """Мониторинг портфолио"""
    
    def __init__(self):
        self.performance_metrics = {}
        self.alert_thresholds = {
            'max_drawdown': 0.15,
            'min_sharpe_ratio': 1.0,
            'max_var': 0.05
        }
        self.alerts = []
    
    def monitor_performance(self, portfolio):
        """Мониторинг производительности портфолио"""
        # Расчет метрик
        returns = portfolio.get_returns()
        metrics = self._calculate_metrics(returns)
        
        # Сохранение метрик
        self.performance_metrics[datetime.now()] = metrics
        
        # Проверка алертов
        alerts = self._check_alerts(metrics)
        
        return {
            'metrics': metrics,
            'alerts': alerts
        }
    
    def _calculate_metrics(self, returns):
        """Расчет метрик производительности"""
        metrics = {
            'total_return': np.sum(returns),
            'annualized_return': np.mean(returns) * 252,
            'volatility': np.std(returns) * np.sqrt(252),
            'sharpe_ratio': np.mean(returns) / np.std(returns) * np.sqrt(252) if np.std(returns) > 0 else 0,
            'max_drawdown': self._calculate_max_drawdown(returns),
            'var_95': self._calculate_var(returns, 0.05)
        }
        
        return metrics
    
    def _check_alerts(self, metrics):
        """Проверка алертов"""
        alerts = []
        
        # Проверка максимальной просадки
        if metrics['max_drawdown'] > self.alert_thresholds['max_drawdown']:
            alerts.append({
                'type': 'high_drawdown',
                'message': f"High drawdown detected: {metrics['max_drawdown']:.2%}",
                'severity': 'high'
            })
        
        # Проверка Sharpe Ratio
        if metrics['sharpe_ratio'] < self.alert_thresholds['min_sharpe_ratio']:
            alerts.append({
                'type': 'low_sharpe',
                'message': f"Low Sharpe ratio: {metrics['sharpe_ratio']:.2f}",
                'severity': 'medium'
            })
        
        # Проверка VaR
        if metrics['var_95'] > self.alert_thresholds['max_var']:
            alerts.append({
                'type': 'high_var',
                'message': f"High VaR: {metrics['var_95']:.2%}",
                'severity': 'high'
            })
        
        return alerts
```

### 2. Автоматическое управление

**Теория:** Автоматическое управление портфолио представляет собой систему, которая автоматически принимает решения и выполняет операции по управлению портфолио. Это критически важно для создания полностью автоматизированных портфолио. Современные системы автоматического управления используют искусственный интеллект для принятия сложных решений.

**Математическая основа:** Автоматическое управление основано на:
- **Алгоритмической торговле:** Автоматизированное выполнение торговых стратегий
- **Машинном обучении:** Адаптивные модели для принятия решений
- **Оптимизации:** Непрерывная оптимизация портфолио
- **Риск-менеджменте:** Автоматическое управление рисками

**Компоненты автоматического управления:**
1. **Система принятия решений:** Алгоритмы для принятия торговых решений
2. **Исполнение сделок:** Автоматическое выполнение торговых операций
3. **Риск-менеджмент:** Автоматическое управление рисками
4. **Перебалансирование:** Автоматическая корректировка весов
5. **Отчетность:** Автоматическая генерация отчетов

**Почему автоматическое управление важно:**
- **Полная автоматизация:** Обеспечивает полную автоматизацию процесса
- **Эффективность:** Обеспечивает высокую эффективность управления
- **Масштабируемость:** Позволяет масштабировать управление
- **Рентабельность:** Снижает затраты на управление
- **Скорость:** Мгновенная реакция на изменения рынка
- **Объективность:** Устранение эмоциональных решений

**Плюсы:**
- Полная автоматизация
- Высокая эффективность
- Масштабируемость
- Снижение затрат

**Минусы:**
- Сложность реализации
- Потенциальные ошибки
- Высокие требования к безопасности

```python
class AutomatedPortfolioManager:
    """Автоматическое управление портфолио"""
    
    def __init__(self, portfolio, optimizer, monitor):
        self.portfolio = portfolio
        self.optimizer = optimizer
        self.monitor = monitor
        self.rebalancing_schedule = 'weekly'
        self.last_rebalancing = None
    
    def run_automated_management(self):
        """Запуск автоматического управления"""
        # Мониторинг производительности
        performance = self.monitor.monitor_performance(self.portfolio)
        
        # Проверка необходимости перебалансирования
        if self._should_rebalance():
            self._rebalance_portfolio()
        
        # Обработка алертов
        if performance['alerts']:
            self._handle_alerts(performance['alerts'])
        
        return performance
    
    def _should_rebalance(self):
        """Проверка необходимости перебалансирования"""
        # Проверка по расписанию
        if self._is_scheduled_rebalancing():
            return True
        
        # Проверка по производительности
        if self._is_performance_based_rebalancing():
            return True
        
        return False
    
    def _rebalance_portfolio(self):
        """Перебалансировка портфолио"""
        print("Starting portfolio rebalancing...")
        
        # Получение текущих весов
        current_weights = self.portfolio.get_weights()
        
        # Оптимизация новых весов
        expected_returns = self._get_expected_returns()
        cov_matrix = self._get_covariance_matrix()
        
        target_weights = self.optimizer.optimize_portfolio(
            expected_returns, cov_matrix
        )
        
        # Выполнение перебалансирования
        rebalancing_trades = self._calculate_rebalancing_trades(
            current_weights, target_weights
        )
        
        # Выполнение сделок
        for trade in rebalancing_trades:
            self.portfolio.execute_trade(trade)
        
        # Обновление времени последнего перебалансирования
        self.last_rebalancing = datetime.now()
        
        print("Portfolio rebalancing completed")
    
    def _handle_alerts(self, alerts):
        """Обработка алертов"""
        for alert in alerts:
            if alert['severity'] == 'high':
                # Критические алерты - немедленные действия
                self._handle_critical_alert(alert)
            elif alert['severity'] == 'medium':
                # Средние алерты - планирование действий
                self._handle_medium_alert(alert)
```

## Следующие шаги

После изучения оптимизации портфолио переходите к:
- **[16_metrics_analysis.md](16_metrics_analysis.md)** - Метрики и анализ
- **[17_examples.md](17_examples.md)** - Практические примеры

## Ключевые выводы

**Теория:** Ключевые выводы суммируют наиболее важные аспекты оптимизации портфолио для создания прибыльных портфолио с доходностью 100%+ в месяц. Эти выводы критически важны для понимания того, как создать эффективные портфолио. Современная оптимизация портфолио требует комплексного подхода, объединяющего передовые технологии и научные методы.

**Математическая основа:** Все выводы основаны на строгих математических принципах:
- **Теория портфолио Марковица:** Классическая основа современной оптимизации
- **Стохастические процессы:** Моделирование динамики финансовых рынков
- **Машинное обучение:** Адаптивные алгоритмы для принятия решений
- **Оптимизация:** Математические методы поиска оптимальных решений

1. **ML-оптимизация - использование машинного обучения для оптимизации портфолио**
   - **Теория:** ML-оптимизация обеспечивает научно обоснованную оптимизацию портфолио
   - **Почему важно:** Обеспечивает высокую точность и эффективность
   - **Плюсы:** Высокая точность, научная обоснованность, автоматизация
   - **Минусы:** Сложность реализации, высокие требования к данным
   - **Практическое применение:** Использование XGBoost, нейронных сетей и ансамблей

2. **Динамическое перебалансирование - автоматическая корректировка весов**
   - **Теория:** Динамическое перебалансирование обеспечивает поддержание оптимальных весов
   - **Почему важно:** Обеспечивает адаптивность к изменениям рынка
   - **Плюсы:** Адаптивность, поддержание оптимальности, автоматизация
   - **Минусы:** Потенциальные частые сделки, комиссии
   - **Практическое применение:** Интеллектуальные системы с учетом транзакционных издержек

3. **Мультиактивный анализ - учет корреляций и волатильности**
   - **Теория:** Мультиактивный анализ обеспечивает комплексное понимание портфолио
   - **Почему важно:** Обеспечивает эффективную диверсификацию
   - **Плюсы:** Комплексный анализ, снижение рисков, повышение доходности
   - **Минусы:** Сложность анализа, высокие вычислительные требования
   - **Практическое применение:** Анализ корреляций, волатильности и кластеризация

4. **Продвинутый риск-менеджмент - VaR, стресс-тестирование**
   - **Теория:** Продвинутый риск-менеджмент критически важен для долгосрочного успеха
   - **Почему важно:** Обеспечивает защиту капитала и стабильность
   - **Плюсы:** Защита капитала, стабильность, долгосрочный успех
   - **Минусы:** Сложность настройки, потенциальные ограничения доходности
   - **Практическое применение:** Множественные методы VaR, стресс-тестирование, бэктестинг

5. **Блокчейн-интеграция - использование DeFi для увеличения доходности**
   - **Теория:** Блокчейн-интеграция предоставляет новые возможности для заработка
   - **Почему важно:** Обеспечивает новые источники доходности
   - **Плюсы:** Новые возможности, децентрализация, прозрачность
   - **Минусы:** Сложность интеграции, высокие требования к безопасности
   - **Практическое применение:** Yield farming, стейкинг, ликвидность

6. **Автоматическое управление - полная автоматизация процесса**
   - **Теория:** Автоматическое управление критически важно для эффективности
   - **Почему важно:** Обеспечивает полную автоматизацию и масштабируемость
   - **Плюсы:** Полная автоматизация, масштабируемость, снижение затрат
   - **Минусы:** Сложность реализации, потенциальные ошибки
   - **Практическое применение:** Системы мониторинга, алгоритмическая торговля

**Интеграция компонентов:** Успешная оптимизация портфолио требует интеграции всех компонентов в единую систему, где каждый элемент дополняет и усиливает другие. Это обеспечивает синергетический эффект и максимальную эффективность.

**Будущие направления:** Развитие оптимизации портфолио будет включать:
- **Квантовые вычисления:** Для решения сложных задач оптимизации
- **ИИ следующего поколения:** Более интеллектуальные системы принятия решений
- **Блокчейн 3.0:** Новые возможности для децентрализованных финансов
- **Регуляторные изменения:** Адаптация к новым требованиям

---

**Важно:** Оптимизация портфолио - это непрерывный процесс, требующий постоянного мониторинга и корректировки.
