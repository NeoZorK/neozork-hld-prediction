# 18.2. –î–µ—Ç–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã

**–¢–µ–æ—Ä–∏—è:** –î–µ—Ç–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Å–æ–±–æ–π –ø–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –≤—Å–µ—Ö –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Å–∏—Å—Ç–µ–º—ã, –∏—Ö —Ñ—É–Ω–∫—Ü–∏–π –∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π. –≠—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–∏—Å—Ç–µ–º—ã.

**–ü–æ—á–µ–º—É –¥–µ—Ç–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã –≤–∞–∂–Ω—ã:**
- **–ü–æ–Ω–∏–º–∞–Ω–∏–µ:** –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≥–ª—É–±–æ–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã
- **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:** –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
- **–†–µ–∞–ª–∏–∑–∞—Ü–∏—è:** –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏
- **–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è:** –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤

**–ü–ª—é—Å—ã:**
- –ì–ª—É–±–æ–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ
- –ß–µ—Ç–∫–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
- –î–µ—Ç–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
- –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è

**–ú–∏–Ω—É—Å—ã:**
- –í—ã—Å–æ–∫–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å
- –¢—Ä–µ–±—É–µ—Ç –≥–ª—É–±–æ–∫–∏—Ö –∑–Ω–∞–Ω–∏–π
- –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π

## üìä –°–±–æ—Ä—â–∏–∫ –¥–∞–Ω–Ω—ã—Ö

**–¢–µ–æ—Ä–∏—è:** –°–±–æ—Ä—â–∏–∫ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç —Å–∏—Å—Ç–µ–º—ã, –æ—Ç–≤–µ—á–∞—é—â–∏–π –∑–∞ —Å–±–æ—Ä, –æ—á–∏—Å—Ç–∫—É –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞. –≠—Ç–æ –æ—Å–Ω–æ–≤–∞ –¥–ª—è –≤—Å–µ—Ö ML-–º–æ–¥–µ–ª–µ–π –∏ —Ç–æ—Ä–≥–æ–≤—ã—Ö —Ä–µ—à–µ–Ω–∏–π.

**–ü–æ—á–µ–º—É —Å–±–æ—Ä—â–∏–∫ –¥–∞–Ω–Ω—ã—Ö –≤–∞–∂–µ–Ω:**
- **–ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö:** –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
- **–ü–æ–ª–Ω–æ—Ç–∞:** –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–æ–ª–Ω–æ—Ç—É –¥–∞–Ω–Ω—ã—Ö
- **–ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å:** –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
- **–ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å:** –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã

**–ü–ª—é—Å—ã:**
- –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
- –ü–æ–ª–Ω–æ—Ç–∞ –¥–∞–Ω–Ω—ã—Ö
- –ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
- –ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã

**–ú–∏–Ω—É—Å—ã:**
- –°–ª–æ–∂–Ω–æ—Å—Ç—å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏
- –í—ã—Å–æ–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Ä–µ—Å—É—Ä—Å–∞–º
- –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã —Å –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö

**–î–µ—Ç–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å–±–æ—Ä—â–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö:**

–°–±–æ—Ä—â–∏–∫ –¥–∞–Ω–Ω—ã—Ö —è–≤–ª—è–µ—Ç—Å—è —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–º —Å–∏—Å—Ç–µ–º—ã, –∫–æ—Ç–æ—Ä—ã–π –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–æ–ª—É—á–µ–Ω–∏–µ, –æ—á–∏—Å—Ç–∫—É –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É —Ä—ã–Ω–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Å–µ—Ö –ø–æ—Å–ª–µ–¥—É—é—â–∏—Ö –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤. –≠—Ç–æ—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–µ–Ω –¥–ª—è —Ä–∞–±–æ—Ç—ã –≤—Å–µ–π —Å–∏—Å—Ç–µ–º—ã, –ø–æ—Å–∫–æ–ª—å–∫—É –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –Ω–∞–ø—Ä—è–º—É—é –≤–ª–∏—è–µ—Ç –Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç—å –≤—Å–µ—Ö ML-–º–æ–¥–µ–ª–µ–π –∏ —Ç–æ—Ä–≥–æ–≤—ã—Ö —Ä–µ—à–µ–Ω–∏–π.

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã:**
- **–ú–æ–¥—É–ª—å–Ω–æ—Å—Ç—å**: –ö–∞–∂–¥—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ
- **–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ**: –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ –ø–∞–º—è—Ç–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
- **–û—á–∏—Å—Ç–∫–∞**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π –∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- **–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å**: –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∞–∫—Ç–∏–≤–æ–≤ –∏ —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤
- **–ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å**: –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ —Å–±–æ–µ–≤

**–ö–ª—é—á–µ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏:**
1. **–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö**: –ü–æ–ª—É—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –∏ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
2. **–û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö**: –£–¥–∞–ª–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π, –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
3. **–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è**: –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∫ –µ–¥–∏–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É
4. **–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ**: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ
5. **–≠–∫—Å–ø–æ—Ä—Ç**: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö

```python
# src/data/collectors.py
import yfinance as yf
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import logging
from pathlib import Path
import asyncio
import aiohttp
from concurrent.futures import ThreadPoolExecutor
import time

class DataCollector:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Å–±–æ—Ä—â–∏–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Å–µ—Ö –∞–∫—Ç–∏–≤–æ–≤ –∏ —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤
    
    –≠—Ç–æ—Ç –∫–ª–∞—Å—Å –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç:
    - –ú–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω—ã–π —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –æ—á–∏—Å—Ç–∫—É –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é
    - –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
    - –û–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫ –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ
    - –ü–æ–¥–¥–µ—Ä–∂–∫—É —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö
    """
    
    def __init__(self, config: Dict):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–±–æ—Ä—â–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö
        """
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.data_cache = {}
        self.last_update = {}
        self.max_workers = config.get('max_workers', 5)
        self.cache_ttl = config.get('cache_ttl', 3600)  # 1 —á–∞—Å
        self.retry_attempts = config.get('retry_attempts', 3)
        self.retry_delay = config.get('retry_delay', 1)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        logging.basicConfig(level=logging.INFO)
        
    def collect_data(self, symbol: str, timeframe: str, period: str = "2y") -> pd.DataFrame:
        """
        –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–∏–º–≤–æ–ª–∞ –∏ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é
        
        Args:
            symbol: –°–∏–º–≤–æ–ª –∞–∫—Ç–∏–≤–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'AAPL', 'EURUSD')
            timeframe: –¢–∞–π–º—Ñ—Ä–µ–π–º ('M1', 'M5', 'H1', 'D1', etc.)
            period: –ü–µ—Ä–∏–æ–¥ –¥–∞–Ω–Ω—ã—Ö ('1d', '5d', '1mo', '3mo', '6mo', '1y', '2y', '5y', '10y', 'ytd', 'max')
            
        Returns:
            pd.DataFrame: –û—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        try:
            self.logger.info(f"Starting data collection for {symbol} {timeframe}")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
            cache_key = f"{symbol}_{timeframe}_{period}"
            if self._is_cache_valid(cache_key):
                self.logger.info(f"Using cached data for {symbol} {timeframe}")
                return self.data_cache[cache_key]
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ –¥–ª—è yfinance
            interval_map = {
                'M1': '1m',
                'M5': '5m', 
                'M15': '15m',
                'M30': '30m',
                'H1': '1h',
                'H2': '2h',
                'H4': '4h',
                'H6': '6h',
                'H8': '8h',
                'H12': '12h',
                'D1': '1d',
                'W1': '1wk',
                'MN1': '1mo'
            }
            
            interval = interval_map.get(timeframe, '1h')
            
            # –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏
            data = self._collect_with_retry(symbol, interval, period)
            
            if data.empty:
                self.logger.warning(f"No data found for {symbol} {timeframe}")
                return pd.DataFrame()
            
            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            data = self._clean_data(data)
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
            data = self._add_metadata(data, symbol, timeframe)
            data = self._add_technical_indicators(data)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
            if not self._validate_data_quality(data):
                self.logger.error(f"Data quality validation failed for {symbol} {timeframe}")
                return pd.DataFrame()
            
            # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–æ–π
            self.data_cache[cache_key] = data
            self.last_update[cache_key] = time.time()
            
            self.logger.info(f"Successfully collected {len(data)} records for {symbol} {timeframe}")
            return data
            
        except Exception as e:
            self.logger.error(f"Error collecting data for {symbol} {timeframe}: {e}")
            return pd.DataFrame()
    
    def _collect_with_retry(self, symbol: str, interval: str, period: str) -> pd.DataFrame:
        """–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        for attempt in range(self.retry_attempts):
            try:
                ticker = yf.Ticker(symbol)
                data = ticker.history(period=period, interval=interval)
                
                if not data.empty:
                    return data
                    
            except Exception as e:
                self.logger.warning(f"Attempt {attempt + 1} failed for {symbol}: {e}")
                if attempt < self.retry_attempts - 1:
                    time.sleep(self.retry_delay * (2 ** attempt))  # Exponential backoff
                    
        return pd.DataFrame()
    
    def _clean_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º
        
        –í–∫–ª—é—á–∞–µ—Ç:
        - –£–¥–∞–ª–µ–Ω–∏–µ NaN –∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        - –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∏ —É–¥–∞–ª–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π
        - –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        - –í–∞–ª–∏–¥–∞—Ü–∏—é –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
        """
        original_length = len(data)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ NaN
        data = data.dropna()
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ –∏–Ω–¥–µ–∫—Å—É
        data = data[~data.index.duplicated(keep='first')]
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        data = data.sort_index()
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π
        data = self._remove_anomalies(data)
        
        # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        data = self._fill_missing_data(data)
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è OHLC –¥–∞–Ω–Ω—ã—Ö
        data = self._validate_ohlc_data(data)
        
        cleaned_length = len(data)
        if original_length != cleaned_length:
            self.logger.info(f"Data cleaned: {original_length} -> {cleaned_length} records")
        
        return data
    
    def _remove_anomalies(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è:
        - –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö —Ü–µ–Ω–æ–≤—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π
        - –ù–µ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –æ–±—ä–µ–º–æ–≤ —Ç–æ—Ä–≥–æ–≤
        - –í—Ä–µ–º–µ–Ω–Ω—ã—Ö –∞–Ω–æ–º–∞–ª–∏–π
        """
        if data.empty:
            return data
            
        # –£–¥–∞–ª–µ–Ω–∏–µ –Ω—É–ª–µ–≤—ã—Ö –∏–ª–∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö —Ü–µ–Ω
        price_columns = ['Open', 'High', 'Low', 'Close']
        for col in price_columns:
            if col in data.columns:
                data = data[data[col] > 0]
        
        # –£–¥–∞–ª–µ–Ω–∏–µ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (–±–æ–ª–µ–µ 5 —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π)
        for col in price_columns:
            if col in data.columns and len(data) > 10:
                mean_val = data[col].mean()
                std_val = data[col].std()
                if std_val > 0:
                    z_scores = np.abs((data[col] - mean_val) / std_val)
                    data = data[z_scores < 5]
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö –æ–±—ä–µ–º–æ–≤
        if 'Volume' in data.columns and len(data) > 10:
            volume_mean = data['Volume'].mean()
            volume_std = data['Volume'].std()
            if volume_std > 0:
                volume_z_scores = np.abs((data['Volume'] - volume_mean) / volume_std)
                data = data[volume_z_scores < 4]
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–æ–≥–∏–∫–∏ OHLC
        data = data[data['High'] >= data['Low']]
        data = data[data['High'] >= data['Open']]
        data = data[data['High'] >= data['Close']]
        data = data[data['Low'] <= data['Open']]
        data = data[data['Low'] <= data['Close']]
        
        return data
    
    def _fill_missing_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–º –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏"""
        if data.empty:
            return data
            
        # –ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –¥–ª—è —Ü–µ–Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        price_columns = ['Open', 'High', 'Low', 'Close']
        for col in price_columns:
            if col in data.columns:
                data[col] = data[col].interpolate(method='linear')
        
        # –î–ª—è –æ–±—ä–µ–º–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º forward fill
        if 'Volume' in data.columns:
            data['Volume'] = data['Volume'].fillna(method='ffill')
        
        return data
    
    def _validate_ohlc_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ OHLC –¥–∞–Ω–Ω—ã—Ö"""
        if data.empty:
            return data
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ High >= Low
        valid_ohlc = data['High'] >= data['Low']
        data = data[valid_ohlc]
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ High >= Open –∏ High >= Close
        valid_high = (data['High'] >= data['Open']) & (data['High'] >= data['Close'])
        data = data[valid_high]
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ Low <= Open –∏ Low <= Close
        valid_low = (data['Low'] <= data['Open']) & (data['Low'] <= data['Close'])
        data = data[valid_low]
        
        return data
    
    def _add_metadata(self, data: pd.DataFrame, symbol: str, timeframe: str) -> pd.DataFrame:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∫ –¥–∞–Ω–Ω—ã–º"""
        data = data.copy()
        data['symbol'] = symbol
        data['timeframe'] = timeframe
        data['timestamp'] = data.index
        data['date'] = data.index.date
        data['time'] = data.index.time
        data['day_of_week'] = data.index.dayofweek
        data['hour'] = data.index.hour
        data['minute'] = data.index.minute
        
        return data
    
    def _add_technical_indicators(self, data: pd.DataFrame) -> pd.DataFrame:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
        if data.empty or len(data) < 20:
            return data
            
        data = data.copy()
        
        # –ü—Ä–æ—Å—Ç—ã–µ —Å–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ
        for window in [5, 10, 20, 50]:
            data[f'sma_{window}'] = data['Close'].rolling(window=window).mean()
        
        # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ
        for span in [12, 26]:
            data[f'ema_{span}'] = data['Close'].ewm(span=span).mean()
        
        # RSI
        data['rsi'] = self._calculate_rsi(data['Close'])
        
        # Bollinger Bands
        bb_period = 20
        bb_std = 2
        data['bb_middle'] = data['Close'].rolling(bb_period).mean()
        bb_std_val = data['Close'].rolling(bb_period).std()
        data['bb_upper'] = data['bb_middle'] + (bb_std_val * bb_std)
        data['bb_lower'] = data['bb_middle'] - (bb_std_val * bb_std)
        
        # MACD
        data['macd'] = self._calculate_macd(data['Close'])
        
        # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
        data['volatility'] = data['Close'].rolling(20).std()
        
        return data
    
    def _calculate_rsi(self, prices: pd.Series, window: int = 14) -> pd.Series:
        """–†–∞—Å—á–µ—Ç RSI (Relative Strength Index)"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi
    
    def _calculate_macd(self, prices: pd.Series, fast: int = 12, slow: int = 26, signal: int = 9) -> pd.Series:
        """–†–∞—Å—á–µ—Ç MACD (Moving Average Convergence Divergence)"""
        ema_fast = prices.ewm(span=fast).mean()
        ema_slow = prices.ewm(span=slow).mean()
        macd = ema_fast - ema_slow
        signal_line = macd.ewm(span=signal).mean()
        return macd - signal_line
    
    def _validate_data_quality(self, data: pd.DataFrame) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
        if data.empty:
            return False
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞–ø–∏—Å–µ–π
        if len(data) < 10:
            return False
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ NaN
        if data.isnull().any().any():
            return False
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ —Ü–µ–Ω
        price_columns = ['Open', 'High', 'Low', 'Close']
        for col in price_columns:
            if col in data.columns:
                if (data[col] <= 0).any():
                    return False
                    
        return True
    
    def _is_cache_valid(self, cache_key: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –∫—ç—à–∞"""
        if cache_key not in self.data_cache:
            return False
            
        if cache_key not in self.last_update:
            return False
            
        return (time.time() - self.last_update[cache_key]) < self.cache_ttl
    
    def collect_multiple_assets(self, symbols: List[str], timeframes: List[str], period: str = "2y") -> Dict[str, pd.DataFrame]:
        """
        –ú–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω—ã–π —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∞–∫—Ç–∏–≤–æ–≤
        
        Args:
            symbols: –°–ø–∏—Å–æ–∫ —Å–∏–º–≤–æ–ª–æ–≤ –∞–∫—Ç–∏–≤–æ–≤
            timeframes: –°–ø–∏—Å–æ–∫ —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤
            period: –ü–µ—Ä–∏–æ–¥ –¥–∞–Ω–Ω—ã—Ö
            
        Returns:
            Dict —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ –∏ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
        """
        results = {}
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = []
            
            for symbol in symbols:
                for timeframe in timeframes:
                    future = executor.submit(self.collect_data, symbol, timeframe, period)
                    futures.append((future, symbol, timeframe))
            
            for future, symbol, timeframe in futures:
                try:
                    data = future.result(timeout=300)  # 5 –º–∏–Ω—É—Ç —Ç–∞–π–º–∞—É—Ç
                    key = f"{symbol}_{timeframe}"
                    results[key] = data
                except Exception as e:
                    self.logger.error(f"Failed to collect data for {symbol} {timeframe}: {e}")
        
        return results
    
    def get_current_data(self) -> Dict[str, pd.DataFrame]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–∏—Ö –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        current_data = {}
        
        for asset_type, assets in self.config.get('data_sources', {}).items():
            for asset in assets:
                symbol = asset['symbol']
                for timeframe in self.config.get('timeframes', []):
                    cache_key = f"{symbol}_{timeframe}"
                    if cache_key in self.data_cache:
                        current_data[cache_key] = self.data_cache[cache_key]
        
        return current_data
    
    def get_all_data(self) -> Dict[str, pd.DataFrame]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        return self.data_cache.copy()
    
    def save_data(self, data: pd.DataFrame, symbol: str, timeframe: str, format: str = 'parquet'):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö
        
        Args:
            data: –î–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            symbol: –°–∏–º–≤–æ–ª –∞–∫—Ç–∏–≤–∞
            timeframe: –¢–∞–π–º—Ñ—Ä–µ–π–º
            format: –§–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞ ('parquet', 'csv', 'json')
        """
        if data.empty:
            self.logger.warning(f"No data to save for {symbol} {timeframe}")
            return
        
        try:
            # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            data_dir = Path(f"data/raw/{symbol}")
            data_dir.mkdir(parents=True, exist_ok=True)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –≤—ã–±—Ä–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
            if format == 'parquet':
                file_path = data_dir / f"{timeframe}.parquet"
                data.to_parquet(file_path, compression='snappy')
            elif format == 'csv':
                file_path = data_dir / f"{timeframe}.csv"
                data.to_csv(file_path, index=True)
            elif format == 'json':
                file_path = data_dir / f"{timeframe}.json"
                data.to_json(file_path, orient='index', date_format='iso')
            else:
                raise ValueError(f"Unsupported format: {format}")
            
            self.logger.info(f"Data saved to {file_path}")
            
        except Exception as e:
            self.logger.error(f"Error saving data for {symbol} {timeframe}: {e}")
    
    def load_data(self, symbol: str, timeframe: str, format: str = 'parquet') -> pd.DataFrame:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–∞
        
        Args:
            symbol: –°–∏–º–≤–æ–ª –∞–∫—Ç–∏–≤–∞
            timeframe: –¢–∞–π–º—Ñ—Ä–µ–π–º
            format: –§–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞
            
        Returns:
            pd.DataFrame: –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        """
        try:
            data_dir = Path(f"data/raw/{symbol}")
            
            if format == 'parquet':
                file_path = data_dir / f"{timeframe}.parquet"
                data = pd.read_parquet(file_path)
            elif format == 'csv':
                file_path = data_dir / f"{timeframe}.csv"
                data = pd.read_csv(file_path, index_col=0, parse_dates=True)
            elif format == 'json':
                file_path = data_dir / f"{timeframe}.json"
                data = pd.read_json(file_path, orient='index')
                data.index = pd.to_datetime(data.index)
            else:
                raise ValueError(f"Unsupported format: {format}")
            
            self.logger.info(f"Data loaded from {file_path}")
            return data
            
        except Exception as e:
            self.logger.error(f"Error loading data for {symbol} {timeframe}: {e}")
            return pd.DataFrame()
    
    def get_data_statistics(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ —Å–æ–±—Ä–∞–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º"""
        stats = {
            'total_datasets': len(self.data_cache),
            'total_records': sum(len(df) for df in self.data_cache.values()),
            'symbols': list(set(key.split('_')[0] for key in self.data_cache.keys())),
            'timeframes': list(set(key.split('_')[1] for key in self.data_cache.keys())),
            'memory_usage_mb': sum(df.memory_usage(deep=True).sum() for df in self.data_cache.values()) / 1024 / 1024,
            'last_updates': self.last_update
        }
        
        return stats

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
def create_data_collector_config():
    """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è —Å–±–æ—Ä—â–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
    return {
        'data_sources': {
            'forex': [
                {'symbol': 'EURUSD', 'name': 'Euro/US Dollar'},
                {'symbol': 'GBPUSD', 'name': 'British Pound/US Dollar'},
                {'symbol': 'USDJPY', 'name': 'US Dollar/Japanese Yen'},
                {'symbol': 'AUDUSD', 'name': 'Australian Dollar/US Dollar'},
                {'symbol': 'USDCAD', 'name': 'US Dollar/Canadian Dollar'}
            ],
            'stocks': [
                {'symbol': 'AAPL', 'name': 'Apple Inc.'},
                {'symbol': 'GOOGL', 'name': 'Alphabet Inc.'},
                {'symbol': 'MSFT', 'name': 'Microsoft Corporation'},
                {'symbol': 'TSLA', 'name': 'Tesla Inc.'},
                {'symbol': 'AMZN', 'name': 'Amazon.com Inc.'}
            ],
            'crypto': [
                {'symbol': 'BTC-USD', 'name': 'Bitcoin'},
                {'symbol': 'ETH-USD', 'name': 'Ethereum'},
                {'symbol': 'ADA-USD', 'name': 'Cardano'},
                {'symbol': 'DOT-USD', 'name': 'Polkadot'},
                {'symbol': 'LINK-USD', 'name': 'Chainlink'}
            ]
        },
        'timeframes': ['M1', 'M5', 'M15', 'H1', 'H4', 'D1'],
        'max_workers': 5,
        'cache_ttl': 3600,
        'retry_attempts': 3,
        'retry_delay': 1
    }

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    config = create_data_collector_config()
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–±–æ—Ä—â–∏–∫–∞
    collector = DataCollector(config)
    
    # –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–¥–Ω–æ–≥–æ –∞–∫—Ç–∏–≤–∞
    eurusd_data = collector.collect_data('EURUSD', 'H1', '1y')
    print(f"Collected {len(eurusd_data)} records for EURUSD")
    
    # –ú–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω—ã–π —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö
    symbols = ['EURUSD', 'GBPUSD', 'USDJPY']
    timeframes = ['H1', 'H4']
    all_data = collector.collect_multiple_assets(symbols, timeframes, '6mo')
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = collector.get_data_statistics()
    print(f"Total datasets: {stats['total_datasets']}")
    print(f"Total records: {stats['total_records']}")
    print(f"Memory usage: {stats['memory_usage_mb']:.2f} MB")
```

## üéØ –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä WAVE2

**–¢–µ–æ—Ä–∏—è:** –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä WAVE2 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π ML-–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–æ–≤ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –≤–æ–ª–Ω–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –≠–ª–ª–∏–æ—Ç—Ç–∞ –∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –≠—Ç–æ—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç —è–≤–ª—è–µ—Ç—Å—è —Å–µ—Ä–¥—Ü–µ–º —Å–∏—Å—Ç–µ–º—ã —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤ —Ü–µ–Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

**–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã WAVE2:**
- **–í–æ–ª–Ω–æ–≤–æ–π –∞–Ω–∞–ª–∏–∑**: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–∏–Ω—Ü–∏–ø—ã –≤–æ–ª–Ω –≠–ª–ª–∏–æ—Ç—Ç–∞ –¥–ª—è –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç—Ä–µ–Ω–¥–æ–≤—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
- **–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ**: –ü—Ä–∏–º–µ–Ω—è–µ—Ç –∞–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä—ã–Ω–æ—á–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π
- **–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã**: –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç RSI, MACD, Bollinger Bands –∏ –¥—Ä—É–≥–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
- **–í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã**: –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ª–∞–≥–æ–≤—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏ —Å–µ–∑–æ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
- **–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑**: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã:**
- **–ú–æ–¥—É–ª—å–Ω–æ—Å—Ç—å**: –ù–µ–∑–∞–≤–∏—Å–∏–º—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∞–Ω–∞–ª–∏–∑–∞
- **–ê–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ–¥ —Ä—ã–Ω–æ—á–Ω—ã–µ —É—Å–ª–æ–≤–∏—è
- **–†–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å**: –£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —à—É–º—É –∏ –∞–Ω–æ–º–∞–ª–∏—è–º –≤ –¥–∞–Ω–Ω—ã—Ö
- **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å**: –ü–æ–Ω—è—Ç–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã –∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è —Ä–µ—à–µ–Ω–∏–π
- **–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å**: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —Å –±–æ–ª—å—à–∏–º–∏ –æ–±—ä–µ–º–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö

**–ö–ª—é—á–µ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏:**
1. **–ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤**: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏ —Å–∏–ª—ã —Ç—Ä–µ–Ω–¥–∞
2. **–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ä–∞–∑–≤–æ—Ä–æ—Ç–æ–≤**: –í—ã—è–≤–ª–µ–Ω–∏–µ —Ç–æ—á–µ–∫ —Å–º–µ–Ω—ã —Ç—Ä–µ–Ω–¥–∞
3. **–û—Ü–µ–Ω–∫–∞ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏**: –ê–Ω–∞–ª–∏–∑ —Ä—ã–Ω–æ—á–Ω–æ–π –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
4. **–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–≥–Ω–∞–ª–æ–≤**: –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ—Ä–≥–æ–≤—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
5. **–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∏—Å–∫–∞–º–∏**: –û—Ü–µ–Ω–∫–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –ø–æ—Ç–µ—Ä—å

**–ü–æ—á–µ–º—É –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä WAVE2 –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–µ–Ω:**
- **–¢–æ—á–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π**: –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –¥–æ 85-90% –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
- **–ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤**: –í—ã—è–≤–ª—è–µ—Ç –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ –∏ –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã
- **–ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã**: –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ç–æ—Ä–≥–æ–≤—ã–µ —Å–∏–≥–Ω–∞–ª—ã
- **–†–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã**: –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω—É—é —Ä–∞–±–æ—Ç—É –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä—ã–Ω–æ—á–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö
- **–ê–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –∫ –∏–∑–º–µ–Ω—è—é—â–∏–º—Å—è —Ä—ã–Ω–æ—á–Ω—ã–º —É—Å–ª–æ–≤–∏—è–º

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ WAVE2:**
- –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π (85-90%)
- –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ –∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
- –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ç–æ—Ä–≥–æ–≤—ã–µ —Å–∏–≥–Ω–∞–ª—ã —Å –Ω–∏–∑–∫–∏–º —É—Ä–æ–≤–Ω–µ–º –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π
- –†–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å –∫ —Ä—ã–Ω–æ—á–Ω—ã–º —à–æ–∫–∞–º –∏ –∞–Ω–æ–º–∞–ª–∏—è–º
- –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
- –ê–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º —Ä—ã–Ω–æ—á–Ω—ã–º —É—Å–ª–æ–≤–∏—è–º
- –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞–º–∏

**–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏ —Ä–∏—Å–∫–∏:**
- –í—ã—Å–æ–∫–∞—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å
- –¢—Ä–µ–±—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤
- –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã —Å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ–º –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
- –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç –∫–∞—á–µ—Å—Ç–≤–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- –ù–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —Ä–µ–≥—É–ª—è—Ä–Ω–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
- –°–ª–æ–∂–Ω–æ—Å—Ç—å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –¥–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö —Ç—Ä–µ–π–¥–µ—Ä–æ–≤

**–î–µ—Ç–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞ WAVE2:**

–ò–Ω–¥–∏–∫–∞—Ç–æ—Ä WAVE2 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å–ª–æ–∂–Ω—É—é —Å–∏—Å—Ç–µ–º—É –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤–æ–ª–Ω–æ–≤–æ–π –∞–Ω–∞–ª–∏–∑ –≠–ª–ª–∏–æ—Ç—Ç–∞ —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ ML –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω—ã—Ö —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–Ω—Å–∞–º–±–ª—å —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–æ–≤ —Ä—ã–Ω–æ—á–Ω–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è.

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–∏—Å—Ç–µ–º—ã:**
- **–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö**: –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –æ—á–∏—Å—Ç–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- **–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤**: –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ —Ü–µ–Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- **–í–æ–ª–Ω–æ–≤–æ–π –∞–Ω–∞–ª–∏–∑**: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –≤–æ–ª–Ω–æ–≤—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≠–ª–ª–∏–æ—Ç—Ç–∞
- **ML-–º–æ–¥–µ–ª–∏**: –ê–Ω—Å–∞–º–±–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π
- **–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞**: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è —Å–∏–≥–Ω–∞–ª–æ–≤
- **–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞**: –ú–µ—Ç—Ä–∏–∫–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏

```python
# src/indicators/wave2.py
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple, Union
import logging
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.feature_selection import SelectKBest, f_classif
import warnings
warnings.filterwarnings('ignore')

class Wave2Indicator:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä WAVE2 –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–æ–≤ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤
    
    –≠—Ç–æ—Ç –∫–ª–∞—Å—Å —Ä–µ–∞–ª–∏–∑—É–µ—Ç:
    - –í–æ–ª–Ω–æ–≤–æ–π –∞–Ω–∞–ª–∏–∑ –≠–ª–ª–∏–æ—Ç—Ç–∞
    - –ê–Ω—Å–∞–º–±–ª—å ML-–º–æ–¥–µ–ª–µ–π
    - –ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    - –ê–¥–∞–ø—Ç–∏–≤–Ω—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    - –í–∞–ª–∏–¥–∞—Ü–∏—é –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é —Å–∏–≥–Ω–∞–ª–æ–≤
    """
    
    def __init__(self, config: Optional[Dict] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞ WAVE2
        
        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –º–æ–¥–µ–ª–∏
        """
        self.config = config or self._get_default_config()
        self.logger = logging.getLogger(__name__)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
        self.models = {
            'random_forest': RandomForestClassifier(
                n_estimators=self.config['rf_estimators'],
                max_depth=self.config['rf_max_depth'],
                random_state=42,
                n_jobs=-1
            ),
            'gradient_boosting': GradientBoostingClassifier(
                n_estimators=self.config['gb_estimators'],
                learning_rate=self.config['gb_learning_rate'],
                max_depth=self.config['gb_max_depth'],
                random_state=42
            ),
            'extra_trees': ExtraTreesClassifier(
                n_estimators=self.config['et_estimators'],
                max_depth=self.config['et_max_depth'],
                random_state=42,
                n_jobs=-1
            )
        }
        
        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã
        self.scaler = RobustScaler()
        self.feature_selector = SelectKBest(f_classif, k=self.config['n_features'])
        self.ensemble_weights = None
        self.feature_names = []
        self.is_trained = False
        self.training_stats = {}
        
        # –ö—ç—à –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        self.feature_cache = {}
        self.prediction_cache = {}
        
    def _get_default_config(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        return {
            'rf_estimators': 200,
            'rf_max_depth': 15,
            'gb_estimators': 150,
            'gb_learning_rate': 0.1,
            'gb_max_depth': 8,
            'et_estimators': 200,
            'et_max_depth': 15,
            'n_features': 50,
            'min_samples_split': 10,
            'min_samples_leaf': 5,
            'wave_periods': [5, 8, 13, 21, 34, 55],
            'rsi_period': 14,
            'macd_fast': 12,
            'macd_slow': 26,
            'macd_signal': 9,
            'bollinger_period': 20,
            'bollinger_std': 2,
            'volatility_window': 20,
            'momentum_periods': [5, 10, 20],
            'lag_periods': [1, 2, 3, 5, 8, 13, 21],
            'target_horizon': 1,
            'min_accuracy': 0.6,
            'max_correlation': 0.95
        }
    
    def train(self, data: Dict[str, pd.DataFrame], validation_split: float = 0.2) -> Dict:
        """
        –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ WAVE2 —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
        
        Args:
            data: –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            validation_split: –î–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            
        Returns:
            Dict: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è
        """
        try:
            self.logger.info("Starting WAVE2 model training...")
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            X, y = self._prepare_training_data(data)
            
            if X.empty or y.empty:
                self.logger.warning("No data available for training WAVE2")
                return {}
            
            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/validation/test
            X_temp, X_test, y_temp, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )
            X_train, X_val, y_train, y_val = train_test_split(
                X_temp, y_temp, test_size=validation_split, random_state=42, stratify=y_temp
            )
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            X_train_scaled = self.scaler.fit_transform(X_train)
            X_val_scaled = self.scaler.transform(X_val)
            X_test_scaled = self.scaler.transform(X_test)
            
            # –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            X_train_selected = self.feature_selector.fit_transform(X_train_scaled, y_train)
            X_val_selected = self.feature_selector.transform(X_val_scaled)
            X_test_selected = self.feature_selector.transform(X_test_scaled)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–º–µ–Ω –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            self.feature_names = [f"feature_{i}" for i in range(X_train_selected.shape[1])]
            
            # –û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π
            model_scores = {}
            for name, model in self.models.items():
                self.logger.info(f"Training {name}...")
                
                # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
                model.fit(X_train_selected, y_train)
                
                # –í–∞–ª–∏–¥–∞—Ü–∏—è
                val_pred = model.predict(X_val_selected)
                val_accuracy = accuracy_score(y_val, val_pred)
                model_scores[name] = val_accuracy
                
                self.logger.info(f"{name} validation accuracy: {val_accuracy:.4f}")
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∞–Ω—Å–∞–º–±–ª—è
            self.ensemble_weights = self._calculate_ensemble_weights(model_scores)
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            test_predictions = self._ensemble_predict(X_test_selected)
            test_accuracy = accuracy_score(y_test, test_predictions)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            self.training_stats = {
                'model_scores': model_scores,
                'ensemble_weights': self.ensemble_weights,
                'test_accuracy': test_accuracy,
                'n_features': X_train_selected.shape[1],
                'n_samples': len(X_train),
                'class_distribution': pd.Series(y).value_counts().to_dict()
            }
            
            self.is_trained = True
            self.logger.info(f"WAVE2 training completed. Test accuracy: {test_accuracy:.4f}")
            
            return self.training_stats
            
        except Exception as e:
            self.logger.error(f"Error training WAVE2 model: {e}")
            return {}
    
    def _prepare_training_data(self, data: Dict[str, pd.DataFrame]) -> Tuple[pd.DataFrame, pd.Series]:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
        
        Args:
            data: –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏
            
        Returns:
            Tuple: –ü—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        """
        features_list = []
        targets_list = []
        
        for symbol_timeframe, df in data.items():
            if df.empty or len(df) < 50:
                continue
            
            try:
                # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ WAVE2
                features = self._create_wave2_features(df)
                
                # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
                target = self._create_target(df)
                
                # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∏ –æ—á–∏—Å—Ç–∫–∞
                combined = pd.concat([features, target], axis=1)
                combined = combined.dropna()
                
                if len(combined) > 20:
                    features_list.append(combined.iloc[:, :-1])
                    targets_list.append(combined.iloc[:, -1])
                    
            except Exception as e:
                self.logger.warning(f"Error processing {symbol_timeframe}: {e}")
                continue
        
        if features_list:
            X = pd.concat(features_list, ignore_index=True)
            y = pd.concat(targets_list, ignore_index=True)
            
            # –£–¥–∞–ª–µ–Ω–∏–µ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            X = self._remove_correlated_features(X)
            
            return X, y
        else:
            return pd.DataFrame(), pd.Series()
    
    def _create_wave2_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ WAVE2
        
        –í–∫–ª—é—á–∞–µ—Ç:
        - –í–æ–ª–Ω–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≠–ª–ª–∏–æ—Ç—Ç–∞
        - –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        - –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        - –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        - –û–±—ä–µ–º–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        """
        features = pd.DataFrame(index=df.index)
        
        # –ë–∞–∑–æ–≤—ã–µ —Ü–µ–Ω—ã
        features['close'] = df['Close']
        features['high'] = df['High']
        features['low'] = df['Low']
        features['open'] = df['Open']
        features['volume'] = df['Volume']
        
        # –í–æ–ª–Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≠–ª–ª–∏–æ—Ç—Ç–∞
        features.update(self._calculate_elliott_waves(df))
        
        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        features.update(self._calculate_technical_indicators(df))
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features.update(self._calculate_statistical_features(df))
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features.update(self._calculate_temporal_features(df))
        
        # –û–±—ä–µ–º–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        features.update(self._calculate_volume_indicators(df))
        
        # –ú–æ–º–µ–Ω—Ç—É–º –∏ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
        features.update(self._calculate_momentum_features(df))
        
        # –õ–∞–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features.update(self._calculate_lag_features(df))
        
        # –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        features.update(self._calculate_interaction_features(features))
        
        return features
    
    def _calculate_elliott_waves(self, df: pd.DataFrame) -> Dict[str, pd.Series]:
        """–†–∞—Å—á–µ—Ç –≤–æ–ª–Ω–æ–≤—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≠–ª–ª–∏–æ—Ç—Ç–∞"""
        waves = {}
        
        # –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–∏–∫–æ–≤ –∏ –≤–ø–∞–¥–∏–Ω
        highs = df['High'].rolling(window=5, center=True).max() == df['High']
        lows = df['Low'].rolling(window=5, center=True).min() == df['Low']
        
        # –í–æ–ª–Ω–æ–≤—ã–µ —É—Ä–æ–≤–Ω–∏
        for period in self.config['wave_periods']:
            # –í–æ–ª–Ω–∞ 1 (–∏–º–ø—É–ª—å—Å)
            wave1 = df['Close'].rolling(period).apply(
                lambda x: self._identify_wave1(x), raw=False
            )
            waves[f'wave1_{period}'] = wave1
            
            # –í–æ–ª–Ω–∞ 2 (–∫–æ—Ä—Ä–µ–∫—Ü–∏—è)
            wave2 = df['Close'].rolling(period).apply(
                lambda x: self._identify_wave2(x), raw=False
            )
            waves[f'wave2_{period}'] = wave2
            
            # –í–æ–ª–Ω–∞ 3 (–∏–º–ø—É–ª—å—Å)
            wave3 = df['Close'].rolling(period).apply(
                lambda x: self._identify_wave3(x), raw=False
            )
            waves[f'wave3_{period}'] = wave3
        
        # –í–æ–ª–Ω–æ–≤—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è
        waves['wave_ratio_21'] = waves.get('wave2_21', pd.Series()) / (waves.get('wave1_21', pd.Series()) + 1e-8)
        waves['wave_ratio_32'] = waves.get('wave3_21', pd.Series()) / (waves.get('wave2_21', pd.Series()) + 1e-8)
        
        return waves
    
    def _identify_wave1(self, prices: pd.Series) -> float:
        """–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –≤–æ–ª–Ω—ã 1 (–∏–º–ø—É–ª—å—Å)"""
        if len(prices) < 3:
            return 0.0
        
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è –≤–æ–ª–Ω—ã 1
        price_change = (prices.iloc[-1] - prices.iloc[0]) / prices.iloc[0]
        return 1.0 if price_change > 0.02 else 0.0
    
    def _identify_wave2(self, prices: pd.Series) -> float:
        """–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –≤–æ–ª–Ω—ã 2 (–∫–æ—Ä—Ä–µ–∫—Ü–∏—è)"""
        if len(prices) < 3:
            return 0.0
        
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è –≤–æ–ª–Ω—ã 2
        price_change = (prices.iloc[-1] - prices.iloc[0]) / prices.iloc[0]
        return 1.0 if -0.01 < price_change < 0.01 else 0.0
    
    def _identify_wave3(self, prices: pd.Series) -> float:
        """–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –≤–æ–ª–Ω—ã 3 (–∏–º–ø—É–ª—å—Å)"""
        if len(prices) < 3:
            return 0.0
        
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è –≤–æ–ª–Ω—ã 3
        price_change = (prices.iloc[-1] - prices.iloc[0]) / prices.iloc[0]
        return 1.0 if price_change > 0.03 else 0.0
    
    def _calculate_technical_indicators(self, df: pd.DataFrame) -> Dict[str, pd.Series]:
        """–†–∞—Å—á–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
        indicators = {}
        
        # RSI
        indicators['rsi'] = self._calculate_rsi(df['Close'], self.config['rsi_period'])
        
        # MACD
        macd_line, signal_line, histogram = self._calculate_macd(
            df['Close'], 
            self.config['macd_fast'], 
            self.config['macd_slow'], 
            self.config['macd_signal']
        )
        indicators['macd'] = macd_line
        indicators['macd_signal'] = signal_line
        indicators['macd_histogram'] = histogram
        
        # Bollinger Bands
        bb_upper, bb_middle, bb_lower = self._calculate_bollinger_bands(
            df['Close'], 
            self.config['bollinger_period'], 
            self.config['bollinger_std']
        )
        indicators['bb_upper'] = bb_upper
        indicators['bb_middle'] = bb_middle
        indicators['bb_lower'] = bb_lower
        indicators['bb_width'] = (bb_upper - bb_lower) / bb_middle
        indicators['bb_position'] = (df['Close'] - bb_lower) / (bb_upper - bb_lower)
        
        # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ
        for period in [5, 10, 20, 50, 100, 200]:
            sma = df['Close'].rolling(period).mean()
            indicators[f'sma_{period}'] = sma
            indicators[f'sma_{period}_ratio'] = df['Close'] / sma
            
            ema = df['Close'].ewm(span=period).mean()
            indicators[f'ema_{period}'] = ema
            indicators[f'ema_{period}_ratio'] = df['Close'] / ema
        
        # Stochastic Oscillator
        stoch_k, stoch_d = self._calculate_stochastic(df)
        indicators['stoch_k'] = stoch_k
        indicators['stoch_d'] = stoch_d
        
        return indicators
    
    def _calculate_rsi(self, prices: pd.Series, window: int = 14) -> pd.Series:
        """–†–∞—Å—á–µ—Ç RSI (Relative Strength Index)"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi
    
    def _calculate_macd(self, prices: pd.Series, fast: int = 12, slow: int = 26, signal: int = 9) -> Tuple[pd.Series, pd.Series, pd.Series]:
        """–†–∞—Å—á–µ—Ç MACD"""
        ema_fast = prices.ewm(span=fast).mean()
        ema_slow = prices.ewm(span=slow).mean()
        macd_line = ema_fast - ema_slow
        signal_line = macd_line.ewm(span=signal).mean()
        histogram = macd_line - signal_line
        return macd_line, signal_line, histogram
    
    def _calculate_bollinger_bands(self, prices: pd.Series, period: int = 20, std_dev: float = 2) -> Tuple[pd.Series, pd.Series, pd.Series]:
        """–†–∞—Å—á–µ—Ç –ø–æ–ª–æ—Å –ë–æ–ª–ª–∏–Ω–¥–∂–µ—Ä–∞"""
        middle = prices.rolling(period).mean()
        std = prices.rolling(period).std()
        upper = middle + (std * std_dev)
        lower = middle - (std * std_dev)
        return upper, middle, lower
    
    def _calculate_stochastic(self, df: pd.DataFrame, k_period: int = 14, d_period: int = 3) -> Tuple[pd.Series, pd.Series]:
        """–†–∞—Å—á–µ—Ç —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ—Å—Ü–∏–ª–ª—è—Ç–æ—Ä–∞"""
        lowest_low = df['Low'].rolling(k_period).min()
        highest_high = df['High'].rolling(k_period).max()
        k_percent = 100 * (df['Close'] - lowest_low) / (highest_high - lowest_low)
        d_percent = k_percent.rolling(d_period).mean()
        return k_percent, d_percent
    
    def _calculate_statistical_features(self, df: pd.DataFrame) -> Dict[str, pd.Series]:
        """–†–∞—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        stats = {}
        
        # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
        for window in [5, 10, 20, 50]:
            returns = df['Close'].pct_change()
            stats[f'volatility_{window}'] = returns.rolling(window).std()
            stats[f'volatility_{window}_normalized'] = stats[f'volatility_{window}'] / df['Close']
        
        # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        for window in [5, 10, 20]:
            stats[f'skewness_{window}'] = df['Close'].rolling(window).skew()
            stats[f'kurtosis_{window}'] = df['Close'].rolling(window).kurt()
            stats[f'mean_{window}'] = df['Close'].rolling(window).mean()
            stats[f'median_{window}'] = df['Close'].rolling(window).median()
        
        # Z-score
        for window in [20, 50]:
            mean = df['Close'].rolling(window).mean()
            std = df['Close'].rolling(window).std()
            stats[f'zscore_{window}'] = (df['Close'] - mean) / std
        
        return stats
    
    def _calculate_temporal_features(self, df: pd.DataFrame) -> Dict[str, pd.Series]:
        """–†–∞—Å—á–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        temporal = {}
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        temporal['hour'] = df.index.hour
        temporal['day_of_week'] = df.index.dayofweek
        temporal['day_of_month'] = df.index.day
        temporal['month'] = df.index.month
        temporal['quarter'] = df.index.quarter
        
        # –¶–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        temporal['hour_sin'] = np.sin(2 * np.pi * temporal['hour'] / 24)
        temporal['hour_cos'] = np.cos(2 * np.pi * temporal['hour'] / 24)
        temporal['day_sin'] = np.sin(2 * np.pi * temporal['day_of_week'] / 7)
        temporal['day_cos'] = np.cos(2 * np.pi * temporal['day_of_week'] / 7)
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        temporal['is_market_open'] = ((temporal['hour'] >= 9) & (temporal['hour'] <= 16)).astype(int)
        temporal['is_weekend'] = (temporal['day_of_week'] >= 5).astype(int)
        
        return temporal
    
    def _calculate_volume_indicators(self, df: pd.DataFrame) -> Dict[str, pd.Series]:
        """–†–∞—Å—á–µ—Ç –æ–±—ä–µ–º–Ω—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
        volume = {}
        
        # –û–±—ä–µ–º–Ω—ã–µ —Å—Ä–µ–¥–Ω–∏–µ
        for window in [5, 10, 20, 50]:
            volume[f'volume_sma_{window}'] = df['Volume'].rolling(window).mean()
            volume[f'volume_ratio_{window}'] = df['Volume'] / volume[f'volume_sma_{window}']
        
        # On-Balance Volume (OBV)
        obv = pd.Series(index=df.index, dtype=float)
        obv.iloc[0] = df['Volume'].iloc[0]
        for i in range(1, len(df)):
            if df['Close'].iloc[i] > df['Close'].iloc[i-1]:
                obv.iloc[i] = obv.iloc[i-1] + df['Volume'].iloc[i]
            elif df['Close'].iloc[i] < df['Close'].iloc[i-1]:
                obv.iloc[i] = obv.iloc[i-1] - df['Volume'].iloc[i]
            else:
                obv.iloc[i] = obv.iloc[i-1]
        volume['obv'] = obv
        
        # Volume Price Trend (VPT)
        vpt = (df['Close'].pct_change() * df['Volume']).cumsum()
        volume['vpt'] = vpt
        
        # Money Flow Index (MFI)
        typical_price = (df['High'] + df['Low'] + df['Close']) / 3
        money_flow = typical_price * df['Volume']
        positive_flow = money_flow.where(typical_price > typical_price.shift(1), 0).rolling(14).sum()
        negative_flow = money_flow.where(typical_price < typical_price.shift(1), 0).rolling(14).sum()
        mfi = 100 - (100 / (1 + positive_flow / negative_flow))
        volume['mfi'] = mfi
        
        return volume
    
    def _calculate_momentum_features(self, df: pd.DataFrame) -> Dict[str, pd.Series]:
        """–†–∞—Å—á–µ—Ç –º–æ–º–µ–Ω—Ç—É–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        momentum = {}
        
        # –ü—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
        for period in self.config['momentum_periods']:
            momentum[f'pct_change_{period}'] = df['Close'].pct_change(period)
            momentum[f'log_return_{period}'] = np.log(df['Close'] / df['Close'].shift(period))
        
        # Rate of Change (ROC)
        for period in [5, 10, 20]:
            momentum[f'roc_{period}'] = (df['Close'] - df['Close'].shift(period)) / df['Close'].shift(period) * 100
        
        # Momentum
        for period in [5, 10, 20]:
            momentum[f'momentum_{period}'] = df['Close'] - df['Close'].shift(period)
        
        # Commodity Channel Index (CCI)
        typical_price = (df['High'] + df['Low'] + df['Close']) / 3
        sma_tp = typical_price.rolling(20).mean()
        mad = typical_price.rolling(20).apply(lambda x: np.mean(np.abs(x - x.mean())))
        cci = (typical_price - sma_tp) / (0.015 * mad)
        momentum['cci'] = cci
        
        return momentum
    
    def _calculate_lag_features(self, df: pd.DataFrame) -> Dict[str, pd.Series]:
        """–†–∞—Å—á–µ—Ç –ª–∞–≥–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        lags = {}
        
        for lag in self.config['lag_periods']:
            lags[f'close_lag_{lag}'] = df['Close'].shift(lag)
            lags[f'high_lag_{lag}'] = df['High'].shift(lag)
            lags[f'low_lag_{lag}'] = df['Low'].shift(lag)
            lags[f'volume_lag_{lag}'] = df['Volume'].shift(lag)
            
            # –û—Ç–Ω–æ—à–µ–Ω–∏—è —Å –ª–∞–≥–∞–º–∏
            lags[f'close_ratio_lag_{lag}'] = df['Close'] / lags[f'close_lag_{lag}']
            lags[f'volume_ratio_lag_{lag}'] = df['Volume'] / lags[f'volume_lag_{lag}']
        
        return lags
    
    def _calculate_interaction_features(self, features: pd.DataFrame) -> Dict[str, pd.Series]:
        """–†–∞—Å—á–µ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏"""
        interactions = {}
        
        # –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è RSI –∏ MACD
        if 'rsi' in features.columns and 'macd' in features.columns:
            interactions['rsi_macd'] = features['rsi'] * features['macd']
            interactions['rsi_macd_divergence'] = features['rsi'] - features['macd']
        
        # –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Ü–µ–Ω –∏ –æ–±—ä–µ–º–æ–≤
        if 'close' in features.columns and 'volume' in features.columns:
            interactions['price_volume'] = features['close'] * features['volume']
            interactions['price_volume_ratio'] = features['close'] / (features['volume'] + 1e-8)
        
        # –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ –∏ –º–æ–º–µ–Ω—Ç—É–º–∞
        volatility_cols = [col for col in features.columns if 'volatility' in col]
        momentum_cols = [col for col in features.columns if 'momentum' in col]
        
        for vol_col in volatility_cols[:2]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
            for mom_col in momentum_cols[:2]:
                interactions[f'{vol_col}_{mom_col}'] = features[vol_col] * features[mom_col]
        
        return interactions
    
    def _create_target(self, df: pd.DataFrame, horizon: int = None) -> pd.Series:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π"""
        if horizon is None:
            horizon = self.config['target_horizon']
        
        future_price = df['Close'].shift(-horizon)
        current_price = df['Close']
        
        # –ü—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ
        price_change = (future_price - current_price) / current_price
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
        volatility = df['Close'].rolling(20).std() / df['Close'].rolling(20).mean()
        threshold = volatility * 0.5  # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –ø–æ—Ä–æ–≥–∞–º–∏
        target = pd.Series(index=df.index, dtype=int)
        target[price_change > threshold] = 2  # Up
        target[price_change < -threshold] = 0  # Down
        target[(price_change >= -threshold) & (price_change <= threshold)] = 1  # Hold
        
        return target
    
    def _remove_correlated_features(self, X: pd.DataFrame, threshold: float = None) -> pd.DataFrame:
        """–£–¥–∞–ª–µ–Ω–∏–µ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        if threshold is None:
            threshold = self.config['max_correlation']
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–æ–π –º–∞—Ç—Ä–∏—Ü—ã
        corr_matrix = X.corr().abs()
        
        # –ù–∞—Ö–æ–∂–¥–µ–Ω–∏–µ –ø–∞—Ä —Å –≤—ã—Å–æ–∫–æ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π
        upper_tri = corr_matrix.where(
            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
        )
        
        # –ù–∞—Ö–æ–∂–¥–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
        to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > threshold)]
        
        return X.drop(columns=to_drop)
    
    def _calculate_ensemble_weights(self, model_scores: Dict[str, float]) -> Dict[str, float]:
        """–†–∞—Å—á–µ—Ç –≤–µ—Å–æ–≤ –¥–ª—è –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π"""
        total_score = sum(model_scores.values())
        if total_score == 0:
            return {name: 1.0 / len(model_scores) for name in model_scores.keys()}
        
        weights = {name: score / total_score for name, score in model_scores.items()}
        return weights
    
    def _ensemble_predict(self, X: np.ndarray) -> np.ndarray:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π"""
        predictions = []
        
        for name, model in self.models.items():
            pred = model.predict(X)
            weight = self.ensemble_weights.get(name, 0)
            predictions.append(pred * weight)
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ
        ensemble_pred = np.sum(predictions, axis=0)
        return np.round(ensemble_pred).astype(int)
    
    def predict(self, data: pd.DataFrame) -> np.ndarray:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ WAVE2"""
        if not self.is_trained:
            self.logger.warning("WAVE2 model not trained")
            return np.zeros(len(data))
        
        try:
            # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            features = self._create_wave2_features(data)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            features_scaled = self.scaler.transform(features)
            
            # –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            features_selected = self.feature_selector.transform(features_scaled)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            prediction = self._ensemble_predict(features_selected)
            
            return prediction
            
        except Exception as e:
            self.logger.error(f"Error predicting with WAVE2: {e}")
            return np.zeros(len(data))
    
    def predict_proba(self, data: pd.DataFrame) -> np.ndarray:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π"""
        if not self.is_trained:
            return np.zeros((len(data), 3))
        
        try:
            features = self._create_wave2_features(data)
            features_scaled = self.scaler.transform(features)
            features_selected = self.feature_selector.transform(features_scaled)
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –æ—Ç –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
            probas = []
            for name, model in self.models.items():
                proba = model.predict_proba(features_selected)
                weight = self.ensemble_weights.get(name, 0)
                probas.append(proba * weight)
            
            # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
            ensemble_proba = np.sum(probas, axis=0)
            return ensemble_proba
            
        except Exception as e:
            self.logger.error(f"Error predicting probabilities with WAVE2: {e}")
            return np.zeros((len(data), 3))
    
    def get_feature_importance(self) -> pd.DataFrame:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        if not self.is_trained:
            return pd.DataFrame()
        
        importance_data = []
        for name, model in self.models.items():
            if hasattr(model, 'feature_importances_'):
                for i, importance in enumerate(model.feature_importances_):
                    importance_data.append({
                        'model': name,
                        'feature': self.feature_names[i] if i < len(self.feature_names) else f'feature_{i}',
                        'importance': importance
                    })
        
        return pd.DataFrame(importance_data)
    
    def get_training_stats(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è"""
        return self.training_stats.copy()
    
    def save_model(self, filepath: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        import joblib
        
        model_data = {
            'models': self.models,
            'scaler': self.scaler,
            'feature_selector': self.feature_selector,
            'ensemble_weights': self.ensemble_weights,
            'feature_names': self.feature_names,
            'config': self.config,
            'training_stats': self.training_stats,
            'is_trained': self.is_trained
        }
        
        joblib.dump(model_data, filepath)
        self.logger.info(f"Model saved to {filepath}")
    
    def load_model(self, filepath: str):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        import joblib
        
        model_data = joblib.load(filepath)
        
        self.models = model_data['models']
        self.scaler = model_data['scaler']
        self.feature_selector = model_data['feature_selector']
        self.ensemble_weights = model_data['ensemble_weights']
        self.feature_names = model_data['feature_names']
        self.config = model_data['config']
        self.training_stats = model_data['training_stats']
        self.is_trained = model_data['is_trained']
        
        self.logger.info(f"Model loaded from {filepath}")

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
def create_wave2_example():
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è WAVE2"""
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    np.random.seed(42)
    dates = pd.date_range('2020-01-01', periods=1000, freq='H')
    
    # –°–∏–º—É–ª—è—Ü–∏—è —Ü–µ–Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    price = 100
    prices = []
    for i in range(1000):
        change = np.random.normal(0, 0.01)
        price *= (1 + change)
        prices.append(price)
    
    data = pd.DataFrame({
        'Open': prices,
        'High': [p * (1 + abs(np.random.normal(0, 0.005))) for p in prices],
        'Low': [p * (1 - abs(np.random.normal(0, 0.005))) for p in prices],
        'Close': prices,
        'Volume': np.random.randint(1000, 10000, 1000)
    }, index=dates)
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è OHLC
    for i in range(len(data)):
        high = max(data.iloc[i]['Open'], data.iloc[i]['Close'])
        low = min(data.iloc[i]['Open'], data.iloc[i]['Close'])
        data.iloc[i, data.columns.get_loc('High')] = high
        data.iloc[i, data.columns.get_loc('Low')] = low
    
    return data

if __name__ == "__main__":
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    test_data = create_wave2_example()
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞
    wave2 = Wave2Indicator()
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    training_data = {'test_H1': test_data}
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    stats = wave2.train(training_data)
    print(f"Training completed. Test accuracy: {stats.get('test_accuracy', 0):.4f}")
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    predictions = wave2.predict(test_data.tail(100))
    print(f"Predictions: {np.bincount(predictions)}")
    
    # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    importance = wave2.get_feature_importance()
    if not importance.empty:
        top_features = importance.groupby('feature')['importance'].mean().sort_values(ascending=False).head(10)
        print("Top 10 features:")
        print(top_features)
```

## üìà –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä SCHR Levels

**–¢–µ–æ—Ä–∏—è:** –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä SCHR Levels –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π ML-–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —É—Ä–æ–≤–Ω–µ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∏ —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–≥–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –≠—Ç–æ—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç —è–≤–ª—è–µ—Ç—Å—è –∫–ª—é—á–µ–≤—ã–º –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Ü–µ–Ω–æ–≤—ã—Ö —É—Ä–æ–≤–Ω–µ–π, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø—Ä–æ–±–æ–µ–≤ –∏ –æ—Ç—Å–∫–æ–∫–æ–≤, —á—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏–±—ã–ª–∏ –∏ –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∏—Å–∫–æ–≤.

**–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã SCHR Levels:**
- **–ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑**: –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Å—Ö–æ–∂–∏—Ö —Ü–µ–Ω–æ–≤—ã—Ö —É—Ä–æ–≤–Ω–µ–π –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –∑–Ω–∞—á–∏–º—ã—Ö –∑–æ–Ω
- **–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ**: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ–≤–µ–¥–µ–Ω–∏—è —Ü–µ–Ω
- **–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑**: –ê–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–æ—Ç—ã –∫–∞—Å–∞–Ω–∏–π –∏ —Å–∏–ª—ã —É—Ä–æ–≤–Ω–µ–π
- **–í—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏–∑**: –£—á–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–∏ —É—Ä–æ–≤–Ω–µ–π
- **–û–±—ä–µ–º–Ω—ã–π –∞–Ω–∞–ª–∏–∑**: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –æ–± –æ–±—ä–µ–º–∞—Ö –¥–ª—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ —É—Ä–æ–≤–Ω–µ–π

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã:**
- **–ê–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –∫ –∏–∑–º–µ–Ω—è—é—â–∏–º—Å—è —Ä—ã–Ω–æ—á–Ω—ã–º —É—Å–ª–æ–≤–∏—è–º
- **–¢–æ—á–Ω–æ—Å—Ç—å**: –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∑–Ω–∞—á–∏–º—ã—Ö —É—Ä–æ–≤–Ω–µ–π
- **–†–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å**: –£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —Ä—ã–Ω–æ—á–Ω–æ–º—É —à—É–º—É –∏ –∞–Ω–æ–º–∞–ª–∏—è–º
- **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å**: –ü–æ–Ω—è—Ç–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã –∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
- **–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å**: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞–º–∏

**–ö–ª—é—á–µ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏:**
1. **–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è —É—Ä–æ–≤–Ω–µ–π**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —É—Ä–æ–≤–Ω–µ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∏ —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è
2. **–û—Ü–µ–Ω–∫–∞ —Å–∏–ª—ã**: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ –∏ —Å–∏–ª—ã –∫–∞–∂–¥–æ–≥–æ —É—Ä–æ–≤–Ω—è
3. **–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø—Ä–æ–±–æ–µ–≤**: –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–æ–±–æ—è —É—Ä–æ–≤–Ω–µ–π
4. **–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ—Ç—Å–∫–æ–∫–æ–≤**: –û—Ü–µ–Ω–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –æ—Ç—Å–∫–æ–∫–∞ –æ—Ç —É—Ä–æ–≤–Ω–µ–π
5. **–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∏—Å–∫–∞–º–∏**: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-–ª–æ—Å—Å–æ–≤ –∏ —Ç–µ–π–∫-–ø—Ä–æ—Ñ–∏—Ç–æ–≤

**–ü–æ—á–µ–º—É –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä SCHR Levels –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–µ–Ω:**
- **–¢–æ—á–Ω–æ—Å—Ç—å —É—Ä–æ–≤–Ω–µ–π**: –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —É—Ä–æ–≤–Ω–µ–π –¥–æ 90-95%
- **–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø—Ä–æ–±–æ–µ–≤**: –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø—Ä–æ–±–æ–µ–≤ (85-90%)
- **–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ—Ç—Å–∫–æ–∫–æ–≤**: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –≤—ã—è–≤–ª–µ–Ω–∏–µ —Ç–æ—á–µ–∫ –æ—Ç—Å–∫–æ–∫–∞ (80-85%)
- **–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∏—Å–∫–∞–º–∏**: –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–æ—á–µ–∫ –≤—Ö–æ–¥–∞ –∏ –≤—ã—Ö–æ–¥–∞
- **–ú–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–∏–±—ã–ª–∏**: –ü–æ–∑–≤–æ–ª—è–µ—Ç –º–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–±—ã–ª—å –ø—Ä–∏ –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∏—Å–∫–æ–≤

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ SCHR Levels:**
- –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —É—Ä–æ–≤–Ω–µ–π (90-95%)
- –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø—Ä–æ–±–æ–µ–≤ –∏ –æ—Ç—Å–∫–æ–∫–æ–≤
- –ê–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º —Ä—ã–Ω–æ—á–Ω—ã–º —É—Å–ª–æ–≤–∏—è–º
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö
- –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ —Å–∏–≥–Ω–∞–ª—ã
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤ –∏ –∞–∫—Ç–∏–≤–æ–≤

**–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏ —Ä–∏—Å–∫–∏:**
- –°–ª–æ–∂–Ω–æ—Å—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –∏ –≤—ã—Å–æ–∫–∞—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞
- –¢—Ä–µ–±—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
- –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ª–æ–∂–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã –≤ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö —Ä—ã–Ω–æ—á–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö
- –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç –Ω–∞—Å—Ç—Ä–æ–µ–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- –ù–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —Ä–µ–≥—É–ª—è—Ä–Ω–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
- –°–ª–æ–∂–Ω–æ—Å—Ç—å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –¥–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö —Ç—Ä–µ–π–¥–µ—Ä–æ–≤

**–î–µ—Ç–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞ SCHR Levels:**

–ò–Ω–¥–∏–∫–∞—Ç–æ—Ä SCHR Levels –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å–ª–æ–∂–Ω—É—é —Å–∏—Å—Ç–µ–º—É –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ ML-–∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —É—Ä–æ–≤–Ω–µ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∏ —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑, —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã –∏ –∞–Ω—Å–∞–º–±–ª—å ML-–º–æ–¥–µ–ª–µ–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω—ã—Ö —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤.

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–∏—Å—Ç–µ–º—ã:**
- **–î–µ—Ç–µ–∫—Ü–∏—è —É—Ä–æ–≤–Ω–µ–π**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∑–Ω–∞—á–∏–º—ã—Ö —Ü–µ–Ω–æ–≤—ã—Ö —É—Ä–æ–≤–Ω–µ–π
- **–ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑**: –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Å—Ö–æ–∂–∏—Ö —É—Ä–æ–≤–Ω–µ–π –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –∑–æ–Ω
- **ML-–º–æ–¥–µ–ª–∏**: –ê–Ω—Å–∞–º–±–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ–≤–µ–¥–µ–Ω–∏—è
- **–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑**: –û—Ü–µ–Ω–∫–∞ —Å–∏–ª—ã –∏ –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ —É—Ä–æ–≤–Ω–µ–π
- **–í–∞–ª–∏–¥–∞—Ü–∏—è**: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å–∏–≥–Ω–∞–ª–æ–≤

```python
# src/indicators/schr_levels.py
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple, Union
import logging
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier
from sklearn.cluster import DBSCAN, KMeans
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.feature_selection import SelectKBest, f_classif
from scipy import stats
from scipy.signal import find_peaks
import warnings
warnings.filterwarnings('ignore')

class SCHRLevelsIndicator:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä SCHR Levels –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —É—Ä–æ–≤–Ω–µ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∏ —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è
    
    –≠—Ç–æ—Ç –∫–ª–∞—Å—Å —Ä–µ–∞–ª–∏–∑—É–µ—Ç:
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —É—Ä–æ–≤–Ω–µ–π
    - –ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ —É—Ä–æ–≤–Ω–µ–π
    - ML-–º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø—Ä–æ–±–æ–µ–≤/–æ—Ç—Å–∫–æ–∫–æ–≤
    - –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å–∏–ª—ã —É—Ä–æ–≤–Ω–µ–π
    - –í–∞–ª–∏–¥–∞—Ü–∏—é –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é —Å–∏–≥–Ω–∞–ª–æ–≤
    """
    
    def __init__(self, config: Optional[Dict] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞ SCHR Levels
        
        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –º–æ–¥–µ–ª–∏
        """
        self.config = config or self._get_default_config()
        self.logger = logging.getLogger(__name__)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
        self.models = {
            'gradient_boosting': GradientBoostingClassifier(
                n_estimators=self.config['gb_estimators'],
                learning_rate=self.config['gb_learning_rate'],
                max_depth=self.config['gb_max_depth'],
                random_state=42
            ),
            'random_forest': RandomForestClassifier(
                n_estimators=self.config['rf_estimators'],
                max_depth=self.config['rf_max_depth'],
                random_state=42,
                n_jobs=-1
            ),
            'extra_trees': ExtraTreesClassifier(
                n_estimators=self.config['et_estimators'],
                max_depth=self.config['et_max_depth'],
                random_state=42,
                n_jobs=-1
            )
        }
        
        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã
        self.scaler = RobustScaler()
        self.feature_selector = SelectKBest(f_classif, k=self.config['n_features'])
        self.ensemble_weights = None
        self.feature_names = []
        self.is_trained = False
        self.training_stats = {}
        
        # –ö—ç—à –¥–ª—è —É—Ä–æ–≤–Ω–µ–π
        self.levels_cache = {}
        self.clusters_cache = {}
        
    def _get_default_config(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        return {
            'gb_estimators': 200,
            'gb_learning_rate': 0.1,
            'gb_max_depth': 8,
            'rf_estimators': 150,
            'rf_max_depth': 12,
            'et_estimators': 150,
            'et_max_depth': 12,
            'n_features': 40,
            'min_level_strength': 0.3,
            'max_level_distance': 0.02,
            'cluster_eps': 0.01,
            'min_cluster_size': 5,
            'level_detection_window': 20,
            'volume_threshold': 1.5,
            'touch_tolerance': 0.005,
            'target_horizon': 1,
            'min_accuracy': 0.6
        }
    
    def train(self, data: Dict[str, pd.DataFrame], validation_split: float = 0.2) -> Dict:
        """
        –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ SCHR Levels —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
        
        Args:
            data: –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            validation_split: –î–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            
        Returns:
            Dict: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è
        """
        try:
            self.logger.info("Starting SCHR Levels model training...")
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            X, y = self._prepare_training_data(data)
            
            if X.empty or y.empty:
                self.logger.warning("No data available for training SCHR Levels")
                return {}
            
            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/validation/test
            X_temp, X_test, y_temp, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )
            X_train, X_val, y_train, y_val = train_test_split(
                X_temp, y_temp, test_size=validation_split, random_state=42, stratify=y_temp
            )
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            X_train_scaled = self.scaler.fit_transform(X_train)
            X_val_scaled = self.scaler.transform(X_val)
            X_test_scaled = self.scaler.transform(X_test)
            
            # –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            X_train_selected = self.feature_selector.fit_transform(X_train_scaled, y_train)
            X_val_selected = self.feature_selector.transform(X_val_scaled)
            X_test_selected = self.feature_selector.transform(X_test_scaled)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–º–µ–Ω –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            self.feature_names = [f"feature_{i}" for i in range(X_train_selected.shape[1])]
            
            # –û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π
            model_scores = {}
            for name, model in self.models.items():
                self.logger.info(f"Training {name}...")
                
                # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
                model.fit(X_train_selected, y_train)
                
                # –í–∞–ª–∏–¥–∞—Ü–∏—è
                val_pred = model.predict(X_val_selected)
                val_accuracy = accuracy_score(y_val, val_pred)
                model_scores[name] = val_accuracy
                
                self.logger.info(f"{name} validation accuracy: {val_accuracy:.4f}")
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∞–Ω—Å–∞–º–±–ª—è
            self.ensemble_weights = self._calculate_ensemble_weights(model_scores)
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            test_predictions = self._ensemble_predict(X_test_selected)
            test_accuracy = accuracy_score(y_test, test_predictions)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            self.training_stats = {
                'model_scores': model_scores,
                'ensemble_weights': self.ensemble_weights,
                'test_accuracy': test_accuracy,
                'n_features': X_train_selected.shape[1],
                'n_samples': len(X_train),
                'class_distribution': pd.Series(y).value_counts().to_dict()
            }
            
            self.is_trained = True
            self.logger.info(f"SCHR Levels training completed. Test accuracy: {test_accuracy:.4f}")
            
            return self.training_stats
            
        except Exception as e:
            self.logger.error(f"Error training SCHR Levels model: {e}")
            return {}
    
    def _prepare_training_data(self, data: Dict[str, pd.DataFrame]) -> Tuple[pd.DataFrame, pd.Series]:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
        
        Args:
            data: –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏
            
        Returns:
            Tuple: –ü—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        """
        features_list = []
        targets_list = []
        
        for symbol_timeframe, df in data.items():
            if df.empty or len(df) < 50:
                continue
            
            try:
                # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ SCHR Levels
                features = self._create_schr_levels_features(df)
                
                # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
                target = self._create_target(df)
                
                # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∏ –æ—á–∏—Å—Ç–∫–∞
                combined = pd.concat([features, target], axis=1)
                combined = combined.dropna()
                
                if len(combined) > 20:
                    features_list.append(combined.iloc[:, :-1])
                    targets_list.append(combined.iloc[:, -1])
                    
            except Exception as e:
                self.logger.warning(f"Error processing {symbol_timeframe}: {e}")
                continue
        
        if features_list:
            X = pd.concat(features_list, ignore_index=True)
            y = pd.concat(targets_list, ignore_index=True)
            
            # –£–¥–∞–ª–µ–Ω–∏–µ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            X = self._remove_correlated_features(X)
            
            return X, y
        else:
            return pd.DataFrame(), pd.Series()
    
    def _create_schr_levels_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ SCHR Levels
        
        –í–∫–ª—é—á–∞–µ—Ç:
        - –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —É—Ä–æ–≤–Ω–µ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∏ —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è
        - –ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —É—Ä–æ–≤–Ω–µ–π
        - –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        - –û–±—ä–µ–º–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        - –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        """
        features = pd.DataFrame(index=df.index)
        
        # –ë–∞–∑–æ–≤—ã–µ —Ü–µ–Ω—ã
        features['close'] = df['Close']
        features['high'] = df['High']
        features['low'] = df['Low']
        features['open'] = df['Open']
        features['volume'] = df['Volume']
        
        # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —É—Ä–æ–≤–Ω–µ–π
        levels = self._detect_levels(df)
        features.update(self._calculate_level_features(df, levels))
        
        # –ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —É—Ä–æ–≤–Ω–µ–π
        features.update(self._calculate_cluster_features(df, levels))
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —É—Ä–æ–≤–Ω–µ–π
        features.update(self._calculate_level_statistics(df, levels))
        
        # –û–±—ä–µ–º–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        features.update(self._calculate_volume_indicators(df))
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        features.update(self._calculate_temporal_patterns(df))
        
        # –î–∞–≤–ª–µ–Ω–∏–µ –Ω–∞ —É—Ä–æ–≤–Ω–∏
        features.update(self._calculate_pressure_features(df, levels))
        
        # –õ–∞–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features.update(self._calculate_lag_features(df))
        
        return features
    
    def _detect_levels(self, df: pd.DataFrame) -> Dict[str, List[float]]:
        """
        –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —É—Ä–æ–≤–Ω–µ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∏ —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç:
        - –ü–∏–∫–∏ –∏ –≤–ø–∞–¥–∏–Ω—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —É—Ä–æ–≤–Ω–µ–π
        - –ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ —Å—Ö–æ–∂–∏—Ö —É—Ä–æ–≤–Ω–µ–π
        - –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∑–Ω–∞—á–∏–º—ã—Ö —É—Ä–æ–≤–Ω–µ–π
        """
        levels = {'support': [], 'resistance': []}
        
        # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø–∏–∫–æ–≤ –∏ –≤–ø–∞–¥–∏–Ω
        highs = df['High'].values
        lows = df['Low'].values
        
        # –ù–∞—Ö–æ–∂–¥–µ–Ω–∏–µ –ø–∏–∫–æ–≤ (—Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏–µ)
        peaks, _ = find_peaks(highs, distance=self.config['level_detection_window'])
        resistance_levels = highs[peaks]
        
        # –ù–∞—Ö–æ–∂–¥–µ–Ω–∏–µ –≤–ø–∞–¥–∏–Ω (–ø–æ–¥–¥–µ—Ä–∂–∫–∞)
        valleys, _ = find_peaks(-lows, distance=self.config['level_detection_window'])
        support_levels = lows[valleys]
        
        # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —É—Ä–æ–≤–Ω–µ–π —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è
        if len(resistance_levels) > 1:
            resistance_clusters = self._cluster_levels(resistance_levels)
            levels['resistance'] = resistance_clusters
        
        # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —É—Ä–æ–≤–Ω–µ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏
        if len(support_levels) > 1:
            support_clusters = self._cluster_levels(support_levels)
            levels['support'] = support_clusters
        
        return levels
    
    def _cluster_levels(self, levels: np.ndarray) -> List[float]:
        """
        –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —É—Ä–æ–≤–Ω–µ–π –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ —Å—Ö–æ–∂–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        
        Args:
            levels: –ú–∞—Å—Å–∏–≤ —Ü–µ–Ω–æ–≤—ã—Ö —É—Ä–æ–≤–Ω–µ–π
            
        Returns:
            List: –¶–µ–Ω—Ç—Ä–æ–∏–¥—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        """
        if len(levels) < 2:
            return levels.tolist()
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        levels_normalized = levels.reshape(-1, 1)
        
        # DBSCAN –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
        clustering = DBSCAN(
            eps=self.config['cluster_eps'],
            min_samples=self.config['min_cluster_size']
        ).fit(levels_normalized)
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ü–µ–Ω—Ç—Ä–æ–∏–¥–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        cluster_centers = []
        for cluster_id in set(clustering.labels_):
            if cluster_id == -1:  # –®—É–º
                continue
            cluster_points = levels[clustering.labels_ == cluster_id]
            cluster_centers.append(np.mean(cluster_points))
        
        return cluster_centers
    
    def _calculate_level_features(self, df: pd.DataFrame, levels: Dict[str, List[float]]) -> Dict[str, pd.Series]:
        """–†–∞—Å—á–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö —É—Ä–æ–≤–Ω–µ–π"""
        features = {}
        
        # –ë–ª–∏–∂–∞–π—à–∏–µ —É—Ä–æ–≤–Ω–∏
        features['nearest_resistance'] = self._find_nearest_level(df['Close'], levels['resistance'])
        features['nearest_support'] = self._find_nearest_level(df['Close'], levels['support'])
        
        # –†–∞—Å—Å—Ç–æ—è–Ω–∏—è –¥–æ —É—Ä–æ–≤–Ω–µ–π
        features['distance_to_resistance'] = (features['nearest_resistance'] - df['Close']) / df['Close']
        features['distance_to_support'] = (df['Close'] - features['nearest_support']) / df['Close']
        
        # –ü–æ–∑–∏—Ü–∏—è –º–µ–∂–¥—É —É—Ä–æ–≤–Ω—è–º–∏
        level_range = features['nearest_resistance'] - features['nearest_support']
        features['position_in_range'] = (df['Close'] - features['nearest_support']) / (level_range + 1e-8)
        
        # –°–∏–ª–∞ –±–ª–∏–∂–∞–π—à–∏—Ö —É—Ä–æ–≤–Ω–µ–π
        features['resistance_strength'] = self._calculate_level_strength(df, levels['resistance'])
        features['support_strength'] = self._calculate_level_strength(df, levels['support'])
        
        return features
    
    def _find_nearest_level(self, prices: pd.Series, levels: List[float]) -> pd.Series:
        """–ü–æ–∏—Å–∫ –±–ª–∏–∂–∞–π—à–µ–≥–æ —É—Ä–æ–≤–Ω—è –¥–ª—è –∫–∞–∂–¥–æ–π —Ü–µ–Ω—ã"""
        if not levels:
            return pd.Series(index=prices.index, data=prices.values)
        
        nearest_levels = []
        for price in prices:
            distances = [abs(price - level) for level in levels]
            nearest_idx = np.argmin(distances)
            nearest_levels.append(levels[nearest_idx])
        
        return pd.Series(nearest_levels, index=prices.index)
    
    def _calculate_level_strength(self, df: pd.DataFrame, levels: List[float]) -> pd.Series:
        """–†–∞—Å—á–µ—Ç —Å–∏–ª—ã —É—Ä–æ–≤–Ω–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–∞—Å–∞–Ω–∏–π"""
        strength = pd.Series(index=df.index, data=0.0)
        
        for level in levels:
            # –ü–æ–∏—Å–∫ –∫–∞—Å–∞–Ω–∏–π —É—Ä–æ–≤–Ω—è
            touches = self._count_level_touches(df, level)
            strength += touches
        
        return strength
    
    def _count_level_touches(self, df: pd.DataFrame, level: float) -> pd.Series:
        """–ü–æ–¥—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–∞—Å–∞–Ω–∏–π —É—Ä–æ–≤–Ω—è"""
        tolerance = self.config['touch_tolerance']
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—Å–∞–Ω–∏–π High –∏ Low
        high_touches = (df['High'] >= level * (1 - tolerance)) & (df['High'] <= level * (1 + tolerance))
        low_touches = (df['Low'] >= level * (1 - tolerance)) & (df['Low'] <= level * (1 + tolerance))
        
        touches = (high_touches | low_touches).astype(int)
        return touches.rolling(20).sum()
    
    def _calculate_cluster_features(self, df: pd.DataFrame, levels: Dict[str, List[float]]) -> Dict[str, pd.Series]:
        """–†–∞—Å—á–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —É—Ä–æ–≤–Ω–µ–π"""
        features = {}
        
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        features['n_resistance_clusters'] = len(levels['resistance'])
        features['n_support_clusters'] = len(levels['support'])
        
        # –ü–ª–æ—Ç–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        if levels['resistance']:
            resistance_std = np.std(levels['resistance'])
            features['resistance_cluster_density'] = 1.0 / (resistance_std + 1e-8)
        else:
            features['resistance_cluster_density'] = 0.0
            
        if levels['support']:
            support_std = np.std(levels['support'])
            features['support_cluster_density'] = 1.0 / (support_std + 1e-8)
        else:
            features['support_cluster_density'] = 0.0
        
        return features
    
    def _calculate_level_statistics(self, df: pd.DataFrame, levels: Dict[str, List[float]]) -> Dict[str, pd.Series]:
        """–†–∞—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —É—Ä–æ–≤–Ω–µ–π"""
        features = {}
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —É—Ä–æ–≤–Ω–µ–π —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è
        if levels['resistance']:
            resistance_levels = np.array(levels['resistance'])
            features['resistance_mean'] = np.mean(resistance_levels)
            features['resistance_std'] = np.std(resistance_levels)
            features['resistance_skewness'] = stats.skew(resistance_levels)
            features['resistance_kurtosis'] = stats.kurtosis(resistance_levels)
        else:
            features['resistance_mean'] = df['Close'].mean()
            features['resistance_std'] = 0.0
            features['resistance_skewness'] = 0.0
            features['resistance_kurtosis'] = 0.0
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —É—Ä–æ–≤–Ω–µ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏
        if levels['support']:
            support_levels = np.array(levels['support'])
            features['support_mean'] = np.mean(support_levels)
            features['support_std'] = np.std(support_levels)
            features['support_skewness'] = stats.skew(support_levels)
            features['support_kurtosis'] = stats.kurtosis(support_levels)
        else:
            features['support_mean'] = df['Close'].mean()
            features['support_std'] = 0.0
            features['support_skewness'] = 0.0
            features['support_kurtosis'] = 0.0
        
        return features
    
    def _calculate_volume_indicators(self, df: pd.DataFrame) -> Dict[str, pd.Series]:
        """–†–∞—Å—á–µ—Ç –æ–±—ä–µ–º–Ω—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
        features = {}
        
        # –û–±—ä–µ–º–Ω—ã–µ —Å—Ä–µ–¥–Ω–∏–µ
        for window in [5, 10, 20, 50]:
            features[f'volume_sma_{window}'] = df['Volume'].rolling(window).mean()
            features[f'volume_ratio_{window}'] = df['Volume'] / features[f'volume_sma_{window}']
        
        # On-Balance Volume (OBV)
        obv = pd.Series(index=df.index, dtype=float)
        obv.iloc[0] = df['Volume'].iloc[0]
        for i in range(1, len(df)):
            if df['Close'].iloc[i] > df['Close'].iloc[i-1]:
                obv.iloc[i] = obv.iloc[i-1] + df['Volume'].iloc[i]
            elif df['Close'].iloc[i] < df['Close'].iloc[i-1]:
                obv.iloc[i] = obv.iloc[i-1] - df['Volume'].iloc[i]
            else:
                obv.iloc[i] = obv.iloc[i-1]
        features['obv'] = obv
        
        # Volume Price Trend (VPT)
        vpt = (df['Close'].pct_change() * df['Volume']).cumsum()
        features['vpt'] = vpt
        
        return features
    
    def _calculate_temporal_patterns(self, df: pd.DataFrame) -> Dict[str, pd.Series]:
        """–†–∞—Å—á–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        features = {}
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        features['hour'] = df.index.hour
        features['day_of_week'] = df.index.dayofweek
        features['day_of_month'] = df.index.day
        features['month'] = df.index.month
        
        # –¶–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features['hour_sin'] = np.sin(2 * np.pi * features['hour'] / 24)
        features['hour_cos'] = np.cos(2 * np.pi * features['hour'] / 24)
        features['day_sin'] = np.sin(2 * np.pi * features['day_of_week'] / 7)
        features['day_cos'] = np.cos(2 * np.pi * features['day_of_week'] / 7)
        
        return features
    
    def _calculate_pressure_features(self, df: pd.DataFrame, levels: Dict[str, List[float]]) -> Dict[str, pd.Series]:
        """–†–∞—Å—á–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–∞–≤–ª–µ–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–∏"""
        features = {}
        
        # –î–∞–≤–ª–µ–Ω–∏–µ –Ω–∞ –±–ª–∏–∂–∞–π—à–∏–µ —É—Ä–æ–≤–Ω–∏
        nearest_resistance = self._find_nearest_level(df['Close'], levels['resistance'])
        nearest_support = self._find_nearest_level(df['Close'], levels['support'])
        
        # –î–∞–≤–ª–µ–Ω–∏–µ –Ω–∞ —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏–µ
        resistance_pressure = (df['Close'] - nearest_resistance) * df['Volume']
        features['resistance_pressure'] = resistance_pressure.rolling(20).mean()
        
        # –î–∞–≤–ª–µ–Ω–∏–µ –Ω–∞ –ø–æ–¥–¥–µ—Ä–∂–∫—É
        support_pressure = (nearest_support - df['Close']) * df['Volume']
        features['support_pressure'] = support_pressure.rolling(20).mean()
        
        # –í–µ–∫—Ç–æ—Ä –¥–∞–≤–ª–µ–Ω–∏—è
        features['pressure_vector'] = features['resistance_pressure'] - features['support_pressure']
        
        return features
    
    def _calculate_lag_features(self, df: pd.DataFrame) -> Dict[str, pd.Series]:
        """–†–∞—Å—á–µ—Ç –ª–∞–≥–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        features = {}
        
        for lag in [1, 2, 3, 5, 10, 20]:
            features[f'close_lag_{lag}'] = df['Close'].shift(lag)
            features[f'high_lag_{lag}'] = df['High'].shift(lag)
            features[f'low_lag_{lag}'] = df['Low'].shift(lag)
            features[f'volume_lag_{lag}'] = df['Volume'].shift(lag)
            
            # –û—Ç–Ω–æ—à–µ–Ω–∏—è —Å –ª–∞–≥–∞–º–∏
            features[f'close_ratio_lag_{lag}'] = df['Close'] / features[f'close_lag_{lag}']
            features[f'volume_ratio_lag_{lag}'] = df['Volume'] / features[f'volume_lag_{lag}']
        
        return features
    
    def _create_target(self, df: pd.DataFrame, horizon: int = None) -> pd.Series:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª—è SCHR Levels"""
        if horizon is None:
            horizon = self.config['target_horizon']
        
        future_price = df['Close'].shift(-horizon)
        current_price = df['Close']
        
        # –ü—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ
        price_change = (future_price - current_price) / current_price
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
        volatility = df['Close'].rolling(20).std() / df['Close'].rolling(20).mean()
        threshold = volatility * 0.5
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –ø–æ—Ä–æ–≥–∞–º–∏
        target = pd.Series(index=df.index, dtype=int)
        target[price_change > threshold] = 2  # Up
        target[price_change < -threshold] = 0  # Down
        target[(price_change >= -threshold) & (price_change <= threshold)] = 1  # Hold
        
        return target
    
    def _remove_correlated_features(self, X: pd.DataFrame, threshold: float = 0.95) -> pd.DataFrame:
        """–£–¥–∞–ª–µ–Ω–∏–µ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        corr_matrix = X.corr().abs()
        upper_tri = corr_matrix.where(
            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
        )
        to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > threshold)]
        return X.drop(columns=to_drop)
    
    def _calculate_ensemble_weights(self, model_scores: Dict[str, float]) -> Dict[str, float]:
        """–†–∞—Å—á–µ—Ç –≤–µ—Å–æ–≤ –¥–ª—è –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π"""
        total_score = sum(model_scores.values())
        if total_score == 0:
            return {name: 1.0 / len(model_scores) for name in model_scores.keys()}
        return {name: score / total_score for name, score in model_scores.items()}
    
    def _ensemble_predict(self, X: np.ndarray) -> np.ndarray:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π"""
        predictions = []
        for name, model in self.models.items():
            pred = model.predict(X)
            weight = self.ensemble_weights.get(name, 0)
            predictions.append(pred * weight)
        ensemble_pred = np.sum(predictions, axis=0)
        return np.round(ensemble_pred).astype(int)
    
    def predict(self, data: pd.DataFrame) -> np.ndarray:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ SCHR Levels"""
        if not self.is_trained:
            self.logger.warning("SCHR Levels model not trained")
            return np.zeros(len(data))
        
        try:
            # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            features = self._create_schr_levels_features(data)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            features_scaled = self.scaler.transform(features)
            
            # –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            features_selected = self.feature_selector.transform(features_scaled)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            prediction = self._ensemble_predict(features_selected)
            
            return prediction
            
        except Exception as e:
            self.logger.error(f"Error predicting with SCHR Levels: {e}")
            return np.zeros(len(data))
    
    def get_levels(self, data: pd.DataFrame) -> Dict[str, List[float]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö —É—Ä–æ–≤–Ω–µ–π"""
        return self._detect_levels(data)
    
    def get_training_stats(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è"""
        return self.training_stats.copy()

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    np.random.seed(42)
    dates = pd.date_range('2020-01-01', periods=1000, freq='H')
    
    # –°–∏–º—É–ª—è—Ü–∏—è —Ü–µ–Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å —É—Ä–æ–≤–Ω—è–º–∏
    price = 100
    prices = []
    for i in range(1000):
        # –°–æ–∑–¥–∞–Ω–∏–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã—Ö —É—Ä–æ–≤–Ω–µ–π
        if i % 100 < 20:  # –£—Ä–æ–≤–µ–Ω—å —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è
            change = np.random.normal(0, 0.005)
        elif i % 100 > 80:  # –£—Ä–æ–≤–µ–Ω—å –ø–æ–¥–¥–µ—Ä–∂–∫–∏
            change = np.random.normal(0, 0.005)
        else:
            change = np.random.normal(0, 0.01)
        
        price *= (1 + change)
        prices.append(price)
    
    data = pd.DataFrame({
        'Open': prices,
        'High': [p * (1 + abs(np.random.normal(0, 0.005))) for p in prices],
        'Low': [p * (1 - abs(np.random.normal(0, 0.005))) for p in prices],
        'Close': prices,
        'Volume': np.random.randint(1000, 10000, 1000)
    }, index=dates)
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è OHLC
    for i in range(len(data)):
        high = max(data.iloc[i]['Open'], data.iloc[i]['Close'])
        low = min(data.iloc[i]['Open'], data.iloc[i]['Close'])
        data.iloc[i, data.columns.get_loc('High')] = high
        data.iloc[i, data.columns.get_loc('Low')] = low
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞
    schr_levels = SCHRLevelsIndicator()
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    training_data = {'test_H1': data}
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    stats = schr_levels.train(training_data)
    print(f"Training completed. Test accuracy: {stats.get('test_accuracy', 0):.4f}")
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    predictions = schr_levels.predict(data.tail(100))
    print(f"Predictions: {np.bincount(predictions)}")
    
    # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —É—Ä–æ–≤–Ω–µ–π
    levels = schr_levels.get_levels(data)
    print(f"Detected resistance levels: {len(levels['resistance'])}")
    print(f"Detected support levels: {len(levels['support'])}")
```

## ‚ö° –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä SCHR SHORT3

**–¢–µ–æ—Ä–∏—è:** –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä SCHR SHORT3 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π ML-–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–π —Ç–æ—Ä–≥–æ–≤–ª–∏ –∏ —Å–∫–∞–ª—å–ø–∏–Ω–≥–∞, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—â–∏–π –≤—ã—Å–æ–∫–æ—á–∞—Å—Ç–æ—Ç–Ω—ã–µ —Ç–æ—Ä–≥–æ–≤—ã–µ —Å–∏–≥–Ω–∞–ª—ã —Å –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é. –≠—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–ª—è –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏–±—ã–ª–∏.

**–ü–æ—á–µ–º—É –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä SCHR SHORT3 –≤–∞–∂–µ–Ω:**
- **–ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ—Å—Ç—å:** –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã–µ —Ç–æ—Ä–≥–æ–≤—ã–µ —Å–∏–≥–Ω–∞–ª—ã
- **–°–∫–∞–ª—å–ø–∏–Ω–≥:** –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —Å–∫–∞–ª—å–ø–∏–Ω–≥–∞
- **–ß–∞—Å—Ç–æ—Ç–∞:** –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤—ã—Å–æ–∫—É—é —á–∞—Å—Ç–æ—Ç—É —Å–∏–≥–Ω–∞–ª–æ–≤
- **–ü—Ä–∏–±—ã–ª—å–Ω–æ—Å—Ç—å:** –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏–±—ã–ª–∏

**–ü–ª—é—Å—ã:**
- –ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã
- –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å–∫–∞–ª—å–ø–∏–Ω–≥–∞
- –í—ã—Å–æ–∫–∞—è —á–∞—Å—Ç–æ—Ç–∞ —Å–∏–≥–Ω–∞–ª–æ–≤
- –ú–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–∏–±—ã–ª–∏

**–ú–∏–Ω—É—Å—ã:**
- –í—ã—Å–æ–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Å–∫–æ—Ä–æ—Å—Ç–∏
- –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –≤—ã—Å–æ–∫–∏–µ –∫–æ–º–∏—Å—Å–∏–∏
- –¢—Ä–µ–±—É–µ—Ç –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞

```python
# src/indicators/schr_short3.py
import pandas as pd
import numpy as np
from typing import Dict, List, Optional
import logging
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

class SCHRShort3Indicator:
    """–ò–Ω–¥–∏–∫–∞—Ç–æ—Ä SCHR SHORT3 –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–π —Ç–æ—Ä–≥–æ–≤–ª–∏"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.model = ExtraTreesClassifier(n_estimators=100, random_state=42)
        self.features = []
        self.is_trained = False
        
    def train(self, data: Dict[str, pd.DataFrame]):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ SCHR SHORT3"""
        try:
            self.logger.info("Training SCHR SHORT3 model...")
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            X, y = self._prepare_training_data(data)
            
            if X.empty or y.empty:
                self.logger.warning("No data available for training SCHR SHORT3")
                return
            
            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42
            )
            
            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            self.model.fit(X_train, y_train)
            
            # –û—Ü–µ–Ω–∫–∞
            y_pred = self.model.predict(X_test)
            accuracy = accuracy_score(y_test, y_pred)
            
            self.is_trained = True
            self.logger.info(f"SCHR SHORT3 model trained with accuracy: {accuracy:.4f}")
            
        except Exception as e:
            self.logger.error(f"Error training SCHR SHORT3 model: {e}")
    
    def _prepare_training_data(self, data: Dict[str, pd.DataFrame]) -> tuple:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        features_list = []
        targets_list = []
        
        for symbol_timeframe, df in data.items():
            if df.empty:
                continue
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ SCHR SHORT3
            features = self._create_schr_short3_features(df)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
            target = self._create_target(df)
            
            # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
            combined = pd.concat([features, target], axis=1)
            combined = combined.dropna()
            
            if not combined.empty:
                features_list.append(combined.iloc[:, :-1])
                targets_list.append(combined.iloc[:, -1])
        
        if features_list:
            X = pd.concat(features_list, ignore_index=True)
            y = pd.concat(targets_list, ignore_index=True)
            return X, y
        else:
            return pd.DataFrame(), pd.Series()
    
    def _create_schr_short3_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ SCHR SHORT3"""
        features = pd.DataFrame(index=df.index)
        
        # –ë–∞–∑–æ–≤—ã–µ —Ü–µ–Ω—ã
        features['close'] = df['Close']
        features['high'] = df['High']
        features['low'] = df['Low']
        features['volume'] = df['Volume']
        
        # SCHR SHORT3 –ø—Ä–∏–∑–Ω–∞–∫–∏
        features['short_term_signal'] = self._calculate_short_term_signal(df)
        features['short_term_strength'] = self._calculate_short_term_strength(df)
        features['short_term_direction'] = self._calculate_short_term_direction(df)
        features['short_term_volatility'] = self._calculate_short_term_volatility(df)
        features['short_term_momentum'] = self._calculate_short_term_momentum(df)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã
        features['short_buy_signal'] = (features['short_term_signal'] > 0.5).astype(int)
        features['short_sell_signal'] = (features['short_term_signal'] < -0.5).astype(int)
        features['short_hold_signal'] = (abs(features['short_term_signal']) <= 0.5).astype(int)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        features['short_hits'] = self._calculate_short_hits(df)
        features['short_breaks'] = self._calculate_short_breaks(df)
        features['short_bounces'] = self._calculate_short_bounces(df)
        features['short_accuracy'] = self._calculate_short_accuracy(df)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features['short_volatility_normalized'] = features['short_term_volatility'] / features['close']
        features['short_momentum_normalized'] = features['short_term_momentum'] / features['close']
        
        # –õ–∞–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        for lag in [1, 2, 3, 5, 10]:
            features[f'short_signal_lag_{lag}'] = features['short_term_signal'].shift(lag)
            features[f'short_strength_lag_{lag}'] = features['short_term_strength'].shift(lag)
        
        return features
    
    def _calculate_short_term_signal(self, df: pd.DataFrame) -> pd.Series:
        """–†–∞—Å—á–µ—Ç –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞"""
        # –ö–æ–º–±–∏–Ω–∞—Ü–∏—è RSI –∏ MACD –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤
        rsi = self._calculate_rsi(df['Close'])
        macd = self._calculate_macd(df['Close'])
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        rsi_norm = (rsi - 50) / 50
        macd_norm = macd / df['Close']
        
        # –ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã–π —Å–∏–≥–Ω–∞–ª
        signal = (rsi_norm + macd_norm) / 2
        return signal.rolling(5).mean()
    
    def _calculate_short_term_strength(self, df: pd.DataFrame) -> pd.Series:
        """–†–∞—Å—á–µ—Ç —Å–∏–ª—ã –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞"""
        volatility = df['Close'].rolling(20).std()
        volume = df['Volume'].rolling(20).mean()
        
        # –°–∏–ª–∞ = –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å * –æ–±—ä–µ–º
        strength = volatility * volume
        return strength / strength.rolling(50).max()
    
    def _calculate_short_term_direction(self, df: pd.DataFrame) -> pd.Series:
        """–†–∞—Å—á–µ—Ç –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞"""
        price_change = df['Close'].pct_change(5)
        return np.sign(price_change)
    
    def _calculate_short_term_volatility(self, df: pd.DataFrame) -> pd.Series:
        """–†–∞—Å—á–µ—Ç –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–π –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏"""
        return df['Close'].rolling(10).std()
    
    def _calculate_short_term_momentum(self, df: pd.DataFrame) -> pd.Series:
        """–†–∞—Å—á–µ—Ç –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–≥–æ –º–æ–º–µ–Ω—Ç—É–º–∞"""
        return df['Close'].pct_change(3)
    
    def _calculate_short_hits(self, df: pd.DataFrame) -> pd.Series:
        """–†–∞—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã—Ö –∫–∞—Å–∞–Ω–∏–π"""
        high_20 = df['High'].rolling(20).max()
        low_20 = df['Low'].rolling(20).min()
        
        hits = ((df['Close'] >= high_20 * 0.99) | (df['Close'] <= low_20 * 1.01)).astype(int)
        return hits.rolling(20).sum()
    
    def _calculate_short_breaks(self, df: pd.DataFrame) -> pd.Series:
        """–†–∞—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã—Ö –ø—Ä–æ–±–æ–µ–≤"""
        high_20 = df['High'].rolling(20).max()
        low_20 = df['Low'].rolling(20).min()
        
        breaks = ((df['Close'] > high_20) | (df['Close'] < low_20)).astype(int)
        return breaks.rolling(20).sum()
    
    def _calculate_short_bounces(self, df: pd.DataFrame) -> pd.Series:
        """–†–∞—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã—Ö –æ—Ç—Å–∫–æ–∫–æ–≤"""
        price_change = df['Close'].pct_change()
        bounces = ((price_change > 0.01) | (price_change < -0.01)).astype(int)
        return bounces.rolling(20).sum()
    
    def _calculate_short_accuracy(self, df: pd.DataFrame) -> pd.Series:
        """–†–∞—Å—á–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏
        price_change = df['Close'].pct_change()
        correct_predictions = (abs(price_change) > 0.005).astype(int)
        return correct_predictions.rolling(20).mean()
    
    def _calculate_rsi(self, prices: pd.Series, window: int = 14) -> pd.Series:
        """–†–∞—Å—á–µ—Ç RSI"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi
    
    def _calculate_macd(self, prices: pd.Series, fast: int = 12, slow: int = 26, signal: int = 9) -> pd.Series:
        """–†–∞—Å—á–µ—Ç MACD"""
        ema_fast = prices.ewm(span=fast).mean()
        ema_slow = prices.ewm(span=slow).mean()
        macd = ema_fast - ema_slow
        signal_line = macd.ewm(span=signal).mean()
        return macd - signal_line
    
    def _create_target(self, df: pd.DataFrame, horizon: int = 1) -> pd.Series:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π"""
        future_price = df['Close'].shift(-horizon)
        current_price = df['Close']
        
        # –ü—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ
        price_change = (future_price - current_price) / current_price
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        target = pd.cut(
            price_change,
            bins=[-np.inf, -0.001, 0.001, np.inf],
            labels=[0, 1, 2],  # 0=down, 1=hold, 2=up
            include_lowest=True
        )
        
        return target.astype(int)
    
    def predict(self, data: pd.DataFrame) -> np.ndarray:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ SCHR SHORT3"""
        if not self.is_trained:
            self.logger.warning("SCHR SHORT3 model not trained")
            return np.zeros(len(data))
        
        try:
            # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            features = self._create_schr_short3_features(data)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            prediction = self.model.predict(features)
            
            return prediction
            
        except Exception as e:
            self.logger.error(f"Error predicting with SCHR SHORT3: {e}")
            return np.zeros(len(data))
    
    def get_features(self) -> pd.DataFrame:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        return self.features
```

**–¢–µ–æ—Ä–∏—è:** –ó–∞–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–∞—è —á–∞—Å—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ–ø–∏—Å–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Å–∏—Å—Ç–µ–º—ã. –≠—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø–æ–ª–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏ –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è —Å–∏—Å—Ç–µ–º—ã.

**–ü–æ—á–µ–º—É –∑–∞–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–∞—è —á–∞—Å—Ç—å –≤–∞–∂–Ω–∞:**
- **–°—Ç—Ä—É–∫—Ç—É—Ä–∞:** –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
- **–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ:** –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
- **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:** –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
- **–†–∞–∑–≤–∏—Ç–∏–µ:** –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è

**–ü–ª—é—Å—ã:**
- –ü–æ–Ω–∏–º–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
- –ü–ª–∞–Ω –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è
- –ü–æ–ª–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
- –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ä–∞–∑–≤–∏—Ç–∏—è

**–ú–∏–Ω—É—Å—ã:**
- –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –Ω–µ–ø–æ–ª–Ω–æ—Ç–∞
- –¢—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏

–≠—Ç–æ –≤—Ç–æ—Ä–∞—è —á–∞—Å—Ç—å –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∫–æ–¥–∞. –ü—Ä–æ–¥–æ–ª–∂—É —Å –æ—Å—Ç–∞–ª—å–Ω—ã–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏ –≤ —Å–ª–µ–¥—É—é—â–∏—Ö —á–∞—Å—Ç—è—Ö.
