# 18.2. –î–µ—Ç–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã

**–¢–µ–æ—Ä–∏—è:** –î–µ—Ç–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Å–æ–±–æ–π –ø–æ–¥—Ä–æ–±–Ω–æ–µ description –≤—Å–µ—Ö –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Å–∏—Å—Ç–µ–º—ã, –∏—Ö —Ñ—É–Ω–∫—Ü–∏–π and –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π. –≠—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ for –ø–æ–Ω–∏–º–∞–Ω–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã and —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–∏—Å—Ç–µ–º—ã.

**–ü–æ—á–µ–º—É –¥–µ—Ç–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã –≤–∞–∂–Ω—ã:**
- **–ü–æ–Ω–∏–º–∞–Ω–∏–µ:** –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≥–ª—É–±–æ–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã
- **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:** –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
- **–†–µ–∞–ª–∏–∑–∞—Ü–∏—è:** –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏
- **integration:** –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ for –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤

**–ü–ª—é—Å—ã:**
- –ì–ª—É–±–æ–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ
- –ß–µ—Ç–∫–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
- –î–µ—Ç–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
- –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è integration

**–ú–∏–Ω—É—Å—ã:**
- –í—ã—Å–æ–∫–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å
- –¢—Ä–µ–±—É–µ—Ç –≥–ª—É–±–æ–∫–∏—Ö –∑–Ω–∞–Ω–∏–π
- –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ Issues with –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π

## üìä –°–±–æ—Ä—â–∏–∫ –¥–∞–Ω–Ω—ã—Ö

**–¢–µ–æ—Ä–∏—è:** –°–±–æ—Ä—â–∏–∫ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç —Å–∏—Å—Ç–µ–º—ã, –æ—Ç–≤–µ—á–∞—é—â–∏–π –∑–∞ —Å–±–æ—Ä, –æ—á–∏—Å—Ç–∫—É and –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –¥–∞–Ω–Ω—ã—Ö for –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞. –≠—Ç–æ –æ—Å–Ω–æ–≤–∞ for –≤—Å–µ—Ö ML-–º–æ–¥–µ–ª–µ–π and —Ç–æ—Ä–≥–æ–≤—ã—Ö —Ä–µ—à–µ–Ω–∏–π.

**–ü–æ—á–µ–º—É —Å–±–æ—Ä—â–∏–∫ –¥–∞–Ω–Ω—ã—Ö –≤–∞–∂–µ–Ω:**
- **–ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö:** –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
- **–ü–æ–ª–Ω–æ—Ç–∞:** –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–æ–ª–Ω–æ—Ç—É –¥–∞–Ω–Ω—ã—Ö
- **–ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å:** –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
- **–ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å:** –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ for –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã

**–ü–ª—é—Å—ã:**
- –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
- –ü–æ–ª–Ω–æ—Ç–∞ –¥–∞–Ω–Ω—ã—Ö
- –ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
- –ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã

**–ú–∏–Ω—É—Å—ã:**
- –°–ª–æ–∂–Ω–æ—Å—Ç—å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏
- –í—ã—Å–æ–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Ä–µ—Å—É—Ä—Å–∞–º
- –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ Issues with –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö

**–î–µ—Ç–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å–±–æ—Ä—â–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö:**

–°–±–æ—Ä—â–∏–∫ –¥–∞–Ω–Ω—ã—Ö —è–≤–ª—è–µ—Ç—Å—è —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–º —Å–∏—Å—Ç–µ–º—ã, –∫–æ—Ç–æ—Ä—ã–π –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–æ–ª—É—á–µ–Ω–∏–µ, –æ—á–∏—Å—Ç–∫—É and –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É —Ä—ã–Ω–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö for –≤—Å–µ—Ö –ø–æ—Å–ª–µ–¥—É—é—â–∏—Ö –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤. –≠—Ç–æ—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–µ–Ω for —Ä–∞–±–æ—Ç—ã –≤—Å–µ–π —Å–∏—Å—Ç–µ–º—ã, –ø–æ—Å–∫–æ–ª—å–∫—É –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –Ω–∞–ø—Ä—è–º—É—é –≤–ª–∏—è–µ—Ç on —Ç–æ—á–Ω–æ—Å—Ç—å –≤—Å–µ—Ö ML-–º–æ–¥–µ–ª–µ–π and —Ç–æ—Ä–≥–æ–≤—ã—Ö —Ä–µ—à–µ–Ω–∏–π.

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã:**
- **–ú–æ–¥—É–ª—å–Ω–æ—Å—Ç—å**: –ö–∞–∂–¥—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ
- **–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ**: –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è in –ø–∞–º—è—Ç–∏ for –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
- **clean**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ remove –∞–Ω–æ–º–∞–ª–∏–π and –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- **–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å**: –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∞–∫—Ç–∏–≤–æ–≤ and Timeframe–æ–≤
- **–ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å**: –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ and –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ —Å–±–æ–µ–≤

**–ö–ª—é—á–µ–≤—ã–µ functions:**
1. **–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö**: –ü–æ–ª—É—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö and —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö with —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
2. **clean –¥–∞–Ω–Ω—ã—Ö**: remove –∞–Ω–æ–º–∞–ª–∏–π, –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ and –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
3. **–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è**: –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∫ –µ–¥–∏–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É
4. **–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ**: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ
5. **–≠–∫—Å–ø–æ—Ä—Ç**: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö in —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö

```python
# src/data/collectors.py
import yfinance as yf
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import logging
from pathlib import Path
import asyncio
import aiohttp
from concurrent.futures import ThreadPoolExecutor
import time

class DataCollector:
 """
 –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Å–±–æ—Ä—â–∏–∫ –¥–∞–Ω–Ω—ã—Ö for –≤—Å–µ—Ö –∞–∫—Ç–∏–≤–æ–≤ and Timeframe–æ–≤

 –≠—Ç–æ—Ç –∫–ª–∞—Å—Å –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç:
 - –ú–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω—ã–π —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö
 - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –æ—á–∏—Å—Ç–∫—É and –≤–∞–ª–∏–¥–∞—Ü–∏—é
 - –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
 - –û–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫ and –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ
 - –ü–æ–¥–¥–µ—Ä–∂–∫—É —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö
 """

 def __init__(self, config: Dict):
 """
 –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–±–æ—Ä—â–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö

 Args:
 config: configuration —Å–∏—Å—Ç–µ–º—ã with –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö
 """
 self.config = config
 self.logger = logging.getLogger(__name__)
 self.data_cache = {}
 self.last_update = {}
 self.max_workers = config.get('max_workers', 5)
 self.cache_ttl = config.get('cache_ttl', 3600) # 1 —á–∞—Å
 self.retry_attempts = config.get('retry_attempts', 3)
 self.retry_delay = config.get('retry_delay', 1)

 # configuration –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
 logging.basicConfig(level=logging.INFO)

 def collect_data(self, symbol: str, timeframe: str, period: str = "2y") -> pd.DataFrame:
 """
 –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö for —Å–∏–º–≤–æ–ª–∞ and Timeframe–∞ with —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é

 Args:
 symbol: –°–∏–º–≤–æ–ª –∞–∫—Ç–∏–≤–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'AAPL', 'EURUSD')
 timeframe: Timeframe ('M1', 'M5', 'H1', 'D1', etc.)
 period: –ü–µ—Ä–∏–æ–¥ –¥–∞–Ω–Ω—ã—Ö ('1d', '5d', '1mo', '3mo', '6mo', '1y', '2y', '5y', '10y', 'ytd', 'max')

 Returns:
 pd.DataFrame: –û—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ with –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
 """
 try:
 self.logger.info(f"starting data collection for {symbol} {timeframe}")

 # check cache
 cache_key = f"{symbol}_{timeframe}_{period}"
 if self._is_cache_valid(cache_key):
 self.logger.info(f"Using cached data for {symbol} {timeframe}")
 return self.data_cache[cache_key]

 # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ Timeframe–∞ for yfinance
 interval_map = {
 'M1': '1m',
 'M5': '5m',
 'M15': '15m',
 'M30': '30m',
 'H1': '1h',
 'H2': '2h',
 'H4': '4h',
 'H6': '6h',
 'H8': '8h',
 'H12': '12h',
 'D1': '1d',
 'W1': '1wk',
 'MN1': '1mo'
 }

 interval = interval_map.get(timeframe, '1h')

 # –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö with –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏
 data = self._collect_with_retry(symbol, interval, period)

 if data.empty:
 self.logger.warning(f"No data found for {symbol} {timeframe}")
 return pd.DataFrame()

 # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è clean –¥–∞–Ω–Ω—ã—Ö
 data = self._clean_data(data)

 # add –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö and —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö indicators
 data = self._add_metadata(data, symbol, timeframe)
 data = self._add_technical_indicators(data)

 # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
 if not self._validate_data_quality(data):
 self.logger.error(f"Data quality validation failed for {symbol} {timeframe}")
 return pd.DataFrame()

 # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ with –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–æ–π
 self.data_cache[cache_key] = data
 self.last_update[cache_key] = time.time()

 self.logger.info(f"Successfully collected {len(data)} records for {symbol} {timeframe}")
 return data

 except Exception as e:
 self.logger.error(f"Error collecting data for {symbol} {timeframe}: {e}")
 return pd.DataFrame()

 def _collect_with_retry(self, symbol: str, interval: str, period: str) -> pd.DataFrame:
 """–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö with –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
 for attempt in range(self.retry_attempts):
 try:
 ticker = yf.Ticker(symbol)
 data = ticker.history(period=period, interval=interval)

 if not data.empty:
 return data

 except Exception as e:
 self.logger.warning(f"Attempt {attempt + 1} failed for {symbol}: {e}")
 if attempt < self.retry_attempts - 1:
 time.sleep(self.retry_delay * (2 ** attempt)) # Exponential backoff

 return pd.DataFrame()

 def _clean_data(self, data: pd.DataFrame) -> pd.DataFrame:
 """
 –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è clean –¥–∞–Ω–Ω—ã—Ö with –¥–µ—Ç–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º

 –í–∫–ª—é—á–∞–µ—Ç:
 - remove NaN and –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
 - –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ and remove –∞–Ω–æ–º–∞–ª–∏–π
 - –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
 - –í–∞–ª–∏–¥–∞—Ü–∏—é –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
 """
 original_length = len(data)

 # remove NaN
 data = data.dropna()

 # remove –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ on –∏–Ω–¥–µ–∫—Å—É
 data = data[~data.index.duplicated(keep='first')]

 # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ in time
 data = data.sort_index()

 # remove –∞–Ω–æ–º–∞–ª–∏–π
 data = self._remove_anomalies(data)

 # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
 data = self._fill_missing_data(data)

 # –í–∞–ª–∏–¥–∞—Ü–∏—è OHLC –¥–∞–Ω–Ω—ã—Ö
 data = self._validate_ohlc_data(data)

 cleaned_length = len(data)
 if original_length != cleaned_length:
 self.logger.info(f"Data cleaned: {original_length} -> {cleaned_length} records")

 return data

 def _remove_anomalies(self, data: pd.DataFrame) -> pd.DataFrame:
 """
 –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ remove –∞–Ω–æ–º–∞–ª–∏–π

 –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã for –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è:
 - –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö —Ü–µ–Ω–æ–≤—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π
 - –ù–µ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –æ–±—ä–µ–º–æ–≤ —Ç–æ—Ä–≥–æ–≤
 - –í—Ä–µ–º–µ–Ω–Ω—ã—Ö –∞–Ω–æ–º–∞–ª–∏–π
 """
 if data.empty:
 return data

 # remove –Ω—É–ª–µ–≤—ã—Ö or –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö —Ü–µ–Ω
 price_columns = ['Open', 'High', 'Low', 'Close']
 for col in price_columns:
 if col in data.columns:
 data = data[data[col] > 0]

 # remove —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (–±–æ–ª–µ–µ 5 —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π)
 for col in price_columns:
 if col in data.columns and len(data) > 10:
 mean_val = data[col].mean()
 std_val = data[col].std()
 if std_val > 0:
 z_scores = np.abs((data[col] - mean_val) / std_val)
 data = data[z_scores < 5]

 # remove –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö –æ–±—ä–µ–º–æ–≤
 if 'Volume' in data.columns and len(data) > 10:
 volume_mean = data['Volume'].mean()
 volume_std = data['Volume'].std()
 if volume_std > 0:
 volume_z_scores = np.abs((data['Volume'] - volume_mean) / volume_std)
 data = data[volume_z_scores < 4]

 # check –ª–æ–≥–∏–∫–∏ OHLC
 data = data[data['High'] >= data['Low']]
 data = data[data['High'] >= data['Open']]
 data = data[data['High'] >= data['Close']]
 data = data[data['Low'] <= data['Open']]
 data = data[data['Low'] <= data['Close']]

 return data

 def _fill_missing_data(self, data: pd.DataFrame) -> pd.DataFrame:
 """–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–º –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏"""
 if data.empty:
 return data

 # –ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è for —Ü–µ–Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
 price_columns = ['Open', 'High', 'Low', 'Close']
 for col in price_columns:
 if col in data.columns:
 data[col] = data[col].interpolate(method='linear')

 # for –æ–±—ä–µ–º–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º forward fill
 if 'Volume' in data.columns:
 data['Volume'] = data['Volume'].fillna(method='ffill')

 return data

 def _validate_ohlc_data(self, data: pd.DataFrame) -> pd.DataFrame:
 """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ OHLC –¥–∞–Ω–Ω—ã—Ö"""
 if data.empty:
 return data

 # check, —á—Ç–æ High >= Low
 valid_ohlc = data['High'] >= data['Low']
 data = data[valid_ohlc]

 # check, —á—Ç–æ High >= Open and High >= Close
 valid_high = (data['High'] >= data['Open']) & (data['High'] >= data['Close'])
 data = data[valid_high]

 # check, —á—Ç–æ Low <= Open and Low <= Close
 valid_low = (data['Low'] <= data['Open']) & (data['Low'] <= data['Close'])
 data = data[valid_low]

 return data

 def _add_metadata(self, data: pd.DataFrame, symbol: str, timeframe: str) -> pd.DataFrame:
 """add –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∫ –¥–∞–Ω–Ω—ã–º"""
 data = data.copy()
 data['symbol'] = symbol
 data['timeframe'] = timeframe
 data['timestamp'] = data.index
 data['date'] = data.index.date
 data['time'] = data.index.time
 data['day_of_week'] = data.index.dayofweek
 data['hour'] = data.index.hour
 data['minute'] = data.index.minute

 return data

 def _add_technical_indicators(self, data: pd.DataFrame) -> pd.DataFrame:
 """add –±–∞–∑–æ–≤—ã—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö indicators"""
 if data.empty or len(data) < 20:
 return data

 data = data.copy()

 # –ü—Ä–æ—Å—Ç—ã–µ —Å–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ
 for window in [5, 10, 20, 50]:
 data[f'sma_{window}'] = data['Close'].rolling(window=window).mean()

 # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ
 for span in [12, 26]:
 data[f'ema_{span}'] = data['Close'].ewm(span=span).mean()

 # RSI
 data['rsi'] = self._calculate_rsi(data['Close'])

 # Bollinger Bands
 bb_period = 20
 bb_std = 2
 data['bb_middle'] = data['Close'].rolling(bb_period).mean()
 bb_std_val = data['Close'].rolling(bb_period).std()
 data['bb_upper'] = data['bb_middle'] + (bb_std_val * bb_std)
 data['bb_lower'] = data['bb_middle'] - (bb_std_val * bb_std)

 # MACD
 data['macd'] = self._calculate_macd(data['Close'])

 # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
 data['volatility'] = data['Close'].rolling(20).std()

 return data

 def _calculate_rsi(self, prices: pd.Series, window: int = 14) -> pd.Series:
 """–†–∞—Å—á–µ—Ç RSI (Relative Strength Index)"""
 delta = prices.diff()
 gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
 loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
 rs = gain / loss
 rsi = 100 - (100 / (1 + rs))
 return rsi

 def _calculate_macd(self, prices: pd.Series, fast: int = 12, slow: int = 26, signal: int = 9) -> pd.Series:
 """–†–∞—Å—á–µ—Ç MACD (Moving Average Convergence Divergence)"""
 ema_fast = prices.ewm(span=fast).mean()
 ema_slow = prices.ewm(span=slow).mean()
 macd = ema_fast - ema_slow
 signal_line = macd.ewm(span=signal).mean()
 return macd - signal_line

 def _validate_data_quality(self, data: pd.DataFrame) -> bool:
 """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
 if data.empty:
 return False

 # check –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞–ø–∏—Å–µ–π
 if len(data) < 10:
 return False

 # check on –Ω–∞–ª–∏—á–∏–µ NaN
 if data.isnull().any().any():
 return False

 # check –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ —Ü–µ–Ω
 price_columns = ['Open', 'High', 'Low', 'Close']
 for col in price_columns:
 if col in data.columns:
 if (data[col] <= 0).any():
 return False

 return True

 def _is_cache_valid(self, cache_key: str) -> bool:
 """check –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ cache"""
 if cache_key not in self.data_cache:
 return False

 if cache_key not in self.last_update:
 return False

 return (time.time() - self.last_update[cache_key]) < self.cache_ttl

 def collect_multiple_assets(self, symbols: List[str], timeframes: List[str], period: str = "2y") -> Dict[str, pd.DataFrame]:
 """
 –ú–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω—ã–π —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö for –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∞–∫—Ç–∏–≤–æ–≤

 Args:
 symbols: –°–ø–∏—Å–æ–∫ —Å–∏–º–≤–æ–ª–æ–≤ –∞–∫—Ç–∏–≤–æ–≤
 timeframes: –°–ø–∏—Å–æ–∫ Timeframe–æ–≤
 period: –ü–µ—Ä–∏–æ–¥ –¥–∞–Ω–Ω—ã—Ö

 Returns:
 Dict with data for –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ and Timeframe–∞
 """
 results = {}

 with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
 futures = []

 for symbol in symbols:
 for timeframe in timeframes:
 future = executor.submit(self.collect_data, symbol, timeframe, period)
 futures.append((future, symbol, timeframe))

 for future, symbol, timeframe in futures:
 try:
 data = future.result(timeout=300) # 5 minutes —Ç–∞–π–º–∞—É—Ç
 key = f"{symbol}_{timeframe}"
 results[key] = data
 except Exception as e:
 self.logger.error(f"Failed to collect data for {symbol} {timeframe}: {e}")

 return results

 def get_current_data(self) -> Dict[str, pd.DataFrame]:
 """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–∏—Ö –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
 current_data = {}

 for asset_type, assets in self.config.get('data_sources', {}).items():
 for asset in assets:
 symbol = asset['symbol']
 for timeframe in self.config.get('timeframes', []):
 cache_key = f"{symbol}_{timeframe}"
 if cache_key in self.data_cache:
 current_data[cache_key] = self.data_cache[cache_key]

 return current_data

 def get_all_data(self) -> Dict[str, pd.DataFrame]:
 """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
 return self.data_cache.copy()

 def save_data(self, data: pd.DataFrame, symbol: str, timeframe: str, format: str = 'parquet'):
 """
 –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö in —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö

 Args:
 data: –î–∞–Ω–Ω—ã–µ for —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
 symbol: –°–∏–º–≤–æ–ª –∞–∫—Ç–∏–≤–∞
 timeframe: Timeframe
 format: –§–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞ ('parquet', 'csv', 'json')
 """
 if data.empty:
 self.logger.warning(f"No data to save for {symbol} {timeframe}")
 return

 try:
 # create –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
 data_dir = Path(f"data/raw/{symbol}")
 data_dir.mkdir(parents=True, exist_ok=True)

 # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ in –≤—ã–±—Ä–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
 if format == 'parquet':
 file_path = data_dir / f"{timeframe}.parquet"
 data.to_parquet(file_path, compression='snappy')
 elif format == 'csv':
 file_path = data_dir / f"{timeframe}.csv"
 data.to_csv(file_path, index=True)
 elif format == 'json':
 file_path = data_dir / f"{timeframe}.json"
 data.to_json(file_path, orient='index', date_format='iso')
 else:
 raise ValueError(f"Unsupported format: {format}")

 self.logger.info(f"Data saved to {file_path}")

 except Exception as e:
 self.logger.error(f"Error saving data for {symbol} {timeframe}: {e}")

 def load_data(self, symbol: str, timeframe: str, format: str = 'parquet') -> pd.DataFrame:
 """
 Loading data –∏–∑ —Ñ–∞–π–ª–∞

 Args:
 symbol: –°–∏–º–≤–æ–ª –∞–∫—Ç–∏–≤–∞
 timeframe: Timeframe
 format: –§–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞

 Returns:
 pd.DataFrame: –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
 """
 try:
 data_dir = Path(f"data/raw/{symbol}")

 if format == 'parquet':
 file_path = data_dir / f"{timeframe}.parquet"
 data = pd.read_parquet(file_path)
 elif format == 'csv':
 file_path = data_dir / f"{timeframe}.csv"
 data = pd.read_csv(file_path, index_col=0, parse_dates=True)
 elif format == 'json':
 file_path = data_dir / f"{timeframe}.json"
 data = pd.read_json(file_path, orient='index')
 data.index = pd.to_datetime(data.index)
 else:
 raise ValueError(f"Unsupported format: {format}")

 self.logger.info(f"Data loaded from {file_path}")
 return data

 except Exception as e:
 self.logger.error(f"Error loading data for {symbol} {timeframe}: {e}")
 return pd.DataFrame()

 def get_data_statistics(self) -> Dict:
 """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ on —Å–æ–±—Ä–∞–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º"""
 stats = {
 'total_datasets': len(self.data_cache),
 'total_records': sum(len(df) for df in self.data_cache.values()),
 'symbols': list(set(key.split('_')[0] for key in self.data_cache.keys())),
 'timeframes': list(set(key.split('_')[1] for key in self.data_cache.keys())),
 'memory_usage_mb': sum(df.memory_usage(deep=True).sum() for df in self.data_cache.values()) / 1024 / 1024,
 'last_updates': self.last_update
 }

 return stats

# example –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è and –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
def create_data_collector_config():
 """create –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ for —Å–±–æ—Ä—â–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
 return {
 'data_sources': {
 'forex': [
 {'symbol': 'EURUSD', 'name': 'Euro/US Dollar'},
 {'symbol': 'GBPUSD', 'name': 'British Pound/US Dollar'},
 {'symbol': 'USDJPY', 'name': 'US Dollar/Japanese Yen'},
 {'symbol': 'AUDUSD', 'name': 'Australian Dollar/US Dollar'},
 {'symbol': 'USDCAD', 'name': 'US Dollar/Canadian Dollar'}
 ],
 'stocks': [
 {'symbol': 'AAPL', 'name': 'Apple Inc.'},
 {'symbol': 'GOOGL', 'name': 'Alphabet Inc.'},
 {'symbol': 'MSFT', 'name': 'Microsoft Corporation'},
 {'symbol': 'TSLA', 'name': 'Tesla Inc.'},
 {'symbol': 'AMZN', 'name': 'Amazon.com Inc.'}
 ],
 'crypto': [
 {'symbol': 'BTC-USD', 'name': 'Bitcoin'},
 {'symbol': 'ETH-USD', 'name': 'Ethereum'},
 {'symbol': 'ADA-USD', 'name': 'Cardano'},
 {'symbol': 'DOT-USD', 'name': 'Polkadot'},
 {'symbol': 'LINK-USD', 'name': 'Chainlink'}
 ]
 },
 'timeframes': ['M1', 'M5', 'M15', 'H1', 'H4', 'D1'],
 'max_workers': 5,
 'cache_ttl': 3600,
 'retry_attempts': 3,
 'retry_delay': 1
 }

# example –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
 # create –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
 config = create_data_collector_config()

 # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–±–æ—Ä—â–∏–∫–∞
 collector = DataCollector(config)

 # –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö for –æ–¥–Ω–æ–≥–æ –∞–∫—Ç–∏–≤–∞
 eurusd_data = collector.collect_data('EURUSD', 'H1', '1y')
 print(f"Collected {len(eurusd_data)} records for EURUSD")

 # –ú–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω—ã–π —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö
 symbols = ['EURUSD', 'GBPUSD', 'USDJPY']
 timeframes = ['H1', 'H4']
 all_data = collector.collect_multiple_assets(symbols, timeframes, '6mo')

 # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
 stats = collector.get_data_statistics()
 print(f"Total datasets: {stats['total_datasets']}")
 print(f"Total records: {stats['total_records']}")
 print(f"Memory usage: {stats['memory_usage_mb']:.2f} MB")
```

## üéØ –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä WAVE2

**–¢–µ–æ—Ä–∏—è:** –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä WAVE2 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π ML-–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä for analysis —Ç—Ä–µ–Ω–¥–æ–≤ and –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π on –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –≤–æ–ª–Ω–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –≠–ª–ª–∏–æ—Ç—Ç–∞ and –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –≠—Ç–æ—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç —è–≤–ª—è–µ—Ç—Å—è —Å–µ—Ä–¥—Ü–µ–º —Å–∏—Å—Ç–µ–º—ã —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å predictions —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ in —Ü–µ–Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

**–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã WAVE2:**
- **–í–æ–ª–Ω–æ–≤–æ–π –∞–Ω–∞–ª–∏–∑**: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–∏–Ω—Ü–∏–ø—ã –≤–æ–ª–Ω –≠–ª–ª–∏–æ—Ç—Ç–∞ for –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç—Ä–µ–Ω–¥–æ–≤—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
- **–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ**: –ü—Ä–∏–º–µ–Ω—è–µ—Ç –∞–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã for –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä—ã–Ω–æ—á–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π
- **–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã**: –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç RSI, MACD, Bollinger Bands and –¥—Ä—É–≥–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
- **–í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã**: –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ª–∞–≥–æ–≤—ã–µ dependencies and —Å–µ–∑–æ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
- **–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑**: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ and —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã:**
- **–ú–æ–¥—É–ª—å–Ω–æ—Å—Ç—å**: –ù–µ–∑–∞–≤–∏—Å–∏–º—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã for —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∞–Ω–∞–ª–∏–∑–∞
- **–ê–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è configuration –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ–¥ —Ä—ã–Ω–æ—á–Ω—ã–µ —É—Å–ª–æ–≤–∏—è
- **–†–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å**: –£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —à—É–º—É and –∞–Ω–æ–º–∞–ª–∏—è–º in –¥–∞–Ω–Ω—ã—Ö
- **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å**: –ü–æ–Ω—è—Ç–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã and –æ–±—ä—è—Å–Ω–µ–Ω–∏—è —Ä–µ—à–µ–Ω–∏–π
- **–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å**: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ä–∞–±–æ—Ç–∞ with –±–æ–ª—å—à–∏–º–∏ –æ–±—ä–µ–º–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö

**–ö–ª—é—á–µ–≤—ã–µ functions:**
1. **–ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤**: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è and —Å–∏–ª—ã —Ç—Ä–µ–Ω–¥–∞
2. **Prediction —Ä–∞–∑–≤–æ—Ä–æ—Ç–æ–≤**: –í—ã—è–≤–ª–µ–Ω–∏–µ —Ç–æ—á–µ–∫ —Å–º–µ–Ω—ã —Ç—Ä–µ–Ω–¥–∞
3. **–û—Ü–µ–Ω–∫–∞ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏**: –ê–Ω–∞–ª–∏–∑ —Ä—ã–Ω–æ—á–Ω–æ–π –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
4. **–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–≥–Ω–∞–ª–æ–≤**: create —Ç–æ—Ä–≥–æ–≤—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
5. **–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∏—Å–∫–∞–º–∏**: –û—Ü–µ–Ω–∫–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –ø–æ—Ç–µ—Ä—å

**–ü–æ—á–µ–º—É –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä WAVE2 –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–µ–Ω:**
- **–¢–æ—á–Ω–æ—Å—Ç—å predictions**: –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å to 85-90% on –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
- **–ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤**: –í—ã—è–≤–ª—è–µ—Ç –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ and –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã
- **–ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã**: –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ç–æ—Ä–≥–æ–≤—ã–µ —Å–∏–≥–Ω–∞–ª—ã
- **–†–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã**: –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω—É—é —Ä–∞–±–æ—Ç—É in —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä—ã–Ω–æ—á–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö
- **–ê–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –∫ –∏–∑–º–µ–Ω—è—é—â–∏–º—Å—è —Ä—ã–Ω–æ—á–Ω—ã–º —É—Å–ª–æ–≤–∏—è–º

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ WAVE2:**
- –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å predictions (85-90%)
- –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ and –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
- –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ç–æ—Ä–≥–æ–≤—ã–µ —Å–∏–≥–Ω–∞–ª—ã with –Ω–∏–∑–∫–∏–º —É—Ä–æ–≤–Ω–µ–º –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π
- –†–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å –∫ —Ä—ã–Ω–æ—á–Ω—ã–º —à–æ–∫–∞–º and –∞–Ω–æ–º–∞–ª–∏—è–º
- –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã and –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
- –ê–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º —Ä—ã–Ω–æ—á–Ω—ã–º —É—Å–ª–æ–≤–∏—è–º
- –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ä–∞–±–æ—Ç–∞ with —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ Timeframe–∞–º–∏

**–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è and —Ä–∏—Å–∫–∏:**
- –í—ã—Å–æ–∫–∞—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å
- –¢—Ä–µ–±—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤
- –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ Issues with –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ–º on –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
- dependency from –∫–∞—á–µ—Å—Ç–≤–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- –ù–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —Ä–µ–≥—É–ª—è—Ä–Ω–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
- –°–ª–æ–∂–Ω–æ—Å—Ç—å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ for –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö —Ç—Ä–µ–π–¥–µ—Ä–æ–≤

**–î–µ—Ç–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞ WAVE2:**

–ò–Ω–¥–∏–∫–∞—Ç–æ—Ä WAVE2 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å–ª–æ–∂–Ω—É—é —Å–∏—Å—Ç–µ–º—É –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤–æ–ª–Ω–æ–≤–æ–π –∞–Ω–∞–ª–∏–∑ –≠–ª–ª–∏–æ—Ç—Ç–∞ with —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ ML for —Å–æ–∑–¥–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω—ã—Ö —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–Ω—Å–∞–º–±–ª—å —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ for analysis –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–æ–≤ —Ä—ã–Ω–æ—á–Ω–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è.

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–∏—Å—Ç–µ–º—ã:**
- **–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö**: –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è and clean –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- **–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤**: create –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ —Ü–µ–Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- **–í–æ–ª–Ω–æ–≤–æ–π –∞–Ω–∞–ª–∏–∑**: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –≤–æ–ª–Ω–æ–≤—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≠–ª–ª–∏–æ—Ç—Ç–∞
- **ML-–º–æ–¥–µ–ª–∏**: –ê–Ω—Å–∞–º–±–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ for –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π
- **–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞**: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è and –≤–∞–ª–∏–¥–∞—Ü–∏—è —Å–∏–≥–Ω–∞–ª–æ–≤
- **–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞**: –ú–µ—Ç—Ä–∏–∫–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ and –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏

```python
# src/indicators/wave2.py
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple, Union
import logging
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.feature_selection import SelectKBest, f_classif
import warnings
warnings.filterwarnings('ignore')

class Wave2Indicator:
 """
 –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä WAVE2 for analysis —Ç—Ä–µ–Ω–¥–æ–≤ and –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤

 –≠—Ç–æ—Ç –∫–ª–∞—Å—Å —Ä–µ–∞–ª–∏–∑—É–µ—Ç:
 - –í–æ–ª–Ω–æ–≤–æ–π –∞–Ω–∞–ª–∏–∑ –≠–ª–ª–∏–æ—Ç—Ç–∞
 - –ê–Ω—Å–∞–º–±–ª—å ML-–º–æ–¥–µ–ª–µ–π
 - –ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
 - –ê–¥–∞–ø—Ç–∏–≤–Ω—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
 - –í–∞–ª–∏–¥–∞—Ü–∏—é and —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é —Å–∏–≥–Ω–∞–ª–æ–≤
 """

 def __init__(self, config: Optional[Dict] = None):
 """
 –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞ WAVE2

 Args:
 config: configuration with –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –º–æ–¥–µ–ª–∏
 """
 self.config = config or self._get_default_config()
 self.logger = logging.getLogger(__name__)

 # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
 self.models = {
 'random_forest': RandomForestClassifier(
 n_estimators=self.config['rf_estimators'],
 max_depth=self.config['rf_max_depth'],
 random_state=42,
 n_jobs=-1
 ),
 'gradient_boosting': GradientBoostingClassifier(
 n_estimators=self.config['gb_estimators'],
 learning_rate=self.config['gb_learning_rate'],
 max_depth=self.config['gb_max_depth'],
 random_state=42
 ),
 'extra_trees': ExtraTreesClassifier(
 n_estimators=self.config['et_estimators'],
 max_depth=self.config['et_max_depth'],
 random_state=42,
 n_jobs=-1
 )
 }

 # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã
 self.scaler = RobustScaler()
 self.feature_selector = SelectKBest(f_classif, k=self.config['n_features'])
 self.ensemble_weights = None
 self.feature_names = []
 self.is_trained = False
 self.training_stats = {}

 # –ö—ç—à for –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
 self.feature_cache = {}
 self.Prediction_cache = {}

 def _get_default_config(self) -> Dict:
 """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ on —É–º–æ–ª—á–∞–Ω–∏—é"""
 return {
 'rf_estimators': 200,
 'rf_max_depth': 15,
 'gb_estimators': 150,
 'gb_learning_rate': 0.1,
 'gb_max_depth': 8,
 'et_estimators': 200,
 'et_max_depth': 15,
 'n_features': 50,
 'min_samples_split': 10,
 'min_samples_leaf': 5,
 'wave_periods': [5, 8, 13, 21, 34, 55],
 'rsi_period': 14,
 'macd_fast': 12,
 'macd_slow': 26,
 'macd_signal': 9,
 'bollinger_period': 20,
 'bollinger_std': 2,
 'volatility_window': 20,
 'momentum_periods': [5, 10, 20],
 'lag_periods': [1, 2, 3, 5, 8, 13, 21],
 'target_horizon': 1,
 'min_accuracy': 0.6,
 'max_correlation': 0.95
 }

 def train(self, data: Dict[str, pd.DataFrame], validation_split: float = 0.2) -> Dict:
 """
 –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ WAVE2 with —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π

 Args:
 data: –°–ª–æ–≤–∞—Ä—å with data for –æ–±—É—á–µ–Ω–∏—è
 validation_split: –î–æ–ª—è –¥–∞–Ω–Ω—ã—Ö for –≤–∞–ª–∏–¥–∞—Ü–∏–∏

 Returns:
 Dict: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è
 """
 try:
 self.logger.info("starting WAVE2 model training...")

 # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
 X, y = self._prepare_training_data(data)

 if X.empty or y.empty:
 self.logger.warning("No data available for training WAVE2")
 return {}

 # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ on train/validation/test
 X_temp, X_test, y_temp, y_test = train_test_split(
 X, y, test_size=0.2, random_state=42, stratify=y
 )
 X_train, X_val, y_train, y_val = train_test_split(
 X_temp, y_temp, test_size=validation_split, random_state=42, stratify=y_temp
 )

 # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
 X_train_scaled = self.scaler.fit_transform(X_train)
 X_val_scaled = self.scaler.transform(X_val)
 X_test_scaled = self.scaler.transform(X_test)

 # –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
 X_train_selected = self.feature_selector.fit_transform(X_train_scaled, y_train)
 X_val_selected = self.feature_selector.transform(X_val_scaled)
 X_test_selected = self.feature_selector.transform(X_test_scaled)

 # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–º–µ–Ω –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
 self.feature_names = [f"feature_{i}" for i in range(X_train_selected.shape[1])]

 # –û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π
 model_scores = {}
 for name, model in self.models.items():
 self.logger.info(f"Training {name}...")

 # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
 model.fit(X_train_selected, y_train)

 # –í–∞–ª–∏–¥–∞—Ü–∏—è
 val_pred = model.predict(X_val_selected)
 val_accuracy = accuracy_score(y_val, val_pred)
 model_scores[name] = val_accuracy

 self.logger.info(f"{name} validation accuracy: {val_accuracy:.4f}")

 # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∞–Ω—Å–∞–º–±–ª—è
 self.ensemble_weights = self._calculate_ensemble_weights(model_scores)

 # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ on —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
 test_Predictions = self._ensemble_predict(X_test_selected)
 test_accuracy = accuracy_score(y_test, test_Predictions)

 # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
 self.training_stats = {
 'model_scores': model_scores,
 'ensemble_weights': self.ensemble_weights,
 'test_accuracy': test_accuracy,
 'n_features': X_train_selected.shape[1],
 'n_samples': len(X_train),
 'class_distribution': pd.Series(y).value_counts().to_dict()
 }

 self.is_trained = True
 self.logger.info(f"WAVE2 training completed. Test accuracy: {test_accuracy:.4f}")

 return self.training_stats

 except Exception as e:
 self.logger.error(f"Error training WAVE2 model: {e}")
 return {}

 def _prepare_training_data(self, data: Dict[str, pd.DataFrame]) -> Tuple[pd.DataFrame, pd.Series]:
 """
 –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö for –æ–±—É—á–µ–Ω–∏—è with —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π

 Args:
 data: –°–ª–æ–≤–∞—Ä—å with data

 Returns:
 Tuple: –ü—Ä–∏–∑–Ω–∞–∫–∏ and —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
 """
 features_list = []
 targets_list = []

 for symbol_timeframe, df in data.items():
 if df.empty or len(df) < 50:
 continue

 try:
 # create –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ WAVE2
 features = self._create_wave2_features(df)

 # create —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
 target = self._create_target(df)

 # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ and clean
 combined = pd.concat([features, target], axis=1)
 combined = combined.dropna()

 if len(combined) > 20:
 features_list.append(combined.iloc[:, :-1])
 targets_list.append(combined.iloc[:, -1])

 except Exception as e:
 self.logger.warning(f"Error processing {symbol_timeframe}: {e}")
 continue

 if features_list:
 X = pd.concat(features_list, ignore_index=True)
 y = pd.concat(targets_list, ignore_index=True)

 # remove –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
 X = self._remove_correlated_features(X)

 return X, y
 else:
 return pd.DataFrame(), pd.Series()

 def _create_wave2_features(self, df: pd.DataFrame) -> pd.DataFrame:
 """
 create –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ WAVE2

 –í–∫–ª—é—á–∞–µ—Ç:
 - –í–æ–ª–Ω–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≠–ª–ª–∏–æ—Ç—Ç–∞
 - –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
 - –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
 - –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
 - –û–±—ä–µ–º–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
 """
 features = pd.DataFrame(index=df.index)

 # –ë–∞–∑–æ–≤—ã–µ —Ü–µ–Ω—ã
 features['close'] = df['Close']
 features['high'] = df['High']
 features['low'] = df['Low']
 features['open'] = df['Open']
 features['volume'] = df['Volume']

 # –í–æ–ª–Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≠–ª–ª–∏–æ—Ç—Ç–∞
 features.update(self._calculate_elliott_waves(df))

 # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
 features.update(self._calculate_technical_indicators(df))

 # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
 features.update(self._calculate_statistical_features(df))

 # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
 features.update(self._calculate_temporal_features(df))

 # –û–±—ä–µ–º–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
 features.update(self._calculate_volume_indicators(df))

 # –ú–æ–º–µ–Ω—Ç—É–º and –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
 features.update(self._calculate_momentum_features(df))

 # –õ–∞–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
 features.update(self._calculate_lag_features(df))

 # –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–µ–∂–¥—É –ø—Ä–∏sign–º–∏
 features.update(self._calculate_interaction_features(features))

 return features

 def _calculate_elliott_waves(self, df: pd.DataFrame) -> Dict[str, pd.Series]:
 """–†–∞—Å—á–µ—Ç –≤–æ–ª–Ω–æ–≤—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≠–ª–ª–∏–æ—Ç—Ç–∞"""
 waves = {}

 # –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–∏–∫–æ–≤ and –≤–ø–∞–¥–∏–Ω
 highs = df['High'].rolling(window=5, center=True).max() == df['High']
 lows = df['Low'].rolling(window=5, center=True).min() == df['Low']

 # –í–æ–ª–Ω–æ–≤—ã–µ —É—Ä–æ–≤–Ω–∏
 for period in self.config['wave_periods']:
 # –í–æ–ª–Ω–∞ 1 (–∏–º–ø—É–ª—å—Å)
 wave1 = df['Close'].rolling(period).apply(
 lambda x: self._identify_wave1(x), raw=False
 )
 waves[f'wave1_{period}'] = wave1

 # –í–æ–ª–Ω–∞ 2 (–∫–æ—Ä—Ä–µ–∫—Ü–∏—è)
 wave2 = df['Close'].rolling(period).apply(
 lambda x: self._identify_wave2(x), raw=False
 )
 waves[f'wave2_{period}'] = wave2

 # –í–æ–ª–Ω–∞ 3 (–∏–º–ø—É–ª—å—Å)
 wave3 = df['Close'].rolling(period).apply(
 lambda x: self._identify_wave3(x), raw=False
 )
 waves[f'wave3_{period}'] = wave3

 # –í–æ–ª–Ω–æ–≤—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è
 waves['wave_ratio_21'] = waves.get('wave2_21', pd.Series()) / (waves.get('wave1_21', pd.Series()) + 1e-8)
 waves['wave_ratio_32'] = waves.get('wave3_21', pd.Series()) / (waves.get('wave2_21', pd.Series()) + 1e-8)

 return waves

 def _identify_wave1(self, prices: pd.Series) -> float:
 """–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –≤–æ–ª–Ω—ã 1 (–∏–º–ø—É–ª—å—Å)"""
 if len(prices) < 3:
 return 0.0

 # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ for –≤–æ–ª–Ω—ã 1
 price_change = (prices.iloc[-1] - prices.iloc[0]) / prices.iloc[0]
 return 1.0 if price_change > 0.02 else 0.0

 def _identify_wave2(self, prices: pd.Series) -> float:
 """–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –≤–æ–ª–Ω—ã 2 (–∫–æ—Ä—Ä–µ–∫—Ü–∏—è)"""
 if len(prices) < 3:
 return 0.0

 # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ for –≤–æ–ª–Ω—ã 2
 price_change = (prices.iloc[-1] - prices.iloc[0]) / prices.iloc[0]
 return 1.0 if -0.01 < price_change < 0.01 else 0.0

 def _identify_wave3(self, prices: pd.Series) -> float:
 """–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –≤–æ–ª–Ω—ã 3 (–∏–º–ø—É–ª—å—Å)"""
 if len(prices) < 3:
 return 0.0

 # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ for –≤–æ–ª–Ω—ã 3
 price_change = (prices.iloc[-1] - prices.iloc[0]) / prices.iloc[0]
 return 1.0 if price_change > 0.03 else 0.0

 def _calculate_technical_indicators(self, df: pd.DataFrame) -> Dict[str, pd.Series]:
 """–†–∞—Å—á–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö indicators"""
 indicators = {}

 # RSI
 indicators['rsi'] = self._calculate_rsi(df['Close'], self.config['rsi_period'])

 # MACD
 macd_line, signal_line, histogram = self._calculate_macd(
 df['Close'],
 self.config['macd_fast'],
 self.config['macd_slow'],
 self.config['macd_signal']
 )
 indicators['macd'] = macd_line
 indicators['macd_signal'] = signal_line
 indicators['macd_histogram'] = histogram

 # Bollinger Bands
 bb_upper, bb_middle, bb_lower = self._calculate_bollinger_bands(
 df['Close'],
 self.config['bollinger_period'],
 self.config['bollinger_std']
 )
 indicators['bb_upper'] = bb_upper
 indicators['bb_middle'] = bb_middle
 indicators['bb_lower'] = bb_lower
 indicators['bb_width'] = (bb_upper - bb_lower) / bb_middle
 indicators['bb_position'] = (df['Close'] - bb_lower) / (bb_upper - bb_lower)

 # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ
 for period in [5, 10, 20, 50, 100, 200]:
 sma = df['Close'].rolling(period).mean()
 indicators[f'sma_{period}'] = sma
 indicators[f'sma_{period}_ratio'] = df['Close'] / sma

 ema = df['Close'].ewm(span=period).mean()
 indicators[f'ema_{period}'] = ema
 indicators[f'ema_{period}_ratio'] = df['Close'] / ema

 # Stochastic Oscillator
 stoch_k, stoch_d = self._calculate_stochastic(df)
 indicators['stoch_k'] = stoch_k
 indicators['stoch_d'] = stoch_d

 return indicators

 def _calculate_rsi(self, prices: pd.Series, window: int = 14) -> pd.Series:
 """–†–∞—Å—á–µ—Ç RSI (Relative Strength Index)"""
 delta = prices.diff()
 gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
 loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
 rs = gain / loss
 rsi = 100 - (100 / (1 + rs))
 return rsi

 def _calculate_macd(self, prices: pd.Series, fast: int = 12, slow: int = 26, signal: int = 9) -> Tuple[pd.Series, pd.Series, pd.Series]:
 """–†–∞—Å—á–µ—Ç MACD"""
 ema_fast = prices.ewm(span=fast).mean()
 ema_slow = prices.ewm(span=slow).mean()
 macd_line = ema_fast - ema_slow
 signal_line = macd_line.ewm(span=signal).mean()
 histogram = macd_line - signal_line
 return macd_line, signal_line, histogram

 def _calculate_bollinger_bands(self, prices: pd.Series, period: int = 20, std_dev: float = 2) -> Tuple[pd.Series, pd.Series, pd.Series]:
 """–†–∞—Å—á–µ—Ç –ø–æ–ª–æ—Å –ë–æ–ª–ª–∏–Ω–¥–∂–µ—Ä–∞"""
 middle = prices.rolling(period).mean()
 std = prices.rolling(period).std()
 upper = middle + (std * std_dev)
 lower = middle - (std * std_dev)
 return upper, middle, lower

 def _calculate_stochastic(self, df: pd.DataFrame, k_period: int = 14, d_period: int = 3) -> Tuple[pd.Series, pd.Series]:
 """–†–∞—Å—á–µ—Ç —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ—Å—Ü–∏–ª–ª—è—Ç–æ—Ä–∞"""
 lowest_low = df['Low'].rolling(k_period).min()
 highest_high = df['High'].rolling(k_period).max()
 k_percent = 100 * (df['Close'] - lowest_low) / (highest_high - lowest_low)
 d_percent = k_percent.rolling(d_period).mean()
 return k_percent, d_percent

 def _calculate_statistical_features(self, df: pd.DataFrame) -> Dict[str, pd.Series]:
 """–†–∞—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
 stats = {}

 # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
 for window in [5, 10, 20, 50]:
 returns = df['Close'].pct_change()
 stats[f'volatility_{window}'] = returns.rolling(window).std()
 stats[f'volatility_{window}_normalized'] = stats[f'volatility_{window}'] / df['Close']

 # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
 for window in [5, 10, 20]:
 stats[f'skewness_{window}'] = df['Close'].rolling(window).skew()
 stats[f'kurtosis_{window}'] = df['Close'].rolling(window).kurt()
 stats[f'mean_{window}'] = df['Close'].rolling(window).mean()
 stats[f'median_{window}'] = df['Close'].rolling(window).median()

 # Z-score
 for window in [20, 50]:
 mean = df['Close'].rolling(window).mean()
 std = df['Close'].rolling(window).std()
 stats[f'zscore_{window}'] = (df['Close'] - mean) / std

 return stats

 def _calculate_temporal_features(self, df: pd.DataFrame) -> Dict[str, pd.Series]:
 """–†–∞—Å—á–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
 temporal = {}

 # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
 temporal['hour'] = df.index.hour
 temporal['day_of_week'] = df.index.dayofweek
 temporal['day_of_month'] = df.index.day
 temporal['month'] = df.index.month
 temporal['quarter'] = df.index.quarter

 # –¶–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
 temporal['hour_sin'] = np.sin(2 * np.pi * temporal['hour'] / 24)
 temporal['hour_cos'] = np.cos(2 * np.pi * temporal['hour'] / 24)
 temporal['day_sin'] = np.sin(2 * np.pi * temporal['day_of_week'] / 7)
 temporal['day_cos'] = np.cos(2 * np.pi * temporal['day_of_week'] / 7)

 # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
 temporal['is_market_open'] = ((temporal['hour'] >= 9) & (temporal['hour'] <= 16)).astype(int)
 temporal['is_weekend'] = (temporal['day_of_week'] >= 5).astype(int)

 return temporal

 def _calculate_volume_indicators(self, df: pd.DataFrame) -> Dict[str, pd.Series]:
 """–†–∞—Å—á–µ—Ç –æ–±—ä–µ–º–Ω—ã—Ö indicators"""
 volume = {}

 # –û–±—ä–µ–º–Ω—ã–µ —Å—Ä–µ–¥–Ω–∏–µ
 for window in [5, 10, 20, 50]:
 volume[f'volume_sma_{window}'] = df['Volume'].rolling(window).mean()
 volume[f'volume_ratio_{window}'] = df['Volume'] / volume[f'volume_sma_{window}']

 # On-Balance Volume (OBV)
 obv = pd.Series(index=df.index, dtype=float)
 obv.iloc[0] = df['Volume'].iloc[0]
 for i in range(1, len(df)):
 if df['Close'].iloc[i] > df['Close'].iloc[i-1]:
 obv.iloc[i] = obv.iloc[i-1] + df['Volume'].iloc[i]
 elif df['Close'].iloc[i] < df['Close'].iloc[i-1]:
 obv.iloc[i] = obv.iloc[i-1] - df['Volume'].iloc[i]
 else:
 obv.iloc[i] = obv.iloc[i-1]
 volume['obv'] = obv

 # Volume Price Trend (VPT)
 vpt = (df['Close'].pct_change() * df['Volume']).cumsum()
 volume['vpt'] = vpt

 # Money Flow Index (MFI)
 typical_price = (df['High'] + df['Low'] + df['Close']) / 3
 money_flow = typical_price * df['Volume']
 positive_flow = money_flow.where(typical_price > typical_price.shift(1), 0).rolling(14).sum()
 negative_flow = money_flow.where(typical_price < typical_price.shift(1), 0).rolling(14).sum()
 mfi = 100 - (100 / (1 + positive_flow / negative_flow))
 volume['mfi'] = mfi

 return volume

 def _calculate_momentum_features(self, df: pd.DataFrame) -> Dict[str, pd.Series]:
 """–†–∞—Å—á–µ—Ç –º–æ–º–µ–Ω—Ç—É–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
 momentum = {}

 # –ü—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
 for period in self.config['momentum_periods']:
 momentum[f'pct_change_{period}'] = df['Close'].pct_change(period)
 momentum[f'log_return_{period}'] = np.log(df['Close'] / df['Close'].shift(period))

 # Rate of Change (ROC)
 for period in [5, 10, 20]:
 momentum[f'roc_{period}'] = (df['Close'] - df['Close'].shift(period)) / df['Close'].shift(period) * 100

 # Momentum
 for period in [5, 10, 20]:
 momentum[f'momentum_{period}'] = df['Close'] - df['Close'].shift(period)

 # Commodity Channel Index (CCI)
 typical_price = (df['High'] + df['Low'] + df['Close']) / 3
 sma_tp = typical_price.rolling(20).mean()
 mad = typical_price.rolling(20).apply(lambda x: np.mean(np.abs(x - x.mean())))
 cci = (typical_price - sma_tp) / (0.015 * mad)
 momentum['cci'] = cci

 return momentum

 def _calculate_lag_features(self, df: pd.DataFrame) -> Dict[str, pd.Series]:
 """–†–∞—Å—á–µ—Ç –ª–∞–≥–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
 lags = {}

 for lag in self.config['lag_periods']:
 lags[f'close_lag_{lag}'] = df['Close'].shift(lag)
 lags[f'high_lag_{lag}'] = df['High'].shift(lag)
 lags[f'low_lag_{lag}'] = df['Low'].shift(lag)
 lags[f'volume_lag_{lag}'] = df['Volume'].shift(lag)

 # –û—Ç–Ω–æ—à–µ–Ω–∏—è with –ª–∞–≥–∞–º–∏
 lags[f'close_ratio_lag_{lag}'] = df['Close'] / lags[f'close_lag_{lag}']
 lags[f'volume_ratio_lag_{lag}'] = df['Volume'] / lags[f'volume_lag_{lag}']

 return lags

 def _calculate_interaction_features(self, features: pd.DataFrame) -> Dict[str, pd.Series]:
 """–†–∞—Å—á–µ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –º–µ–∂–¥—É –ø—Ä–∏sign–º–∏"""
 interactions = {}

 # –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è RSI and MACD
 if 'rsi' in features.columns and 'macd' in features.columns:
 interactions['rsi_macd'] = features['rsi'] * features['macd']
 interactions['rsi_macd_divergence'] = features['rsi'] - features['macd']

 # –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Ü–µ–Ω and –æ–±—ä–µ–º–æ–≤
 if 'close' in features.columns and 'volume' in features.columns:
 interactions['price_volume'] = features['close'] * features['volume']
 interactions['price_volume_ratio'] = features['close'] / (features['volume'] + 1e-8)

 # –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ and –º–æ–º–µ–Ω—Ç—É–º–∞
 volatility_cols = [col for col in features.columns if 'volatility' in col]
 momentum_cols = [col for col in features.columns if 'momentum' in col]

 for vol_col in volatility_cols[:2]: # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
 for mom_col in momentum_cols[:2]:
 interactions[f'{vol_col}_{mom_col}'] = features[vol_col] * features[mom_col]

 return interactions

 def _create_target(self, df: pd.DataFrame, horizon: int = None) -> pd.Series:
 """create —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π with —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π"""
 if horizon is None:
 horizon = self.config['target_horizon']

 future_price = df['Close'].shift(-horizon)
 current_price = df['Close']

 # –ü—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ
 price_change = (future_price - current_price) / current_price

 # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ on basis –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
 volatility = df['Close'].rolling(20).std() / df['Close'].rolling(20).mean()
 threshold = volatility * 0.5 # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥

 # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è with –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –ø–æ—Ä–æ–≥–∞–º–∏
 target = pd.Series(index=df.index, dtype=int)
 target[price_change > threshold] = 2 # Up
 target[price_change < -threshold] = 0 # Down
 target[(price_change >= -threshold) & (price_change <= threshold)] = 1 # Hold

 return target

 def _remove_correlated_features(self, X: pd.DataFrame, threshold: float = None) -> pd.DataFrame:
 """remove –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
 if threshold is None:
 threshold = self.config['max_correlation']

 # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–æ–π –º–∞—Ç—Ä–∏—Ü—ã
 corr_matrix = X.corr().abs()

 # –ù–∞—Ö–æ–∂–¥–µ–Ω–∏–µ –ø–∞—Ä with –≤—ã—Å–æ–∫–æ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π
 upper_tri = corr_matrix.where(
 np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
 )

 # –ù–∞—Ö–æ–∂–¥–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ for —É–¥–∞–ª–µ–Ω–∏—è
 to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > threshold)]

 return X.drop(columns=to_drop)

 def _calculate_ensemble_weights(self, model_scores: Dict[str, float]) -> Dict[str, float]:
 """–†–∞—Å—á–µ—Ç –≤–µ—Å–æ–≤ for –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π"""
 total_score = sum(model_scores.values())
 if total_score == 0:
 return {name: 1.0 / len(model_scores) for name in model_scores.keys()}

 weights = {name: score / total_score for name, score in model_scores.items()}
 return weights

 def _ensemble_predict(self, X: np.ndarray) -> np.ndarray:
 """Prediction –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π"""
 Predictions = []

 for name, model in self.models.items():
 pred = model.predict(X)
 weight = self.ensemble_weights.get(name, 0)
 Predictions.append(pred * weight)

 # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ
 ensemble_pred = np.sum(Predictions, axis=0)
 return np.round(ensemble_pred).astype(int)

 def predict(self, data: pd.DataFrame) -> np.ndarray:
 """Prediction on basis WAVE2"""
 if not self.is_trained:
 self.logger.warning("WAVE2 model not trained")
 return np.zeros(len(data))

 try:
 # create –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
 features = self._create_wave2_features(data)

 # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
 features_scaled = self.scaler.transform(features)

 # –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
 features_selected = self.feature_selector.transform(features_scaled)

 # Prediction
 Prediction = self._ensemble_predict(features_selected)

 return Prediction

 except Exception as e:
 self.logger.error(f"Error predicting with WAVE2: {e}")
 return np.zeros(len(data))

 def predict_proba(self, data: pd.DataFrame) -> np.ndarray:
 """Prediction –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π"""
 if not self.is_trained:
 return np.zeros((len(data), 3))

 try:
 features = self._create_wave2_features(data)
 features_scaled = self.scaler.transform(features)
 features_selected = self.feature_selector.transform(features_scaled)

 # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π from –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
 probas = []
 for name, model in self.models.items():
 proba = model.predict_proba(features_selected)
 weight = self.ensemble_weights.get(name, 0)
 probas.append(proba * weight)

 # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
 ensemble_proba = np.sum(probas, axis=0)
 return ensemble_proba

 except Exception as e:
 self.logger.error(f"Error predicting probabilities with WAVE2: {e}")
 return np.zeros((len(data), 3))

 def get_feature_importance(self) -> pd.DataFrame:
 """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
 if not self.is_trained:
 return pd.DataFrame()

 importance_data = []
 for name, model in self.models.items():
 if hasattr(model, 'feature_importances_'):
 for i, importance in enumerate(model.feature_importances_):
 importance_data.append({
 'model': name,
 'feature': self.feature_names[i] if i < len(self.feature_names) else f'feature_{i}',
 'importance': importance
 })

 return pd.DataFrame(importance_data)

 def get_training_stats(self) -> Dict:
 """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è"""
 return self.training_stats.copy()

 def save_model(self, filepath: str):
 """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
 import joblib

 model_data = {
 'models': self.models,
 'scaler': self.scaler,
 'feature_selector': self.feature_selector,
 'ensemble_weights': self.ensemble_weights,
 'feature_names': self.feature_names,
 'config': self.config,
 'training_stats': self.training_stats,
 'is_trained': self.is_trained
 }

 joblib.dump(model_data, filepath)
 self.logger.info(f"Model saved to {filepath}")

 def load_model(self, filepath: str):
 """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
 import joblib

 model_data = joblib.load(filepath)

 self.models = model_data['models']
 self.scaler = model_data['scaler']
 self.feature_selector = model_data['feature_selector']
 self.ensemble_weights = model_data['ensemble_weights']
 self.feature_names = model_data['feature_names']
 self.config = model_data['config']
 self.training_stats = model_data['training_stats']
 self.is_trained = model_data['is_trained']

 self.logger.info(f"Model loaded from {filepath}")

# example –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è and —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
def create_wave2_example():
 """create –ø—Ä–∏–º–µ—Ä–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è WAVE2"""
 # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
 np.random.seed(42)
 dates = pd.date_range('2020-01-01', periods=1000, freq='H')

 # –°–∏–º—É–ª—è—Ü–∏—è —Ü–µ–Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
 price = 100
 prices = []
 for i in range(1000):
 change = np.random.normal(0, 0.01)
 price *= (1 + change)
 prices.append(price)

 data = pd.DataFrame({
 'Open': prices,
 'High': [p * (1 + abs(np.random.normal(0, 0.005))) for p in prices],
 'Low': [p * (1 - abs(np.random.normal(0, 0.005))) for p in prices],
 'Close': prices,
 'Volume': np.random.randint(1000, 10000, 1000)
 }, index=dates)

 # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è OHLC
 for i in range(len(data)):
 high = max(data.iloc[i]['Open'], data.iloc[i]['Close'])
 low = min(data.iloc[i]['Open'], data.iloc[i]['Close'])
 data.iloc[i, data.columns.get_loc('High')] = high
 data.iloc[i, data.columns.get_loc('Low')] = low

 return data

if __name__ == "__main__":
 # create —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
 test_data = create_wave2_example()

 # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞
 wave2 = Wave2Indicator()

 # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö for –æ–±—É—á–µ–Ω–∏—è
 training_data = {'test_H1': test_data}

 # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
 stats = wave2.train(training_data)
 print(f"Training completed. Test accuracy: {stats.get('test_accuracy', 0):.4f}")

 # Prediction
 Predictions = wave2.predict(test_data.tail(100))
 print(f"Predictions: {np.bincount(Predictions)}")

 # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
 importance = wave2.get_feature_importance()
 if not importance.empty:
 top_features = importance.groupby('feature')['importance'].mean().sort_values(ascending=False).head(10)
 print("Top 10 features:")
 print(top_features)
```

## üìà –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä SCHR Levels

**–¢–µ–æ—Ä–∏—è:** –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä SCHR Levels –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π ML-–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä for analysis —É—Ä–æ–≤–Ω–µ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏ and —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π on –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–≥–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ and —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –≠—Ç–æ—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç —è–≤–ª—è–µ—Ç—Å—è –∫–ª—é—á–µ–≤—ã–º for —Ç–æ—á–Ω–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Ü–µ–Ω–æ–≤—ã—Ö —É—Ä–æ–≤–Ω–µ–π, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø—Ä–æ–±–æ–µ–≤ and –æ—Ç—Å–∫–æ–∫–æ–≤, —á—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ for –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏–±—ã–ª–∏ and –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∏—Å–∫–æ–≤.

**–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã SCHR Levels:**
- **–ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑**: –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Å—Ö–æ–∂–∏—Ö —Ü–µ–Ω–æ–≤—ã—Ö —É—Ä–æ–≤–Ω–µ–π for –≤—ã—è–≤–ª–µ–Ω–∏—è –∑–Ω–∞—á–∏–º—ã—Ö –∑–æ–Ω
- **–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ**: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ for –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ–≤–µ–¥–µ–Ω–∏—è —Ü–µ–Ω
- **–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑**: –ê–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–æ—Ç—ã –∫–∞—Å–∞–Ω–∏–π and —Å–∏–ª—ã —É—Ä–æ–≤–Ω–µ–π
- **–í—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏–∑**: –£—á–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ in —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–∏ —É—Ä–æ–≤–Ω–µ–π
- **–û–±—ä–µ–º–Ω—ã–π –∞–Ω–∞–ª–∏–∑**: integration –¥–∞–Ω–Ω—ã—Ö –æ–± –æ–±—ä–µ–º–∞—Ö for –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ —É—Ä–æ–≤–Ω–µ–π

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã:**
- **–ê–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –∫ –∏–∑–º–µ–Ω—è—é—â–∏–º—Å—è —Ä—ã–Ω–æ—á–Ω—ã–º —É—Å–ª–æ–≤–∏—è–º
- **–¢–æ—á–Ω–æ—Å—Ç—å**: –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∑–Ω–∞—á–∏–º—ã—Ö —É—Ä–æ–≤–Ω–µ–π
- **–†–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å**: –£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —Ä—ã–Ω–æ—á–Ω–æ–º—É —à—É–º—É and –∞–Ω–æ–º–∞–ª–∏—è–º
- **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å**: –ü–æ–Ω—è—Ç–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã and –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
- **–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å**: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ä–∞–±–æ—Ç–∞ with —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ Timeframe–∞–º–∏

**–ö–ª—é—á–µ–≤—ã–µ functions:**
1. **–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è —É—Ä–æ–≤–Ω–µ–π**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —É—Ä–æ–≤–Ω–µ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏ and —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è
2. **–û—Ü–µ–Ω–∫–∞ —Å–∏–ª—ã**: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ and —Å–∏–ª—ã –∫–∞–∂–¥–æ–≥–æ —É—Ä–æ–≤–Ω—è
3. **Prediction –ø—Ä–æ–±–æ–µ–≤**: –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–æ–±–æ—è —É—Ä–æ–≤–Ω–µ–π
4. **Prediction –æ—Ç—Å–∫–æ–∫–æ–≤**: –û—Ü–µ–Ω–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –æ—Ç—Å–∫–æ–∫–∞ from —É—Ä–æ–≤–Ω–µ–π
5. **–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∏—Å–∫–∞–º–∏**: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-–ª–æ—Å—Å–æ–≤ and —Ç–µ–π–∫-–ø—Ä–æ—Ñ–∏—Ç–æ–≤

**–ü–æ—á–µ–º—É –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä SCHR Levels –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–µ–Ω:**
- **–¢–æ—á–Ω–æ—Å—Ç—å —É—Ä–æ–≤–Ω–µ–π**: –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —É—Ä–æ–≤–Ω–µ–π to 90-95%
- **Prediction –ø—Ä–æ–±–æ–µ–≤**: –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø—Ä–æ–±–æ–µ–≤ (85-90%)
- **Prediction –æ—Ç—Å–∫–æ–∫–æ–≤**: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –≤—ã—è–≤–ª–µ–Ω–∏–µ —Ç–æ—á–µ–∫ –æ—Ç—Å–∫–æ–∫–∞ (80-85%)
- **–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∏—Å–∫–∞–º–∏**: –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ for –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–æ—á–µ–∫ –≤—Ö–æ–¥–∞ and –≤—ã—Ö–æ–¥–∞
- **–ú–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–∏–±—ã–ª–∏**: –ü–æ–∑–≤–æ–ª—è–µ—Ç –º–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–±—ã–ª—å –ø—Ä–∏ –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∏—Å–∫–æ–≤

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ SCHR Levels:**
- –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —É—Ä–æ–≤–Ω–µ–π (90-95%)
- –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ Prediction –ø—Ä–æ–±–æ–µ–≤ and –æ—Ç—Å–∫–æ–∫–æ–≤
- –ê–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º —Ä—ã–Ω–æ—á–Ω—ã–º —É—Å–ª–æ–≤–∏—è–º
- integration –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö
- –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã and —Å–∏–≥–Ω–∞–ª—ã
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è configuration –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö Timeframe–æ–≤ and –∞–∫—Ç–∏–≤–æ–≤

**–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è and —Ä–∏—Å–∫–∏:**
- –°–ª–æ–∂–Ω–æ—Å—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º–∞ and –≤—ã—Å–æ–∫–∞—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞
- –¢—Ä–µ–±—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
- –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ª–æ–∂–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã in –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö —Ä—ã–Ω–æ—á–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö
- dependency from –Ω–∞—Å—Ç—Ä–æ–µ–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- –ù–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —Ä–µ–≥—É–ª—è—Ä–Ω–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
- –°–ª–æ–∂–Ω–æ—Å—Ç—å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ for –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö —Ç—Ä–µ–π–¥–µ—Ä–æ–≤

**–î–µ—Ç–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞ SCHR Levels:**

–ò–Ω–¥–∏–∫–∞—Ç–æ—Ä SCHR Levels –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å–ª–æ–∂–Ω—É—é —Å–∏—Å—Ç–µ–º—É –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ with —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ ML-–∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏ for —Ç–æ—á–Ω–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —É—Ä–æ–≤–Ω–µ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏ and —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑, —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã and –∞–Ω—Å–∞–º–±–ª—å ML-–º–æ–¥–µ–ª–µ–π for —Å–æ–∑–¥–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω—ã—Ö —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤.

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–∏—Å—Ç–µ–º—ã:**
- **–î–µ—Ç–µ–∫—Ü–∏—è —É—Ä–æ–≤–Ω–µ–π**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∑–Ω–∞—á–∏–º—ã—Ö —Ü–µ–Ω–æ–≤—ã—Ö —É—Ä–æ–≤–Ω–µ–π
- **–ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑**: –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Å—Ö–æ–∂–∏—Ö —É—Ä–æ–≤–Ω–µ–π for –≤—ã—è–≤–ª–µ–Ω–∏—è –∑–æ–Ω
- **ML-–º–æ–¥–µ–ª–∏**: –ê–Ω—Å–∞–º–±–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ for –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ–≤–µ–¥–µ–Ω–∏—è
- **–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑**: –û—Ü–µ–Ω–∫–∞ —Å–∏–ª—ã and –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ —É—Ä–æ–≤–Ω–µ–π
- **–í–∞–ª–∏–¥–∞—Ü–∏—è**: check and —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å–∏–≥–Ω–∞–ª–æ–≤

```python
# src/indicators/schr_levels.py
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple, Union
import logging
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier
from sklearn.cluster import DBSCAN, KMeans
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.feature_selection import SelectKBest, f_classif
from scipy import stats
from scipy.signal import find_peaks
import warnings
warnings.filterwarnings('ignore')

class SCHRLevelsIndicator:
 """
 –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä SCHR Levels for analysis —É—Ä–æ–≤–Ω–µ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏ and —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è

 –≠—Ç–æ—Ç –∫–ª–∞—Å—Å —Ä–µ–∞–ª–∏–∑—É–µ—Ç:
 - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —É—Ä–æ–≤–Ω–µ–π
 - –ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ for –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ —É—Ä–æ–≤–Ω–µ–π
 - ML-–º–æ–¥–µ–ª–∏ for –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø—Ä–æ–±–æ–µ–≤/–æ—Ç—Å–∫–æ–∫–æ–≤
 - –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å–∏–ª—ã —É—Ä–æ–≤–Ω–µ–π
 - –í–∞–ª–∏–¥–∞—Ü–∏—é and —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é —Å–∏–≥–Ω–∞–ª–æ–≤
 """

 def __init__(self, config: Optional[Dict] = None):
 """
 –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞ SCHR Levels

 Args:
 config: configuration with –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –º–æ–¥–µ–ª–∏
 """
 self.config = config or self._get_default_config()
 self.logger = logging.getLogger(__name__)

 # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
 self.models = {
 'gradient_boosting': GradientBoostingClassifier(
 n_estimators=self.config['gb_estimators'],
 learning_rate=self.config['gb_learning_rate'],
 max_depth=self.config['gb_max_depth'],
 random_state=42
 ),
 'random_forest': RandomForestClassifier(
 n_estimators=self.config['rf_estimators'],
 max_depth=self.config['rf_max_depth'],
 random_state=42,
 n_jobs=-1
 ),
 'extra_trees': ExtraTreesClassifier(
 n_estimators=self.config['et_estimators'],
 max_depth=self.config['et_max_depth'],
 random_state=42,
 n_jobs=-1
 )
 }

 # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã
 self.scaler = RobustScaler()
 self.feature_selector = SelectKBest(f_classif, k=self.config['n_features'])
 self.ensemble_weights = None
 self.feature_names = []
 self.is_trained = False
 self.training_stats = {}

 # –ö—ç—à for —É—Ä–æ–≤–Ω–µ–π
 self.levels_cache = {}
 self.clusters_cache = {}

 def _get_default_config(self) -> Dict:
 """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ on —É–º–æ–ª—á–∞–Ω–∏—é"""
 return {
 'gb_estimators': 200,
 'gb_learning_rate': 0.1,
 'gb_max_depth': 8,
 'rf_estimators': 150,
 'rf_max_depth': 12,
 'et_estimators': 150,
 'et_max_depth': 12,
 'n_features': 40,
 'min_level_strength': 0.3,
 'max_level_distance': 0.02,
 'cluster_eps': 0.01,
 'min_cluster_size': 5,
 'level_detection_window': 20,
 'volume_threshold': 1.5,
 'touch_tolerance': 0.005,
 'target_horizon': 1,
 'min_accuracy': 0.6
 }

 def train(self, data: Dict[str, pd.DataFrame], validation_split: float = 0.2) -> Dict:
 """
 –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ SCHR Levels with —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π

 Args:
 data: –°–ª–æ–≤–∞—Ä—å with data for –æ–±—É—á–µ–Ω–∏—è
 validation_split: –î–æ–ª—è –¥–∞–Ω–Ω—ã—Ö for –≤–∞–ª–∏–¥–∞—Ü–∏–∏

 Returns:
 Dict: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è
 """
 try:
 self.logger.info("starting SCHR Levels model training...")

 # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
 X, y = self._prepare_training_data(data)

 if X.empty or y.empty:
 self.logger.warning("No data available for training SCHR Levels")
 return {}

 # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ on train/validation/test
 X_temp, X_test, y_temp, y_test = train_test_split(
 X, y, test_size=0.2, random_state=42, stratify=y
 )
 X_train, X_val, y_train, y_val = train_test_split(
 X_temp, y_temp, test_size=validation_split, random_state=42, stratify=y_temp
 )

 # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
 X_train_scaled = self.scaler.fit_transform(X_train)
 X_val_scaled = self.scaler.transform(X_val)
 X_test_scaled = self.scaler.transform(X_test)

 # –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
 X_train_selected = self.feature_selector.fit_transform(X_train_scaled, y_train)
 X_val_selected = self.feature_selector.transform(X_val_scaled)
 X_test_selected = self.feature_selector.transform(X_test_scaled)

 # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–º–µ–Ω –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
 self.feature_names = [f"feature_{i}" for i in range(X_train_selected.shape[1])]

 # –û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π
 model_scores = {}
 for name, model in self.models.items():
 self.logger.info(f"Training {name}...")

 # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
 model.fit(X_train_selected, y_train)

 # –í–∞–ª–∏–¥–∞—Ü–∏—è
 val_pred = model.predict(X_val_selected)
 val_accuracy = accuracy_score(y_val, val_pred)
 model_scores[name] = val_accuracy

 self.logger.info(f"{name} validation accuracy: {val_accuracy:.4f}")

 # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∞–Ω—Å–∞–º–±–ª—è
 self.ensemble_weights = self._calculate_ensemble_weights(model_scores)

 # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ on —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
 test_Predictions = self._ensemble_predict(X_test_selected)
 test_accuracy = accuracy_score(y_test, test_Predictions)

 # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
 self.training_stats = {
 'model_scores': model_scores,
 'ensemble_weights': self.ensemble_weights,
 'test_accuracy': test_accuracy,
 'n_features': X_train_selected.shape[1],
 'n_samples': len(X_train),
 'class_distribution': pd.Series(y).value_counts().to_dict()
 }

 self.is_trained = True
 self.logger.info(f"SCHR Levels training completed. Test accuracy: {test_accuracy:.4f}")

 return self.training_stats

 except Exception as e:
 self.logger.error(f"Error training SCHR Levels model: {e}")
 return {}

 def _prepare_training_data(self, data: Dict[str, pd.DataFrame]) -> Tuple[pd.DataFrame, pd.Series]:
 """
 –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö for –æ–±—É—á–µ–Ω–∏—è with —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π

 Args:
 data: –°–ª–æ–≤–∞—Ä—å with data

 Returns:
 Tuple: –ü—Ä–∏–∑–Ω–∞–∫–∏ and —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
 """
 features_list = []
 targets_list = []

 for symbol_timeframe, df in data.items():
 if df.empty or len(df) < 50:
 continue

 try:
 # create –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ SCHR Levels
 features = self._create_schr_levels_features(df)

 # create —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
 target = self._create_target(df)

 # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ and clean
 combined = pd.concat([features, target], axis=1)
 combined = combined.dropna()

 if len(combined) > 20:
 features_list.append(combined.iloc[:, :-1])
 targets_list.append(combined.iloc[:, -1])

 except Exception as e:
 self.logger.warning(f"Error processing {symbol_timeframe}: {e}")
 continue

 if features_list:
 X = pd.concat(features_list, ignore_index=True)
 y = pd.concat(targets_list, ignore_index=True)

 # remove –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
 X = self._remove_correlated_features(X)

 return X, y
 else:
 return pd.DataFrame(), pd.Series()

 def _create_schr_levels_features(self, df: pd.DataFrame) -> pd.DataFrame:
 """
 create –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ SCHR Levels

 –í–∫–ª—é—á–∞–µ—Ç:
 - –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —É—Ä–æ–≤–Ω–µ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏ and —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è
 - –ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —É—Ä–æ–≤–Ω–µ–π
 - –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
 - –û–±—ä–µ–º–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
 - –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
 """
 features = pd.DataFrame(index=df.index)

 # –ë–∞–∑–æ–≤—ã–µ —Ü–µ–Ω—ã
 features['close'] = df['Close']
 features['high'] = df['High']
 features['low'] = df['Low']
 features['open'] = df['Open']
 features['volume'] = df['Volume']

 # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —É—Ä–æ–≤–Ω–µ–π
 levels = self._detect_levels(df)
 features.update(self._calculate_level_features(df, levels))

 # –ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —É—Ä–æ–≤–Ω–µ–π
 features.update(self._calculate_cluster_features(df, levels))

 # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —É—Ä–æ–≤–Ω–µ–π
 features.update(self._calculate_level_statistics(df, levels))

 # –û–±—ä–µ–º–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
 features.update(self._calculate_volume_indicators(df))

 # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
 features.update(self._calculate_temporal_patterns(df))

 # –î–∞–≤–ª–µ–Ω–∏–µ on —É—Ä–æ–≤–Ω–∏
 features.update(self._calculate_pressure_features(df, levels))

 # –õ–∞–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
 features.update(self._calculate_lag_features(df))

 return features

 def _detect_levels(self, df: pd.DataFrame) -> Dict[str, List[float]]:
 """
 –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —É—Ä–æ–≤–Ω–µ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏ and —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è

 –ò—Å–ø–æ–ª—å–∑—É–µ—Ç:
 - –ü–∏–∫–∏ and –≤–ø–∞–¥–∏–Ω—ã for –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —É—Ä–æ–≤–Ω–µ–π
 - –ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ for –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ —Å—Ö–æ–∂–∏—Ö —É—Ä–æ–≤–Ω–µ–π
 - –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ for —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∑–Ω–∞—á–∏–º—ã—Ö —É—Ä–æ–≤–Ω–µ–π
 """
 levels = {'support': [], 'resistance': []}

 # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø–∏–∫–æ–≤ and –≤–ø–∞–¥–∏–Ω
 highs = df['High'].values
 lows = df['Low'].values

 # –ù–∞—Ö–æ–∂–¥–µ–Ω–∏–µ –ø–∏–∫–æ–≤ (—Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏–µ)
 peaks, _ = find_peaks(highs, distance=self.config['level_detection_window'])
 resistance_levels = highs[peaks]

 # –ù–∞—Ö–æ–∂–¥–µ–Ω–∏–µ –≤–ø–∞–¥–∏–Ω (–ø–æ–¥–¥–µ—Ä–∂–∫–∞)
 valleys, _ = find_peaks(-lows, distance=self.config['level_detection_window'])
 support_levels = lows[valleys]

 # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —É—Ä–æ–≤–Ω–µ–π —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è
 if len(resistance_levels) > 1:
 resistance_clusters = self._cluster_levels(resistance_levels)
 levels['resistance'] = resistance_clusters

 # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —É—Ä–æ–≤–Ω–µ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏
 if len(support_levels) > 1:
 support_clusters = self._cluster_levels(support_levels)
 levels['support'] = support_clusters

 return levels

 def _cluster_levels(self, levels: np.ndarray) -> List[float]:
 """
 –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —É—Ä–æ–≤–Ω–µ–π for –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ —Å—Ö–æ–∂–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π

 Args:
 levels: –ú–∞—Å—Å–∏–≤ —Ü–µ–Ω–æ–≤—ã—Ö —É—Ä–æ–≤–Ω–µ–π

 Returns:
 List: –¶–µ–Ω—Ç—Ä–æ–∏–¥—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
 """
 if len(levels) < 2:
 return levels.tolist()

 # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è for –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
 levels_normalized = levels.reshape(-1, 1)

 # DBSCAN –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
 clustering = DBSCAN(
 eps=self.config['cluster_eps'],
 min_samples=self.config['min_cluster_size']
 ).fit(levels_normalized)

 # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ü–µ–Ω—Ç—Ä–æ–∏–¥–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
 cluster_centers = []
 for cluster_id in set(clustering.labels_):
 if cluster_id == -1: # –®—É–º
 continue
 cluster_points = levels[clustering.labels_ == cluster_id]
 cluster_centers.append(np.mean(cluster_points))

 return cluster_centers

 def _calculate_level_features(self, df: pd.DataFrame, levels: Dict[str, List[float]]) -> Dict[str, pd.Series]:
 """–†–∞—Å—á–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ on basis –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö —É—Ä–æ–≤–Ω–µ–π"""
 features = {}

 # –ë–ª–∏–∂–∞–π—à–∏–µ —É—Ä–æ–≤–Ω–∏
 features['nearest_resistance'] = self._find_nearest_level(df['Close'], levels['resistance'])
 features['nearest_support'] = self._find_nearest_level(df['Close'], levels['support'])

 # –†–∞—Å—Å—Ç–æ—è–Ω–∏—è to —É—Ä–æ–≤–Ω–µ–π
 features['distance_to_resistance'] = (features['nearest_resistance'] - df['Close']) / df['Close']
 features['distance_to_support'] = (df['Close'] - features['nearest_support']) / df['Close']

 # –ü–æ–∑–∏—Ü–∏—è –º–µ–∂–¥—É —É—Ä–æ–≤–Ω—è–º–∏
 level_range = features['nearest_resistance'] - features['nearest_support']
 features['position_in_range'] = (df['Close'] - features['nearest_support']) / (level_range + 1e-8)

 # –°–∏–ª–∞ –±–ª–∏–∂–∞–π—à–∏—Ö —É—Ä–æ–≤–Ω–µ–π
 features['resistance_strength'] = self._calculate_level_strength(df, levels['resistance'])
 features['support_strength'] = self._calculate_level_strength(df, levels['support'])

 return features

 def _find_nearest_level(self, prices: pd.Series, levels: List[float]) -> pd.Series:
 """–ü–æ–∏—Å–∫ –±–ª–∏–∂–∞–π—à–µ–≥–æ —É—Ä–æ–≤–Ω—è for –∫–∞–∂–¥–æ–π —Ü–µ–Ω—ã"""
 if not levels:
 return pd.Series(index=prices.index, data=prices.values)

 nearest_levels = []
 for price in prices:
 distances = [abs(price - level) for level in levels]
 nearest_idx = np.argmin(distances)
 nearest_levels.append(levels[nearest_idx])

 return pd.Series(nearest_levels, index=prices.index)

 def _calculate_level_strength(self, df: pd.DataFrame, levels: List[float]) -> pd.Series:
 """–†–∞—Å—á–µ—Ç —Å–∏–ª—ã —É—Ä–æ–≤–Ω–µ–π on basis –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–∞—Å–∞–Ω–∏–π"""
 strength = pd.Series(index=df.index, data=0.0)

 for level in levels:
 # –ü–æ–∏—Å–∫ –∫–∞—Å–∞–Ω–∏–π —É—Ä–æ–≤–Ω—è
 touches = self._count_level_touches(df, level)
 strength += touches

 return strength

 def _count_level_touches(self, df: pd.DataFrame, level: float) -> pd.Series:
 """–ü–æ–¥—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–∞—Å–∞–Ω–∏–π —É—Ä–æ–≤–Ω—è"""
 tolerance = self.config['touch_tolerance']

 # check –∫–∞—Å–∞–Ω–∏–π High and Low
 high_touches = (df['High'] >= level * (1 - tolerance)) & (df['High'] <= level * (1 + tolerance))
 low_touches = (df['Low'] >= level * (1 - tolerance)) & (df['Low'] <= level * (1 + tolerance))

 touches = (high_touches | low_touches).astype(int)
 return touches.rolling(20).sum()

 def _calculate_cluster_features(self, df: pd.DataFrame, levels: Dict[str, List[float]]) -> Dict[str, pd.Series]:
 """–†–∞—Å—á–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ on basis –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —É—Ä–æ–≤–Ω–µ–π"""
 features = {}

 # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
 features['n_resistance_clusters'] = len(levels['resistance'])
 features['n_support_clusters'] = len(levels['support'])

 # –ü–ª–æ—Ç–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
 if levels['resistance']:
 resistance_std = np.std(levels['resistance'])
 features['resistance_cluster_density'] = 1.0 / (resistance_std + 1e-8)
 else:
 features['resistance_cluster_density'] = 0.0

 if levels['support']:
 support_std = np.std(levels['support'])
 features['support_cluster_density'] = 1.0 / (support_std + 1e-8)
 else:
 features['support_cluster_density'] = 0.0

 return features

 def _calculate_level_statistics(self, df: pd.DataFrame, levels: Dict[str, List[float]]) -> Dict[str, pd.Series]:
 """–†–∞—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —É—Ä–æ–≤–Ω–µ–π"""
 features = {}

 # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —É—Ä–æ–≤–Ω–µ–π —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è
 if levels['resistance']:
 resistance_levels = np.array(levels['resistance'])
 features['resistance_mean'] = np.mean(resistance_levels)
 features['resistance_std'] = np.std(resistance_levels)
 features['resistance_skewness'] = stats.skew(resistance_levels)
 features['resistance_kurtosis'] = stats.kurtosis(resistance_levels)
 else:
 features['resistance_mean'] = df['Close'].mean()
 features['resistance_std'] = 0.0
 features['resistance_skewness'] = 0.0
 features['resistance_kurtosis'] = 0.0

 # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —É—Ä–æ–≤–Ω–µ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏
 if levels['support']:
 support_levels = np.array(levels['support'])
 features['support_mean'] = np.mean(support_levels)
 features['support_std'] = np.std(support_levels)
 features['support_skewness'] = stats.skew(support_levels)
 features['support_kurtosis'] = stats.kurtosis(support_levels)
 else:
 features['support_mean'] = df['Close'].mean()
 features['support_std'] = 0.0
 features['support_skewness'] = 0.0
 features['support_kurtosis'] = 0.0

 return features

 def _calculate_volume_indicators(self, df: pd.DataFrame) -> Dict[str, pd.Series]:
 """–†–∞—Å—á–µ—Ç –æ–±—ä–µ–º–Ω—ã—Ö indicators"""
 features = {}

 # –û–±—ä–µ–º–Ω—ã–µ —Å—Ä–µ–¥–Ω–∏–µ
 for window in [5, 10, 20, 50]:
 features[f'volume_sma_{window}'] = df['Volume'].rolling(window).mean()
 features[f'volume_ratio_{window}'] = df['Volume'] / features[f'volume_sma_{window}']

 # On-Balance Volume (OBV)
 obv = pd.Series(index=df.index, dtype=float)
 obv.iloc[0] = df['Volume'].iloc[0]
 for i in range(1, len(df)):
 if df['Close'].iloc[i] > df['Close'].iloc[i-1]:
 obv.iloc[i] = obv.iloc[i-1] + df['Volume'].iloc[i]
 elif df['Close'].iloc[i] < df['Close'].iloc[i-1]:
 obv.iloc[i] = obv.iloc[i-1] - df['Volume'].iloc[i]
 else:
 obv.iloc[i] = obv.iloc[i-1]
 features['obv'] = obv

 # Volume Price Trend (VPT)
 vpt = (df['Close'].pct_change() * df['Volume']).cumsum()
 features['vpt'] = vpt

 return features

 def _calculate_temporal_patterns(self, df: pd.DataFrame) -> Dict[str, pd.Series]:
 """–†–∞—Å—á–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
 features = {}

 # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
 features['hour'] = df.index.hour
 features['day_of_week'] = df.index.dayofweek
 features['day_of_month'] = df.index.day
 features['month'] = df.index.month

 # –¶–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
 features['hour_sin'] = np.sin(2 * np.pi * features['hour'] / 24)
 features['hour_cos'] = np.cos(2 * np.pi * features['hour'] / 24)
 features['day_sin'] = np.sin(2 * np.pi * features['day_of_week'] / 7)
 features['day_cos'] = np.cos(2 * np.pi * features['day_of_week'] / 7)

 return features

 def _calculate_pressure_features(self, df: pd.DataFrame, levels: Dict[str, List[float]]) -> Dict[str, pd.Series]:
 """–†–∞—Å—á–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–∞–≤–ª–µ–Ω–∏—è on —É—Ä–æ–≤–Ω–∏"""
 features = {}

 # –î–∞–≤–ª–µ–Ω–∏–µ on –±–ª–∏–∂–∞–π—à–∏–µ —É—Ä–æ–≤–Ω–∏
 nearest_resistance = self._find_nearest_level(df['Close'], levels['resistance'])
 nearest_support = self._find_nearest_level(df['Close'], levels['support'])

 # –î–∞–≤–ª–µ–Ω–∏–µ on —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏–µ
 resistance_pressure = (df['Close'] - nearest_resistance) * df['Volume']
 features['resistance_pressure'] = resistance_pressure.rolling(20).mean()

 # –î–∞–≤–ª–µ–Ω–∏–µ on –ø–æ–¥–¥–µ—Ä–∂–∫—É
 support_pressure = (nearest_support - df['Close']) * df['Volume']
 features['support_pressure'] = support_pressure.rolling(20).mean()

 # –í–µ–∫—Ç–æ—Ä –¥–∞–≤–ª–µ–Ω–∏—è
 features['pressure_vector'] = features['resistance_pressure'] - features['support_pressure']

 return features

 def _calculate_lag_features(self, df: pd.DataFrame) -> Dict[str, pd.Series]:
 """–†–∞—Å—á–µ—Ç –ª–∞–≥–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
 features = {}

 for lag in [1, 2, 3, 5, 10, 20]:
 features[f'close_lag_{lag}'] = df['Close'].shift(lag)
 features[f'high_lag_{lag}'] = df['High'].shift(lag)
 features[f'low_lag_{lag}'] = df['Low'].shift(lag)
 features[f'volume_lag_{lag}'] = df['Volume'].shift(lag)

 # –û—Ç–Ω–æ—à–µ–Ω–∏—è with –ª–∞–≥–∞–º–∏
 features[f'close_ratio_lag_{lag}'] = df['Close'] / features[f'close_lag_{lag}']
 features[f'volume_ratio_lag_{lag}'] = df['Volume'] / features[f'volume_lag_{lag}']

 return features

 def _create_target(self, df: pd.DataFrame, horizon: int = None) -> pd.Series:
 """create —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π for SCHR Levels"""
 if horizon is None:
 horizon = self.config['target_horizon']

 future_price = df['Close'].shift(-horizon)
 current_price = df['Close']

 # –ü—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ
 price_change = (future_price - current_price) / current_price

 # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ on basis –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
 volatility = df['Close'].rolling(20).std() / df['Close'].rolling(20).mean()
 threshold = volatility * 0.5

 # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è with –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –ø–æ—Ä–æ–≥–∞–º–∏
 target = pd.Series(index=df.index, dtype=int)
 target[price_change > threshold] = 2 # Up
 target[price_change < -threshold] = 0 # Down
 target[(price_change >= -threshold) & (price_change <= threshold)] = 1 # Hold

 return target

 def _remove_correlated_features(self, X: pd.DataFrame, threshold: float = 0.95) -> pd.DataFrame:
 """remove –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
 corr_matrix = X.corr().abs()
 upper_tri = corr_matrix.where(
 np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
 )
 to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > threshold)]
 return X.drop(columns=to_drop)

 def _calculate_ensemble_weights(self, model_scores: Dict[str, float]) -> Dict[str, float]:
 """–†–∞—Å—á–µ—Ç –≤–µ—Å–æ–≤ for –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π"""
 total_score = sum(model_scores.values())
 if total_score == 0:
 return {name: 1.0 / len(model_scores) for name in model_scores.keys()}
 return {name: score / total_score for name, score in model_scores.items()}

 def _ensemble_predict(self, X: np.ndarray) -> np.ndarray:
 """Prediction –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π"""
 Predictions = []
 for name, model in self.models.items():
 pred = model.predict(X)
 weight = self.ensemble_weights.get(name, 0)
 Predictions.append(pred * weight)
 ensemble_pred = np.sum(Predictions, axis=0)
 return np.round(ensemble_pred).astype(int)

 def predict(self, data: pd.DataFrame) -> np.ndarray:
 """Prediction on basis SCHR Levels"""
 if not self.is_trained:
 self.logger.warning("SCHR Levels model not trained")
 return np.zeros(len(data))

 try:
 # create –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
 features = self._create_schr_levels_features(data)

 # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
 features_scaled = self.scaler.transform(features)

 # –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
 features_selected = self.feature_selector.transform(features_scaled)

 # Prediction
 Prediction = self._ensemble_predict(features_selected)

 return Prediction

 except Exception as e:
 self.logger.error(f"Error predicting with SCHR Levels: {e}")
 return np.zeros(len(data))

 def get_levels(self, data: pd.DataFrame) -> Dict[str, List[float]]:
 """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö —É—Ä–æ–≤–Ω–µ–π"""
 return self._detect_levels(data)

 def get_training_stats(self) -> Dict:
 """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è"""
 return self.training_stats.copy()

# example –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
 # create —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
 np.random.seed(42)
 dates = pd.date_range('2020-01-01', periods=1000, freq='H')

 # –°–∏–º—É–ª—è—Ü–∏—è —Ü–µ–Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö with —É—Ä–æ–≤–Ω—è–º–∏
 price = 100
 prices = []
 for i in range(1000):
 # create –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã—Ö —É—Ä–æ–≤–Ω–µ–π
 if i % 100 < 20: # –£—Ä–æ–≤–µ–Ω—å —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è
 change = np.random.normal(0, 0.005)
 elif i % 100 > 80: # –£—Ä–æ–≤–µ–Ω—å –ø–æ–¥–¥–µ—Ä–∂–∫–∏
 change = np.random.normal(0, 0.005)
 else:
 change = np.random.normal(0, 0.01)

 price *= (1 + change)
 prices.append(price)

 data = pd.DataFrame({
 'Open': prices,
 'High': [p * (1 + abs(np.random.normal(0, 0.005))) for p in prices],
 'Low': [p * (1 - abs(np.random.normal(0, 0.005))) for p in prices],
 'Close': prices,
 'Volume': np.random.randint(1000, 10000, 1000)
 }, index=dates)

 # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è OHLC
 for i in range(len(data)):
 high = max(data.iloc[i]['Open'], data.iloc[i]['Close'])
 low = min(data.iloc[i]['Open'], data.iloc[i]['Close'])
 data.iloc[i, data.columns.get_loc('High')] = high
 data.iloc[i, data.columns.get_loc('Low')] = low

 # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞
 schr_levels = SCHRLevelsIndicator()

 # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö for –æ–±—É—á–µ–Ω–∏—è
 training_data = {'test_H1': data}

 # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
 stats = schr_levels.train(training_data)
 print(f"Training completed. Test accuracy: {stats.get('test_accuracy', 0):.4f}")

 # Prediction
 Predictions = schr_levels.predict(data.tail(100))
 print(f"Predictions: {np.bincount(Predictions)}")

 # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —É—Ä–æ–≤–Ω–µ–π
 levels = schr_levels.get_levels(data)
 print(f"Detected resistance levels: {len(levels['resistance'])}")
 print(f"Detected support levels: {len(levels['support'])}")
```

## ‚ö° –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä SCHR SHORT3

**–¢–µ–æ—Ä–∏—è:** –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä SCHR SHORT3 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π ML-–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä for –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–π —Ç–æ—Ä–≥–æ–≤–ª–∏ and —Å–∫–∞–ª—å–ø–∏–Ω–≥–∞, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—â–∏–π –≤—ã—Å–æ–∫–æ—á–∞—Å—Ç–æ—Ç–Ω—ã–µ —Ç–æ—Ä–≥–æ–≤—ã–µ —Å–∏–≥–Ω–∞–ª—ã with –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é. –≠—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç for –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏–±—ã–ª–∏.

**–ü–æ—á–µ–º—É –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä SCHR SHORT3 –≤–∞–∂–µ–Ω:**
- **–ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ—Å—Ç—å:** –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã–µ —Ç–æ—Ä–≥–æ–≤—ã–µ —Å–∏–≥–Ω–∞–ª—ã
- **–°–∫–∞–ª—å–ø–∏–Ω–≥:** –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ for —Å–∫–∞–ª—å–ø–∏–Ω–≥–∞
- **–ß–∞—Å—Ç–æ—Ç–∞:** –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤—ã—Å–æ–∫—É—é —á–∞—Å—Ç–æ—Ç—É —Å–∏–≥–Ω–∞–ª–æ–≤
- **–ü—Ä–∏–±—ã–ª—å–Ω–æ—Å—Ç—å:** –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ for –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏–±—ã–ª–∏

**–ü–ª—é—Å—ã:**
- –ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã
- –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å–∫–∞–ª—å–ø–∏–Ω–≥–∞
- –í—ã—Å–æ–∫–∞—è —á–∞—Å—Ç–æ—Ç–∞ —Å–∏–≥–Ω–∞–ª–æ–≤
- –ú–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–∏–±—ã–ª–∏

**–ú–∏–Ω—É—Å—ã:**
- –í—ã—Å–æ–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Å–∫–æ—Ä–æ—Å—Ç–∏
- –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –≤—ã—Å–æ–∫–∏–µ –∫–æ–º–∏—Å—Å–∏–∏
- –¢—Ä–µ–±—É–µ—Ç –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–≥–æ Monitoring–∞

```python
# src/indicators/schr_short3.py
import pandas as pd
import numpy as np
from typing import Dict, List, Optional
import logging
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

class SCHRShort3Indicator:
 """–ò–Ω–¥–∏–∫–∞—Ç–æ—Ä SCHR SHORT3 for –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–π —Ç–æ—Ä–≥–æ–≤–ª–∏"""

 def __init__(self):
 self.logger = logging.getLogger(__name__)
 self.model = ExtraTreesClassifier(n_estimators=100, random_state=42)
 self.features = []
 self.is_trained = False

 def train(self, data: Dict[str, pd.DataFrame]):
 """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ SCHR SHORT3"""
 try:
 self.logger.info("Training SCHR SHORT3 model...")

 # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
 X, y = self._prepare_training_data(data)

 if X.empty or y.empty:
 self.logger.warning("No data available for training SCHR SHORT3")
 return

 # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ on train/test
 X_train, X_test, y_train, y_test = train_test_split(
 X, y, test_size=0.2, random_state=42
 )

 # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
 self.model.fit(X_train, y_train)

 # –û—Ü–µ–Ω–∫–∞
 y_pred = self.model.predict(X_test)
 accuracy = accuracy_score(y_test, y_pred)

 self.is_trained = True
 self.logger.info(f"SCHR SHORT3 model trained with accuracy: {accuracy:.4f}")

 except Exception as e:
 self.logger.error(f"Error training SCHR SHORT3 model: {e}")

 def _prepare_training_data(self, data: Dict[str, pd.DataFrame]) -> tuple:
 """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö for –æ–±—É—á–µ–Ω–∏—è"""
 features_list = []
 targets_list = []

 for symbol_timeframe, df in data.items():
 if df.empty:
 continue

 # create –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ SCHR SHORT3
 features = self._create_schr_short3_features(df)

 # create —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
 target = self._create_target(df)

 # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
 combined = pd.concat([features, target], axis=1)
 combined = combined.dropna()

 if not combined.empty:
 features_list.append(combined.iloc[:, :-1])
 targets_list.append(combined.iloc[:, -1])

 if features_list:
 X = pd.concat(features_list, ignore_index=True)
 y = pd.concat(targets_list, ignore_index=True)
 return X, y
 else:
 return pd.DataFrame(), pd.Series()

 def _create_schr_short3_features(self, df: pd.DataFrame) -> pd.DataFrame:
 """create –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ SCHR SHORT3"""
 features = pd.DataFrame(index=df.index)

 # –ë–∞–∑–æ–≤—ã–µ —Ü–µ–Ω—ã
 features['close'] = df['Close']
 features['high'] = df['High']
 features['low'] = df['Low']
 features['volume'] = df['Volume']

 # SCHR SHORT3 –ø—Ä–∏–∑–Ω–∞–∫–∏
 features['short_term_signal'] = self._calculate_short_term_signal(df)
 features['short_term_strength'] = self._calculate_short_term_strength(df)
 features['short_term_direction'] = self._calculate_short_term_direction(df)
 features['short_term_volatility'] = self._calculate_short_term_volatility(df)
 features['short_term_momentum'] = self._calculate_short_term_momentum(df)

 # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã
 features['short_buy_signal'] = (features['short_term_signal'] > 0.5).astype(int)
 features['short_sell_signal'] = (features['short_term_signal'] < -0.5).astype(int)
 features['short_hold_signal'] = (abs(features['short_term_signal']) <= 0.5).astype(int)

 # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
 features['short_hits'] = self._calculate_short_hits(df)
 features['short_breaks'] = self._calculate_short_breaks(df)
 features['short_bounces'] = self._calculate_short_bounces(df)
 features['short_accuracy'] = self._calculate_short_accuracy(df)

 # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
 features['short_volatility_normalized'] = features['short_term_volatility'] / features['close']
 features['short_momentum_normalized'] = features['short_term_momentum'] / features['close']

 # –õ–∞–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
 for lag in [1, 2, 3, 5, 10]:
 features[f'short_signal_lag_{lag}'] = features['short_term_signal'].shift(lag)
 features[f'short_strength_lag_{lag}'] = features['short_term_strength'].shift(lag)

 return features

 def _calculate_short_term_signal(self, df: pd.DataFrame) -> pd.Series:
 """–†–∞—Å—á–µ—Ç –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞"""
 # –ö–æ–º–±–∏–Ω–∞—Ü–∏—è RSI and MACD for –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤
 rsi = self._calculate_rsi(df['Close'])
 macd = self._calculate_macd(df['Close'])

 # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
 rsi_norm = (rsi - 50) / 50
 macd_norm = macd / df['Close']

 # –ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã–π —Å–∏–≥–Ω–∞–ª
 signal = (rsi_norm + macd_norm) / 2
 return signal.rolling(5).mean()

 def _calculate_short_term_strength(self, df: pd.DataFrame) -> pd.Series:
 """–†–∞—Å—á–µ—Ç —Å–∏–ª—ã –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞"""
 volatility = df['Close'].rolling(20).std()
 volume = df['Volume'].rolling(20).mean()

 # –°–∏–ª–∞ = –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å * –æ–±—ä–µ–º
 strength = volatility * volume
 return strength / strength.rolling(50).max()

 def _calculate_short_term_direction(self, df: pd.DataFrame) -> pd.Series:
 """–†–∞—Å—á–µ—Ç –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞"""
 price_change = df['Close'].pct_change(5)
 return np.sign(price_change)

 def _calculate_short_term_volatility(self, df: pd.DataFrame) -> pd.Series:
 """–†–∞—Å—á–µ—Ç –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–π –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏"""
 return df['Close'].rolling(10).std()

 def _calculate_short_term_momentum(self, df: pd.DataFrame) -> pd.Series:
 """–†–∞—Å—á–µ—Ç –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–≥–æ –º–æ–º–µ–Ω—Ç—É–º–∞"""
 return df['Close'].pct_change(3)

 def _calculate_short_hits(self, df: pd.DataFrame) -> pd.Series:
 """–†–∞—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã—Ö –∫–∞—Å–∞–Ω–∏–π"""
 high_20 = df['High'].rolling(20).max()
 low_20 = df['Low'].rolling(20).min()

 hits = ((df['Close'] >= high_20 * 0.99) | (df['Close'] <= low_20 * 1.01)).astype(int)
 return hits.rolling(20).sum()

 def _calculate_short_breaks(self, df: pd.DataFrame) -> pd.Series:
 """–†–∞—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã—Ö –ø—Ä–æ–±–æ–µ–≤"""
 high_20 = df['High'].rolling(20).max()
 low_20 = df['Low'].rolling(20).min()

 breaks = ((df['Close'] > high_20) | (df['Close'] < low_20)).astype(int)
 return breaks.rolling(20).sum()

 def _calculate_short_bounces(self, df: pd.DataFrame) -> pd.Series:
 """–†–∞—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã—Ö –æ—Ç—Å–∫–æ–∫–æ–≤"""
 price_change = df['Close'].pct_change()
 bounces = ((price_change > 0.01) | (price_change < -0.01)).astype(int)
 return bounces.rolling(20).sum()

 def _calculate_short_accuracy(self, df: pd.DataFrame) -> pd.Series:
 """–†–∞—Å—á–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤"""
 # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏
 price_change = df['Close'].pct_change()
 correct_Predictions = (abs(price_change) > 0.005).astype(int)
 return correct_Predictions.rolling(20).mean()

 def _calculate_rsi(self, prices: pd.Series, window: int = 14) -> pd.Series:
 """–†–∞—Å—á–µ—Ç RSI"""
 delta = prices.diff()
 gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
 loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
 rs = gain / loss
 rsi = 100 - (100 / (1 + rs))
 return rsi

 def _calculate_macd(self, prices: pd.Series, fast: int = 12, slow: int = 26, signal: int = 9) -> pd.Series:
 """–†–∞—Å—á–µ—Ç MACD"""
 ema_fast = prices.ewm(span=fast).mean()
 ema_slow = prices.ewm(span=slow).mean()
 macd = ema_fast - ema_slow
 signal_line = macd.ewm(span=signal).mean()
 return macd - signal_line

 def _create_target(self, df: pd.DataFrame, horizon: int = 1) -> pd.Series:
 """create —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π"""
 future_price = df['Close'].shift(-horizon)
 current_price = df['Close']

 # –ü—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ
 price_change = (future_price - current_price) / current_price

 # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
 target = pd.cut(
 price_change,
 bins=[-np.inf, -0.001, 0.001, np.inf],
 labels=[0, 1, 2], # 0=down, 1=hold, 2=up
 include_lowest=True
 )

 return target.astype(int)

 def predict(self, data: pd.DataFrame) -> np.ndarray:
 """Prediction on basis SCHR SHORT3"""
 if not self.is_trained:
 self.logger.warning("SCHR SHORT3 model not trained")
 return np.zeros(len(data))

 try:
 # create –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
 features = self._create_schr_short3_features(data)

 # Prediction
 Prediction = self.model.predict(features)

 return Prediction

 except Exception as e:
 self.logger.error(f"Error predicting with SCHR SHORT3: {e}")
 return np.zeros(len(data))

 def get_features(self) -> pd.DataFrame:
 """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
 return self.features
```

**–¢–µ–æ—Ä–∏—è:** –ó–∞–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–∞—è —á–∞—Å—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π description —Å—Ç—Ä—É–∫—Ç—É—Ä—ã and –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Å–∏—Å—Ç–µ–º—ã. –≠—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ for –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø–æ–ª–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã and –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è —Å–∏—Å—Ç–µ–º—ã.

**–ü–æ—á–µ–º—É –∑–∞–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–∞—è —á–∞—Å—Ç—å –≤–∞–∂–Ω–∞:**
- **–°—Ç—Ä—É–∫—Ç—É—Ä–∞:** –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
- **–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ:** –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
- **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:** –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
- **–†–∞–∑–≤–∏—Ç–∏–µ:** –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ for –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è

**–ü–ª—é—Å—ã:**
- –ü–æ–Ω–∏–º–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
- Plan –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è
- –ü–æ–ª–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
- –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ä–∞–∑–≤–∏—Ç–∏—è

**–ú–∏–Ω—É—Å—ã:**
- –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –Ω–µ–ø–æ–ª–Ω–æ—Ç–∞
- –¢—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏

–≠—Ç–æ –≤—Ç–æ—Ä–∞—è —á–∞—Å—Ç—å –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∫–æ–¥–∞. –ü—Ä–æ–¥–æ–ª–∂—É with –æ—Å—Ç–∞–ª—å–Ω—ã–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏ in —Å–ª–µ–¥—É—é—â–∏—Ö —á–∞—Å—Ç—è—Ö.
