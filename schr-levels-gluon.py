# -*- coding: utf-8 -*-
"""
SCHR Levels AutoML Pipeline
–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è ML-–º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ SCHR Levels –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤

–†–µ—à–∞–µ—Ç 3 –æ—Å–Ω–æ–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏:
1. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∑–Ω–∞–∫–∞ PRESSURE_VECTOR (+ –∏–ª–∏ -)
2. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ü–µ–Ω—ã –Ω–∞ 5 –ø–µ—Ä–∏–æ–¥–æ–≤ (–≤–≤–µ—Ä—Ö/–≤–Ω–∏–∑/hold)  
3. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø—Ä–æ–±–∏—Ç–∏—è PREDICTED_HIGH/PREDICTED_LOW –∏–ª–∏ —É–¥–µ—Ä–∂–∞–Ω–∏—è –º–µ–∂–¥—É –Ω–∏–º–∏

–ê–≤—Ç–æ—Ä: NeoZork HLDP
–í–µ—Ä—Å–∏—è: 1.0
"""

import pandas as pd
import numpy as np
from typing import Dict, Any, List, Optional, Tuple
import logging
from pathlib import Path
import warnings
from datetime import datetime, timedelta
import joblib
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import TimeSeriesSplit
import matplotlib.pyplot as plt
import seaborn as sns

# Disable CUDA for MacBook M1 and set OpenMP paths
import os
os.environ["CUDA_VISIBLE_DEVICES"] = ""
os.environ["AUTOGLUON_USE_GPU"] = "false"
os.environ["AUTOGLUON_USE_GPU_TORCH"] = "false"
os.environ["AUTOGLUON_USE_GPU_FASTAI"] = "false"

# Set OpenMP paths for macOS
os.environ["LDFLAGS"] = "-L/opt/homebrew/opt/libomp/lib"
os.environ["CPPFLAGS"] = "-I/opt/homebrew/opt/libomp/include"

# AutoGluon imports
try:
    from autogluon.tabular import TabularPredictor
    AUTOGLUON_AVAILABLE = True
except ImportError:
    AUTOGLUON_AVAILABLE = False
    TabularPredictor = None

warnings.filterwarnings('ignore')
logger = logging.getLogger(__name__)

# Ray import check
try:
    import ray
    RAY_AVAILABLE = True
    logger.info("Ray –¥–æ—Å—Ç—É–ø–µ–Ω - –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ")
except ImportError:
    RAY_AVAILABLE = False
    logger.warning("Ray –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω - –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ")
    logger.info("–î–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ ray –≤—ã–ø–æ–ª–Ω–∏—Ç–µ: pip install 'ray>=2.10.0,<2.45.0'")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/schr_levels_automl.log'),
        logging.StreamHandler()
    ]
)


class SCHRLevelsAutoMLPipeline:
    """
    –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è ML-–º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ SCHR Levels –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤.
    
    –†–µ—à–∞–µ—Ç 3 –æ—Å–Ω–æ–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏:
    1. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∑–Ω–∞–∫–∞ PRESSURE_VECTOR (+ –∏–ª–∏ -)
    2. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ü–µ–Ω—ã –Ω–∞ 5 –ø–µ—Ä–∏–æ–¥–æ–≤ (–≤–≤–µ—Ä—Ö/–≤–Ω–∏–∑/hold)  
    3. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø—Ä–æ–±–∏—Ç–∏—è PREDICTED_HIGH/PREDICTED_LOW –∏–ª–∏ —É–¥–µ—Ä–∂–∞–Ω–∏—è –º–µ–∂–¥—É –Ω–∏–º–∏
    """
    
    def __init__(self, data_path: str = "data/cache/csv_converted/"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞.
        
        Args:
            data_path: –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –¥–∞–Ω–Ω—ã–º–∏
        """
        if not AUTOGLUON_AVAILABLE:
            raise ImportError("AutoGluon –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install autogluon")
        
        self.data_path = Path(data_path)
        self.models = {}
        self.results = {}
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á
        self.task_configs = {
            'pressure_vector_sign': {
                'problem_type': 'binary',
                'eval_metric': 'roc_auc',
                'time_limit': 1800  # 30 –º–∏–Ω—É—Ç
            },
            'price_direction_1period': {
                'problem_type': 'multiclass', 
                'eval_metric': 'accuracy',
                'time_limit': 1800  # 30 –º–∏–Ω—É—Ç
            },
            'level_breakout': {
                'problem_type': 'multiclass',
                'eval_metric': 'accuracy', 
                'time_limit': 2400  # 40 –º–∏–Ω—É—Ç
            }
        }
        
        logger.info("SCHR Levels AutoML Pipeline –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        # –ò–Ω—Ñ–æ—Ä–º–∏—Ä—É–µ–º –æ —Ä–µ–∂–∏–º–µ –æ–±—É—á–µ–Ω–∏—è
        if RAY_AVAILABLE:
            logger.info("‚úÖ Ray –¥–æ—Å—Ç—É–ø–µ–Ω - –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ")
        else:
            logger.warning("‚ö†Ô∏è  Ray –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ")
            logger.info("üí° –î–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ ray: pip install 'ray>=2.10.0,<2.45.0'")
    
    def load_schr_data(self, symbol: str = "BTCUSD", timeframe: str = "MN1") -> pd.DataFrame:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö SCHR Levels –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ –∏ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞.
        
        Args:
            symbol: –¢–æ—Ä–≥–æ–≤—ã–π —Å–∏–º–≤–æ–ª (BTCUSD, EURUSD, etc.)
            timeframe: –¢–∞–π–º—Ñ—Ä–µ–π–º (MN1, W1, D1, H4, H1, M15, M5, M1)
            
        Returns:
            DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ SCHR Levels
        """
        filename = f"CSVExport_{symbol}_PERIOD_{timeframe}.parquet"
        file_path = self.data_path / filename
        
        if not file_path.exists():
            raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
        
        logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ: {filename}")
        df = pd.read_parquet(file_path)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        required_cols = ['Close', 'High', 'Open', 'Low', 'Volume', 'predicted_low', 'predicted_high', 'pressure', 'pressure_vector']
        missing_cols = [col for col in required_cols if col not in df.columns]
        
        if missing_cols:
            logger.warning(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏: {missing_cols}")
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å –∫–∞–∫ datetime –µ—Å–ª–∏ –µ—Å—Ç—å
        if 'Date' in df.columns:
            df['Date'] = pd.to_datetime(df['Date'])
            df.set_index('Date', inplace=True)
        elif df.index.name != 'Date' and not isinstance(df.index, pd.DatetimeIndex):
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω–¥–µ–∫—Å –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
            df.index = pd.date_range(start='2020-01-01', periods=len(df), freq='MS' if timeframe == 'MN1' else 'D')
        
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π —Å {len(df.columns)} –∫–æ–ª–æ–Ω–∫–∞–º–∏")
        return df
    
    def create_target_variables(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–ª—è –≤—Å–µ—Ö 3 –∑–∞–¥–∞—á.
        
        Args:
            df: –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ SCHR Levels
            
        Returns:
            DataFrame —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ —Ü–µ–ª–µ–≤—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏
        """
        logger.info("–°–æ–∑–¥–∞–µ–º —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è 3 –∑–∞–¥–∞—á...")
        
        data = df.copy()
        
        # –ó–∞–¥–∞—á–∞ 1: –ó–Ω–∞–∫ PRESSURE_VECTOR –≤ —Å–ª–µ–¥—É—é—â–µ–º –ø–µ—Ä–∏–æ–¥–µ
        if 'pressure_vector' in data.columns:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º NaN –∏ inf –∑–Ω–∞—á–µ–Ω–∏—è
            pv_clean = data['pressure_vector'].replace([np.inf, -np.inf], np.nan)
            pv_sign = (pv_clean.shift(-1) > 0)
            data['target_pv_sign'] = pv_sign.astype(float)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º float –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            logger.info("‚úÖ –°–æ–∑–¥–∞–Ω–∞ target_pv_sign (0=–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π, 1=–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π)")
        
        # –ó–∞–¥–∞—á–∞ 2: –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ü–µ–Ω—ã –Ω–∞ 1 –ø–µ—Ä–∏–æ–¥
        if 'Close' in data.columns:
            future_returns = data['Close'].pct_change(1).shift(-1)
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è
            future_returns_clean = future_returns.replace([np.inf, -np.inf], np.nan)
            price_direction = pd.cut(
                future_returns_clean, 
                bins=[-np.inf, -0.01, 0.01, np.inf], 
                labels=[0, 1, 2]  # 0=down, 1=hold, 2=up
            )
            data['target_price_direction'] = price_direction.astype(float)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º float –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            logger.info("‚úÖ –°–æ–∑–¥–∞–Ω–∞ target_price_direction (0=–≤–Ω–∏–∑, 1=—É–¥–µ—Ä–∂–∞–Ω–∏–µ, 2=–≤–≤–µ—Ä—Ö) –Ω–∞ 1 –ø–µ—Ä–∏–æ–¥")
        
        # –ó–∞–¥–∞—á–∞ 3: –ü—Ä–æ–±–∏—Ç–∏–µ —É—Ä–æ–≤–Ω–µ–π –∏–ª–∏ —É–¥–µ—Ä–∂–∞–Ω–∏–µ –º–µ–∂–¥—É –Ω–∏–º–∏
        if all(col in data.columns for col in ['Close', 'predicted_high', 'predicted_low']):
            close_next = data['Close'].shift(-1)
            pred_high = data['predicted_high'].replace([np.inf, -np.inf], np.nan)
            pred_low = data['predicted_low'].replace([np.inf, -np.inf], np.nan)
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–ª—É—á–∞–∏ —Å NaN –≤ —É—Ä–æ–≤–Ω—è—Ö
            valid_levels = ~(pred_high.isna() | pred_low.isna() | close_next.isna())
            
            conditions = [
                (close_next > pred_high) & valid_levels,  # –ü—Ä–æ–±–∏—Ç–∏–µ –≤–≤–µ—Ä—Ö
                (close_next < pred_low) & valid_levels,   # –ü—Ä–æ–±–∏—Ç–∏–µ –≤–Ω–∏–∑
                (close_next >= pred_low) & (close_next <= pred_high) & valid_levels  # –ú–µ–∂–¥—É —É—Ä–æ–≤–Ω—è–º–∏
            ]
            choices = [2, 0, 1]  # 2=–ø—Ä–æ–±–∏—Ç–∏–µ –≤–≤–µ—Ä—Ö, 0=–ø—Ä–æ–±–∏—Ç–∏–µ –≤–Ω–∏–∑, 1=–º–µ–∂–¥—É —É—Ä–æ–≤–Ω—è–º–∏
            
            data['target_level_breakout'] = np.select(conditions, choices, default=1).astype(float)
            logger.info("‚úÖ –°–æ–∑–¥–∞–Ω–∞ target_level_breakout (0=–ø—Ä–æ–±–∏—Ç–∏–µ –≤–Ω–∏–∑, 1=–º–µ–∂–¥—É —É—Ä–æ–≤–Ω—è–º–∏, 2=–ø—Ä–æ–±–∏—Ç–∏–µ –≤–≤–µ—Ä—Ö)")
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN –≤ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        target_cols = [col for col in data.columns if col.startswith('target_')]
        data = data.dropna(subset=target_cols)
        
        logger.info(f"–ü–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö: {len(data)} –∑–∞–ø–∏—Å–µ–π")
        return data
    
    def create_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏.
        
        Args:
            df: –î–∞–Ω–Ω—ã–µ —Å —Ü–µ–ª–µ–≤—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏
            
        Returns:
            DataFrame —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        """
        logger.info("–°–æ–∑–¥–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏...")
        
        data = df.copy()
        
        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ü–µ–Ω—ã
        if 'Close' in data.columns:
            # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ
            for window in [5, 10, 20]:
                data[f'sma_{window}'] = data['Close'].rolling(window).mean()
                data[f'close_sma_{window}_ratio'] = data['Close'] / data[f'sma_{window}']
            
            # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
            data['volatility_5'] = data['Close'].pct_change().rolling(5).std()
            data['volatility_20'] = data['Close'].pct_change().rolling(20).std()
            
            # RSI —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π
            delta = data['Close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
            rs = gain / loss
            data['rsi'] = 100 - (100 / (1 + rs))
        
        # –ü—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ SCHR —É—Ä–æ–≤–Ω–µ–π
        if all(col in data.columns for col in ['Close', 'predicted_high', 'predicted_low']):
            # –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ —É—Ä–æ–≤–Ω–µ–π
            data['distance_to_high'] = (data['predicted_high'] - data['Close']) / data['Close']
            data['distance_to_low'] = (data['Close'] - data['predicted_low']) / data['Close']
            data['levels_spread'] = (data['predicted_high'] - data['predicted_low']) / data['Close']
            
            # –ü–æ–∑–∏—Ü–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —É—Ä–æ–≤–Ω–µ–π (0-1, –≥–¥–µ 0.5 = –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ)
            data['position_in_levels'] = (data['Close'] - data['predicted_low']) / (data['predicted_high'] - data['predicted_low'])
        
        # –ü—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–≤–ª–µ–Ω–∏—è
        if 'pressure' in data.columns:
            # –õ–∞–≥–∏ –¥–∞–≤–ª–µ–Ω–∏—è
            for lag in [1, 2, 3]:
                data[f'pressure_lag_{lag}'] = data['pressure'].shift(lag)
            
            # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ –¥–∞–≤–ª–µ–Ω–∏—è
            for window in [3, 5, 10]:
                data[f'pressure_sma_{window}'] = data['pressure'].rolling(window).mean()
        
        if 'pressure_vector' in data.columns:
            # –õ–∞–≥–∏ –≤–µ–∫—Ç–æ—Ä–∞ –¥–∞–≤–ª–µ–Ω–∏—è
            for lag in [1, 2, 3]:
                data[f'pv_lag_{lag}'] = data['pressure_vector'].shift(lag)
            
            # –ò–∑–º–µ–Ω–µ–Ω–∏–µ –∑–Ω–∞–∫–∞ –≤–µ–∫—Ç–æ—Ä–∞ –¥–∞–≤–ª–µ–Ω–∏—è
            data['pv_sign_change'] = (data['pressure_vector'] * data['pressure_vector'].shift(1) < 0).astype(int)
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –µ—Å–ª–∏ –µ—Å—Ç—å datetime –∏–Ω–¥–µ–∫—Å
        if isinstance(data.index, pd.DatetimeIndex):
            data['month'] = data.index.month
            data['quarter'] = data.index.quarter
            data['year'] = data.index.year
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        data = data.replace([np.inf, -np.inf], np.nan)
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ —É–¥–∞–ª–µ–Ω–∏—è
        # –î–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –∑–∞–ø–æ–ª–Ω—è–µ–º –º–µ–¥–∏–∞–Ω–æ–π
        numeric_cols = data.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            if data[col].isna().any():
                data[col] = data[col].fillna(data[col].median())
        
        # –£–¥–∞–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏ –≥–¥–µ –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è NaN
        data = data.dropna(how='all')
        
        # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –µ—Å—Ç—å NaN, –∑–∞–ø–æ–ª–Ω—è–µ–º 0
        data = data.fillna(0)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        if np.isinf(data.select_dtypes(include=[np.number])).any().any():
            logger.warning("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ 0")
            data = data.replace([np.inf, -np.inf], 0)
        
        logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(data.columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, {len(data)} –∑–∞–ø–∏—Å–µ–π")
        return data
    
    def prepare_data_for_task(self, df: pd.DataFrame, task: str) -> Tuple[pd.DataFrame, str]:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–∏.
        
        Args:
            df: –î–∞–Ω–Ω—ã–µ —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –∏ —Ü–µ–ª–µ–≤—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏
            task: –ù–∞–∑–≤–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏
            
        Returns:
            Tuple[DataFrame, target_column]
        """
        target_mapping = {
            'pressure_vector_sign': 'target_pv_sign',
            'price_direction_1period': 'target_price_direction', 
            'level_breakout': 'target_level_breakout'
        }
        
        target_col = target_mapping[task]
        
        if target_col not in df.columns:
            raise ValueError(f"–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è {target_col} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        
        # –£–¥–∞–ª—è–µ–º –¥—Ä—É–≥–∏–µ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        other_targets = [col for col in target_mapping.values() if col != target_col]
        data = df.drop(columns=other_targets, errors='ignore')
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ –≥–¥–µ —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è NaN
        data = data.dropna(subset=[target_col])
        
        logger.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∑–∞–¥–∞—á–∏ {task}: {len(data)} –∑–∞–ø–∏—Å–µ–π")
        return data, target_col
    
    def train_model(self, df: pd.DataFrame, task: str, test_size: float = 0.2) -> Dict[str, Any]:
        """
        –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ AutoGluon –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–∏.
        
        Args:
            df: –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            task: –ù–∞–∑–≤–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏
            test_size: –î–æ–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è
        """
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∑–∞–¥–∞—á–∏: {task}")
        
        data, target_col = self.prepare_data_for_task(df, task)
        config = self.task_configs[task]
        
        # –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö (–≤–∞–∂–Ω–æ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤)
        split_idx = int(len(data) * (1 - test_size))
        train_data = data.iloc[:split_idx]
        test_data = data.iloc[split_idx:]
        
        logger.info(f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(train_data)} –∑–∞–ø–∏—Å–µ–π")
        logger.info(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(test_data)} –∑–∞–ø–∏—Å–µ–π")
        
        # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å –¥–ª—è –º–æ–¥–µ–ª–∏
        model_path = f"models/schr_levels_{task}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ AutoGluon
        predictor = TabularPredictor(
            label=target_col,
            problem_type=config['problem_type'],
            eval_metric=config['eval_metric'],
            path=model_path
        )
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è MacBook M1 (–ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç–∫–ª—é—á–∞–µ–º GPU –∏ –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –º–æ–¥–µ–ª–∏)
        fit_args = {
            'time_limit': config['time_limit'],
            'presets': 'best_quality',
            'excluded_model_types': [
                'NN_TORCH', 'NN_FASTAI', 'FASTAI', 'NeuralNetFastAI',  # GPU –º–æ–¥–µ–ª–∏
                'XGBoost', 'LightGBM',  # –ú–æ–¥–µ–ª–∏ —Å –ø—Ä–æ–±–ª–µ–º–∞–º–∏ OpenMP –Ω–∞ macOS
                'XGBoostLarge', 'LightGBMLarge'  # –ë–æ–ª—å—à–∏–µ –≤–µ—Ä—Å–∏–∏
            ],
            'num_bag_folds': 5,
            'num_stack_levels': 1,
            'verbosity': 2,
            'ag_args_fit': {
                'use_gpu': False,
                'num_gpus': 0
            }
        }
        
        # –ï—Å–ª–∏ ray –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
        if not RAY_AVAILABLE:
            logger.warning("Ray –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ")
            fit_args['num_bag_folds'] = 0  # –û—Ç–∫–ª—é—á–∞–µ–º bagging –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
            fit_args['num_stack_levels'] = 0  # –û—Ç–∫–ª—é—á–∞–µ–º stacking
        
        logger.info("–ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ AutoGluon...")
        predictor.fit(train_data, **fit_args)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        predictions = predictor.predict(test_data)
        probabilities = predictor.predict_proba(test_data) if predictor.can_predict_proba else None
        
        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        actual = test_data[target_col]
        
        if config['problem_type'] == 'binary':
            metrics = {
                'accuracy': accuracy_score(actual, predictions),
                'precision': precision_score(actual, predictions, average='weighted', zero_division=0),
                'recall': recall_score(actual, predictions, average='weighted', zero_division=0),
                'f1': f1_score(actual, predictions, average='weighted', zero_division=0)
            }
        else:  # multiclass
            metrics = {
                'accuracy': accuracy_score(actual, predictions),
                'precision': precision_score(actual, predictions, average='weighted', zero_division=0),
                'recall': recall_score(actual, predictions, average='weighted', zero_division=0),
                'f1': f1_score(actual, predictions, average='weighted', zero_division=0)
            }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self.models[task] = predictor
        
        results = {
            'task': task,
            'model_path': model_path,
            'metrics': metrics,
            'predictions': predictions,
            'probabilities': probabilities,
            'actual': actual,
            'feature_importance': predictor.feature_importance(test_data),
            'leaderboard': predictor.leaderboard(test_data, silent=True)
        }
        
        self.results[task] = results
        
        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –¥–ª—è –∑–∞–¥–∞—á–∏ {task} –æ–±—É—á–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        logger.info(f"üìä –¢–æ—á–Ω–æ—Å—Ç—å: {metrics['accuracy']:.4f}")
        
        return results
    
    def walk_forward_validation(self, df: pd.DataFrame, task: str, n_splits: int = 5) -> Dict[str, Any]:
        """
        Walk Forward –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏.
        
        Args:
            df: –î–∞–Ω–Ω—ã–µ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            task: –ù–∞–∑–≤–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏
            n_splits: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–π
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        """
        logger.info(f"–ó–∞–ø—É—Å–∫–∞–µ–º Walk Forward –≤–∞–ª–∏–¥–∞—Ü–∏—é –¥–ª—è –∑–∞–¥–∞—á–∏ {task}")
        
        data, target_col = self.prepare_data_for_task(df, task)
        config = self.task_configs[task]
        
        tscv = TimeSeriesSplit(n_splits=n_splits)
        fold_results = []
        
        for fold, (train_idx, test_idx) in enumerate(tscv.split(data)):
            logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º fold {fold + 1}/{n_splits}")
            
            train_data = data.iloc[train_idx]
            test_data = data.iloc[test_idx]
            
            # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ fold
            model_path = f"models/wf_{task}_fold_{fold}_{datetime.now().strftime('%H%M%S')}"
            
            predictor = TabularPredictor(
                label=target_col,
                problem_type=config['problem_type'],
                eval_metric=config['eval_metric'],
                path=model_path
            )
            
            # –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–±–µ–∑ GPU –∏ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π)
            wf_fit_args = {
                'time_limit': 600,  # 10 –º–∏–Ω—É—Ç –Ω–∞ fold
                'presets': 'medium_quality_faster_train',
                'excluded_model_types': [
                    'NN_TORCH', 'NN_FASTAI', 'FASTAI', 'NeuralNetFastAI',  # GPU –º–æ–¥–µ–ª–∏
                    'XGBoost', 'LightGBM',  # –ú–æ–¥–µ–ª–∏ —Å –ø—Ä–æ–±–ª–µ–º–∞–º–∏ OpenMP –Ω–∞ macOS
                    'XGBoostLarge', 'LightGBMLarge'  # –ë–æ–ª—å—à–∏–µ –≤–µ—Ä—Å–∏–∏
                ],
                'verbosity': 0,
                'ag_args_fit': {
                    'use_gpu': False,
                    'num_gpus': 0
                }
            }
            
            # –ï—Å–ª–∏ ray –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
            if not RAY_AVAILABLE:
                wf_fit_args['num_bag_folds'] = 0
                wf_fit_args['num_stack_levels'] = 0
            
            predictor.fit(train_data, **wf_fit_args)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            predictions = predictor.predict(test_data)
            actual = test_data[target_col]
            
            # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è fold
            accuracy = accuracy_score(actual, predictions)
            fold_results.append({
                'fold': fold,
                'accuracy': accuracy,
                'train_size': len(train_data),
                'test_size': len(test_data)
            })
            
            logger.info(f"Fold {fold + 1} accuracy: {accuracy:.4f}")
        
        # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        accuracies = [r['accuracy'] for r in fold_results]
        wf_results = {
            'task': task,
            'n_splits': n_splits,
            'fold_results': fold_results,
            'mean_accuracy': np.mean(accuracies),
            'std_accuracy': np.std(accuracies),
            'min_accuracy': np.min(accuracies),
            'max_accuracy': np.max(accuracies)
        }
        
        logger.info(f"‚úÖ Walk Forward –≤–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        logger.info(f"üìä –°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å: {wf_results['mean_accuracy']:.4f} ¬± {wf_results['std_accuracy']:.4f}")
        
        return wf_results
    
    def monte_carlo_validation(self, df: pd.DataFrame, task: str, n_iterations: int = 100, test_size: float = 0.2) -> Dict[str, Any]:
        """
        Monte Carlo –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏.
        
        Args:
            df: –î–∞–Ω–Ω—ã–µ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            task: –ù–∞–∑–≤–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏
            n_iterations: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π
            test_size: –î–æ–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã Monte Carlo –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        """
        logger.info(f"–ó–∞–ø—É—Å–∫–∞–µ–º Monte Carlo –≤–∞–ª–∏–¥–∞—Ü–∏—é –¥–ª—è –∑–∞–¥–∞—á–∏ {task} ({n_iterations} –∏—Ç–µ—Ä–∞—Ü–∏–π)")
        
        data, target_col = self.prepare_data_for_task(df, task)
        config = self.task_configs[task]
        
        accuracies = []
        
        for i in range(n_iterations):
            if i % 10 == 0:
                logger.info(f"–ò—Ç–µ—Ä–∞—Ü–∏—è {i + 1}/{n_iterations}")
            
            # –°–ª—É—á–∞–π–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞
            split_idx = int(len(data) * (1 - test_size))
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—ã–π —Å–¥–≤–∏–≥ –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö 10% –¥–∞–Ω–Ω—ã—Ö
            max_shift = int(len(data) * 0.1)
            shift = np.random.randint(-max_shift, max_shift)
            split_idx = max(int(len(data) * 0.5), min(int(len(data) * 0.9), split_idx + shift))
            
            train_data = data.iloc[:split_idx]
            test_data = data.iloc[split_idx:]
            
            if len(test_data) < 10:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏
                continue
            
            # –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            model_path = f"models/mc_{task}_iter_{i}_{datetime.now().strftime('%H%M%S')}"
            
            try:
                predictor = TabularPredictor(
                    label=target_col,
                    problem_type=config['problem_type'],
                    eval_metric=config['eval_metric'],
                    path=model_path
                )
                
                mc_fit_args = {
                    'time_limit': 300,  # 5 –º–∏–Ω—É—Ç –Ω–∞ –∏—Ç–µ—Ä–∞—Ü–∏—é
                    'presets': 'medium_quality_faster_train',
                    'excluded_model_types': [
                        'NN_TORCH', 'NN_FASTAI', 'FASTAI', 'NeuralNetFastAI',  # GPU –º–æ–¥–µ–ª–∏
                        'XGBoost', 'LightGBM',  # –ú–æ–¥–µ–ª–∏ —Å –ø—Ä–æ–±–ª–µ–º–∞–º–∏ OpenMP –Ω–∞ macOS
                        'XGBoostLarge', 'LightGBMLarge'  # –ë–æ–ª—å—à–∏–µ –≤–µ—Ä—Å–∏–∏
                    ],
                    'verbosity': 0,
                    'ag_args_fit': {
                        'use_gpu': False,
                        'num_gpus': 0
                    }
                }
                
                # –ï—Å–ª–∏ ray –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
                if not RAY_AVAILABLE:
                    mc_fit_args['num_bag_folds'] = 0
                    mc_fit_args['num_stack_levels'] = 0
                
                predictor.fit(train_data, **mc_fit_args)
                
                predictions = predictor.predict(test_data)
                actual = test_data[target_col]
                accuracy = accuracy_score(actual, predictions)
                accuracies.append(accuracy)
                
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –≤ –∏—Ç–µ—Ä–∞—Ü–∏–∏ {i}: {e}")
                continue
        
        if not accuracies:
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–π —É—Å–ø–µ—à–Ω–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        mc_results = {
            'task': task,
            'n_iterations': len(accuracies),
            'accuracies': accuracies,
            'mean_accuracy': np.mean(accuracies),
            'std_accuracy': np.std(accuracies),
            'min_accuracy': np.min(accuracies),
            'max_accuracy': np.max(accuracies),
            'percentile_5': np.percentile(accuracies, 5),
            'percentile_95': np.percentile(accuracies, 95),
            'stability_score': 1 - (np.std(accuracies) / np.mean(accuracies))  # –ß–µ–º –±–ª–∏–∂–µ –∫ 1, —Ç–µ–º —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ
        }
        
        logger.info(f"‚úÖ Monte Carlo –≤–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        logger.info(f"üìä –°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å: {mc_results['mean_accuracy']:.4f} ¬± {mc_results['std_accuracy']:.4f}")
        logger.info(f"üìä –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å: {mc_results['stability_score']:.4f}")
        
        return mc_results
    
    def run_complete_analysis(self, symbol: str = "BTCUSD", timeframe: str = "MN1") -> Dict[str, Any]:
        """
        –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –≤—Å–µ—Ö —Ç—Ä–µ—Ö –∑–∞–¥–∞—á.
        
        Args:
            symbol: –¢–æ—Ä–≥–æ–≤—ã–π —Å–∏–º–≤–æ–ª
            timeframe: –¢–∞–π–º—Ñ—Ä–µ–π–º
            
        Returns:
            –ü–æ–ª–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        """
        logger.info(f"üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è {symbol} {timeframe}")
        
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        raw_data = self.load_schr_data(symbol, timeframe)
        
        # 2. –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        data_with_targets = self.create_target_variables(raw_data)
        final_data = self.create_features(data_with_targets)
        
        logger.info(f"üìä –ò—Ç–æ–≥–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {len(final_data)} –∑–∞–ø–∏—Å–µ–π, {len(final_data.columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        
        complete_results = {
            'symbol': symbol,
            'timeframe': timeframe,
            'data_info': {
                'total_records': len(final_data),
                'features_count': len(final_data.columns),
                'date_range': (final_data.index.min(), final_data.index.max()) if isinstance(final_data.index, pd.DatetimeIndex) else None
            },
            'models': {},
            'validations': {}
        }
        
        # 3. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è –≤—Å–µ—Ö –∑–∞–¥–∞—á
        for task in self.task_configs.keys():
            logger.info(f"üéØ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–¥–∞—á—É: {task}")
            
            try:
                # –û–±—É—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏
                model_results = self.train_model(final_data, task)
                complete_results['models'][task] = model_results
                
                # Walk Forward –≤–∞–ª–∏–¥–∞—Ü–∏—è
                wf_results = self.walk_forward_validation(final_data, task, n_splits=3)
                complete_results['validations'][f'{task}_walk_forward'] = wf_results
                
                # Monte Carlo –≤–∞–ª–∏–¥–∞—Ü–∏—è (–º–µ–Ω—å—à–µ –∏—Ç–µ—Ä–∞—Ü–∏–π –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –≤—Ä–µ–º–µ–Ω–∏)
                mc_results = self.monte_carlo_validation(final_data, task, n_iterations=20)
                complete_results['validations'][f'{task}_monte_carlo'] = mc_results
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–¥–∞—á–∏ {task}: {e}")
                complete_results['models'][task] = {'error': str(e)}
        
        # 4. –°–≤–æ–¥–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        self._generate_summary_report(complete_results)
        
        logger.info("üéâ –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        return complete_results
    
    def _generate_summary_report(self, results: Dict[str, Any]):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–≤–æ–¥–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞."""
        logger.info("\n" + "="*80)
        logger.info("üìã –°–í–û–î–ù–´–ô –û–¢–ß–ï–¢ –ü–û –ú–û–î–ï–õ–Ø–ú SCHR LEVELS")
        logger.info("="*80)
        
        for task, model_results in results['models'].items():
            if 'error' in model_results:
                logger.info(f"‚ùå {task}: –û–®–ò–ë–ö–ê - {model_results['error']}")
                continue
            
            metrics = model_results['metrics']
            logger.info(f"\nüéØ –ó–ê–î–ê–ß–ê: {task}")
            logger.info(f"   üìä –¢–æ—á–Ω–æ—Å—Ç—å: {metrics['accuracy']:.4f}")
            logger.info(f"   üìä Precision: {metrics['precision']:.4f}")
            logger.info(f"   üìä Recall: {metrics['recall']:.4f}")
            logger.info(f"   üìä F1-score: {metrics['f1']:.4f}")
            
            # Walk Forward —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            wf_key = f'{task}_walk_forward'
            if wf_key in results['validations']:
                wf = results['validations'][wf_key]
                logger.info(f"   üîÑ Walk Forward: {wf['mean_accuracy']:.4f} ¬± {wf['std_accuracy']:.4f}")
            
            # Monte Carlo —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            mc_key = f'{task}_monte_carlo'
            if mc_key in results['validations']:
                mc = results['validations'][mc_key]
                logger.info(f"   üé≤ Monte Carlo: {mc['mean_accuracy']:.4f} ¬± {mc['std_accuracy']:.4f}")
                logger.info(f"   üé≤ –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å: {mc['stability_score']:.4f}")
        
        logger.info("\n" + "="*80)
    
    def predict(self, data: pd.DataFrame, task: str) -> pd.Series:
        """
        –ü—Ä–æ—Å—Ç—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        
        Args:
            data: –î–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            task: –ù–∞–∑–≤–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏
            
        Returns:
            –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        """
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
            model_path = f"models/schr_levels_{task}_{self.timestamp}"
            predictor = TabularPredictor.load(model_path)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            predictions = predictor.predict(data)
            return predictions
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            raise

    def predict_for_trading(self, new_data: pd.DataFrame, task: str) -> Dict[str, Any]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–π —Ç–æ—Ä–≥–æ–≤–ª–∏.
        
        Args:
            new_data: –ù–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            task: –ó–∞–¥–∞—á–∞ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            
        Returns:
            –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏
        """
        if task not in self.models:
            raise ValueError(f"–ú–æ–¥–µ–ª—å –¥–ª—è –∑–∞–¥–∞—á–∏ {task} –Ω–µ –æ–±—É—á–µ–Ω–∞")
        
        predictor = self.models[task]
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–±–µ–∑ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö)
        features_data = self.create_features(new_data)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –Ω–µ –ø—É—Å—Ç—ã–µ
        if len(features_data) == 0:
            raise ValueError("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        
        # –£–¥–∞–ª—è–µ–º —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
        target_cols = [col for col in features_data.columns if col.startswith('target_')]
        features_data = features_data.drop(columns=target_cols, errors='ignore')
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        predictions = predictor.predict(features_data)
        probabilities = predictor.predict_proba(features_data) if predictor.can_predict_proba else None
        
        return {
            'predictions': predictions,
            'probabilities': probabilities,
            'confidence': probabilities.max(axis=1) if probabilities is not None else None
        }
    
    def save_models(self, save_path: str = "models/schr_levels_production/"):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞."""
        save_path = Path(save_path)
        save_path.mkdir(parents=True, exist_ok=True)
        
        for task, predictor in self.models.items():
            model_file = save_path / f"{task}_model.pkl"
            joblib.dump(predictor, model_file)
            logger.info(f"üíæ –ú–æ–¥–µ–ª—å {task} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_file}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results_file = save_path / "analysis_results.pkl"
        joblib.dump(self.results, results_file)
        logger.info(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_file}")
    
    def load_models(self, load_path: str = "models/schr_levels_production/"):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."""
        load_path = Path(load_path)
        
        for task in self.task_configs.keys():
            model_file = load_path / f"{task}_model.pkl"
            if model_file.exists():
                self.models[task] = joblib.load(model_file)
                logger.info(f"üìÇ –ú–æ–¥–µ–ª—å {task} –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_file}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results_file = load_path / "analysis_results.pkl"
        if results_file.exists():
            self.results = joblib.load(results_file)
            logger.info(f"üìÇ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {results_file}")


def main():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞."""
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –ª–æ–≥–æ–≤ –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
    Path("logs").mkdir(exist_ok=True)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞
    pipeline = SCHRLevelsAutoMLPipeline()
    
    try:
        # –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è BTCUSD –º–µ—Å—è—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        logger.info("üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ SCHR Levels...")
        results = pipeline.run_complete_analysis(symbol="BTCUSD", timeframe="MN1")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        pipeline.save_models()
        
        # –ü—Ä–∏–º–µ—Ä –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–∑–∞–≥—Ä—É–∂–∞–µ–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ)
        logger.info("üîÆ –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è...")
        new_data = pipeline.load_schr_data("BTCUSD", "MN1").tail(10)  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 10 –∑–∞–ø–∏—Å–µ–π –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        new_data = pipeline.create_features(new_data)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö –∑–∞–¥–∞—á
        for task in pipeline.task_configs.keys():
            if task in pipeline.models:
                try:
                    prediction_results = pipeline.predict_for_trading(new_data, task)
                    logger.info(f"üîÆ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è {task}: {prediction_results['predictions']}")
                    if prediction_results['probabilities'] is not None:
                        logger.info(f"üîÆ –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏: {prediction_results['probabilities'].values}")
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è {task}: {e}")
        
        logger.info("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ: {e}")
        raise


if __name__ == "__main__":
    main()
