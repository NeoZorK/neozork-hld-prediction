# -*- coding: utf-8 -*-
"""
SCHR Levels AutoML Pipeline
Comprehensive solution for creating ML models on basis SCHR Levels indicators

Solves 3 main tasks:
1. Prediction sign PRESSURE_VECTOR (+ or -)
2. Prediction price direction for 5 periods (up/down/hold)
3. Prediction breakthrough PREDICTED_HIGH/PREDICTED_LOW or holding between them

Author: NeoZork HLDP
Version: 1.0
"""

import pandas as pd
import numpy as np
from typing import Dict, Any, List, Optional, Tuple
import logging
from pathlib import Path
import warnings
from datetime import datetime, timedelta
import joblib
import argparse
import sys
import os
from contextlib import redirect_stdout, redirect_stderr
from io import StringIO
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import TimeSeriesSplit
import matplotlib.pyplot as plt
import seaborn as sns
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn, TimeRemainingColumn, MofNCompleteColumn
from rich.panel import Panel
from rich.table import Table
from rich import print as rprint

# Disable CUDA for MacBook M1 and set OpenMP paths
import os
os.environ["CUDA_VISIBLE_DEVICES"] = ""
os.environ["AUTOGLUON_Use_GPU"] = "false"
os.environ["AUTOGLUON_Use_GPU_TORCH"] = "false"
os.environ["AUTOGLUON_Use_GPU_FASTAI"] = "false"

# Set OpenMP paths for macOS
os.environ["LDFLAGS"] = "-L/opt/homebrew/opt/libomp/lib"
os.environ["CPPFLAGS"] = "-I/opt/homebrew/opt/libomp/include"

# Configure threading for XGBoost and LightGBM to avoid OpenMP issues
os.environ['OMP_NUM_THREADS'] = '1'
os.environ['OPENBLAS_NUM_THREADS'] = '1'
os.environ['MKL_NUM_THREADS'] = '1'
os.environ['VECLIB_MAXIMUM_THREADS'] = '1'
os.environ['NUMEXPR_NUM_THREADS'] = '1'

# Suppress AutoGluon output
os.environ['AUTOGLUON_VERBOSITY'] = '0'
os.environ['AUTOGLUON_LOG_LEVEL'] = 'ERROR'
os.environ['AUTOGLUON_QUIET'] = '1'
os.environ['AUTOGLUON_SILENT'] = '1'

# AutoGluon imports
try:
 from autogluon.tabular import TabularPredictor
 AUTOGLUON_available = True
except ImportError:
 AUTOGLUON_available = False
 TabularPredictor = None

warnings.filterwarnings('ignore')

# Initialize Rich console
console = Console()

# Setup logging with minimal verbosity
logging.basicConfig(
 level=logging.WARNING, # Minimal verbosity
 format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
 handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# Suppress AutoGluon preset messages
logging.getLogger('autogluon').setLevel(logging.ERROR)
logging.getLogger('autogluon.tabular').setLevel(logging.ERROR)

# Suppress Ray messages
logging.getLogger('ray').setLevel(logging.ERROR)
os.environ['RAY_DISABLE_IMPORT_WARNING'] = '1'
os.environ['RAY_DEDUP_LOGS'] = '0'

# Function to suppress AutoGluon output
def suppress_autogluon_output():
 """Suppresses output AutoGluon including 'Preset alias specified' messages."""
 # Redirect stdout and stderr to devnull
 devnull = open(os.devnull, 'w')
 sys.stdout = devnull
 sys.stderr = devnull
 return devnull

def restore_output(devnull):
 """Restores standard output."""
 devnull.close()
 sys.stdout = sys.__stdout__
 sys.stderr = sys.__stderr__

# Custom print function to filter out preset messages
original_print = print
def filtered_print(*args, **kwargs):
 """Filters messages 'Preset alias specified'."""
 message = ' '.join(str(arg) for arg in args)
 if 'Preset alias specified' not in message:
 original_print(*args, **kwargs)

# Monkey patch print function
import builtins
builtins.print = filtered_print

# Ray import check
try:
 import ray
 RAY_available = True
 console.print("‚úÖ Ray available - will be Used parallel training", style="green")
except ImportError:
 RAY_available = False
 console.print("‚ö†Ô∏è Ray not installed - will be used sequential training", style="yellow")
 console.print("üí° to install ray execute: pip install 'ray>=2.10.0,<2.45.0'", style="blue")

# File logging setup
os.makedirs('logs', exist_ok=True)
file_handler = logging.FileHandler('logs/schr_levels_automl.log')
file_handler.setLevel(logging.INFO)
file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
file_handler.setFormatter(file_formatter)
logger.addHandler(file_handler)


class SCHRLevelsAutoMLPipeline:
 """
 Comprehensive pipeline for creating ML models on basis SCHR Levels indicators.

 Solves 3 main tasks:
 1. Prediction sign PRESSURE_VECTOR (+ or -)
 2. Prediction price direction for 5 periods (up/down/hold)
 3. Prediction breakthrough PREDICTED_HIGH/PREDICTED_LOW or holding between them
 """

 def __init__(self, data_path: str = "data/cache/csv_converted/", data_file: Optional[str] = None):
 """
 Pipeline initialization.

 Args:
 data_path: Path to folder with data
 data_file: Specific data file for Analysis
 """
 if not AUTOGLUON_available:
 raise ImportError("AutoGluon not installed. install: pip install autogluon")

 self.data_path = Path(data_path)
 self.data_file = data_file
 self.models = {}
 self.results = {}
 self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

 # settings for different tasks
 self.task_configs = {
 'pressure_vector_sign': {
 'problem_type': 'binary',
 'eval_metric': 'roc_auc',
 'time_limit': 1800 # 30 minutes
 },
 'price_direction_1period': {
 'problem_type': 'multiclass',
 'eval_metric': 'accuracy',
 'time_limit': 1800 # 30 minutes
 },
 'level_breakout': {
 'problem_type': 'multiclass',
 'eval_metric': 'accuracy',
 'time_limit': 2400 # 40 minutes
 }
 }

 console.print("üöÄ SCHR Levels AutoML Pipeline initialized", style="bold blue")

 # Informing about training mode
 if RAY_available:
 console.print("‚úÖ Ray available - will be Used parallel training", style="green")
 else:
 console.print("‚ö†Ô∏è Ray not available - will be used sequential training", style="yellow")
 console.print("üí° for acceleration install ray: pip install 'ray>=2.10.0,<2.45.0'", style="blue")

 def load_schr_data(self, symbol: str = "BTCUSD", Timeframe: str = "MN1") -> pd.dataFrame:
 """
 Loading data SCHR Levels for specified symbol and Timeframe.

 Args:
 symbol: Trading symbol (BTCUSD, EURUSD, etc.)
 Timeframe: Timeframe (MN1, W1, D1, H4, H1, M15, M5, M1)

 Returns:
 dataFrame with data SCHR Levels
 """
 if self.data_file:
 # Use specific file if specified
 file_path = Path(self.data_file)
 if not file_path.exists():
 raise FileNotfoundError(f"data file not found: {file_path}")
 console.print(f"üìÅ Loading data: {file_path.name}", style="blue")
 else:
 # Use standard path
 filename = f"CSVExport_{symbol}_PERIOD_{Timeframe}.parquet"
 file_path = self.data_path / filename
 if not file_path.exists():
 raise FileNotfoundError(f"File not found: {file_path}")
 console.print(f"üìÅ Loading data: {filename}", style="blue")

 df = pd.read_parquet(file_path)

 # checking presence of required columns
 required_cols = ['Close', 'High', 'Open', 'Low', 'Volume', 'predicted_low', 'predicted_high', 'pressure', 'pressure_vector']
 missing_cols = [col for col in required_cols if col not in df.columns]

 if missing_cols:
 logger.warning(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏: {missing_cols}")

 # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å –∫–∞–∫ datetime –µ—Å–ª–∏ –µ—Å—Ç—å
 if 'Date' in df.columns:
 df['Date'] = pd.to_datetime(df['Date'])
 df.set_index('Date', inplace=True)
 elif df.index.name != 'Date' and not isinstance(df.index, pd.DatetimeIndex):
 # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω–¥–µ–∫—Å –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
 df.index = pd.date_range(start='2020-01-01', periods=len(df), freq='MS' if Timeframe == 'MN1' else 'D')

 console.print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π with {len(df.columns)} –∫–æ–ª–æ–Ω–∫–∞–º–∏", style="green")
 return df

 def create_target_variables(self, df: pd.dataFrame) -> pd.dataFrame:
 """
 create —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö for –≤—Å–µ—Ö 3 –∑–∞–¥–∞—á.

 Args:
 df: –ò—Å—Ö–æ–¥–Ω—ã–µ data SCHR Levels

 Returns:
 dataFrame with –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ —Ü–µ–ª–µ–≤—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏
 """
 logger.info("–°–æ–∑–¥–∞–µ–º —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ for 3 –∑–∞–¥–∞—á...")

 data = df.copy()

 # –ó–∞–¥–∞—á–∞ 1: –ó–Ω–∞–∫ PRESSURE_VECTOR in —Å–ª–µ–¥—É—é—â–µ–º –ø–µ—Ä–∏–æ–¥–µ
 if 'pressure_vector' in data.columns:
 # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º NaN and inf –∑–Ω–∞—á–µ–Ω–∏—è
 pv_clean = data['pressure_vector'].replace([np.inf, -np.inf], np.nan)
 pv_sign = (pv_clean.shift(-1) > 0)
 data['target_pv_sign'] = pv_sign.astype(float) # Use float for —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
 logger.info("‚úÖ –°–æ–∑–¥–∞–Ω–∞ target_pv_sign (0=–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π, 1=–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π)")

 # –ó–∞–¥–∞—á–∞ 2: –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ü–µ–Ω—ã on 1 –ø–µ—Ä–∏–æ–¥
 if 'Close' in data.columns:
 future_returns = data['Close'].pct_change(1).shift(-1)
 # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è
 future_returns_clean = future_returns.replace([np.inf, -np.inf], np.nan)
 price_direction = pd.cut(
 future_returns_clean,
 bins=[-np.inf, -0.01, 0.01, np.inf],
 labels=[0, 1, 2] # 0=down, 1=hold, 2=up
 )
 data['target_price_direction'] = price_direction.astype(float) # Use float for —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
 logger.info("‚úÖ –°–æ–∑–¥–∞–Ω–∞ target_price_direction (0=–≤–Ω–∏–∑, 1=—É–¥–µ—Ä–∂–∞–Ω–∏–µ, 2=–≤–≤–µ—Ä—Ö) on 1 –ø–µ—Ä–∏–æ–¥")

 # –ó–∞–¥–∞—á–∞ 3: –ü—Ä–æ–±–∏—Ç–∏–µ —É—Ä–æ–≤–Ω–µ–π or —É–¥–µ—Ä–∂–∞–Ω–∏–µ between them
 if all(col in data.columns for col in ['Close', 'predicted_high', 'predicted_low']):
 close_next = data['Close'].shift(-1)
 pred_high = data['predicted_high'].replace([np.inf, -np.inf], np.nan)
 pred_low = data['predicted_low'].replace([np.inf, -np.inf], np.nan)

 # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–ª—É—á–∞–∏ with NaN in —É—Ä–æ–≤–Ω—è—Ö
 valid_levels = ~(pred_high.isna() | pred_low.isna() | close_next.isna())

 conditions = [
 (close_next > pred_high) & valid_levels, # –ü—Ä–æ–±–∏—Ç–∏–µ –≤–≤–µ—Ä—Ö
 (close_next < pred_low) & valid_levels, # –ü—Ä–æ–±–∏—Ç–∏–µ –≤–Ω–∏–∑
 (close_next >= pred_low) & (close_next <= pred_high) & valid_levels # –ú–µ–∂–¥—É —É—Ä–æ–≤–Ω—è–º–∏
 ]
 choices = [2, 0, 1] # 2=–ø—Ä–æ–±–∏—Ç–∏–µ –≤–≤–µ—Ä—Ö, 0=–ø—Ä–æ–±–∏—Ç–∏–µ –≤–Ω–∏–∑, 1=–º–µ–∂–¥—É —É—Ä–æ–≤–Ω—è–º–∏

 data['target_level_breakout'] = np.select(conditions, choices, default=1).astype(float)
 logger.info("‚úÖ –°–æ–∑–¥–∞–Ω–∞ target_level_breakout (0=–ø—Ä–æ–±–∏—Ç–∏–µ –≤–Ω–∏–∑, 1=–º–µ–∂–¥—É —É—Ä–æ–≤–Ω—è–º–∏, 2=–ø—Ä–æ–±–∏—Ç–∏–µ –≤–≤–µ—Ä—Ö)")

 # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ with NaN in —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
 target_cols = [col for col in data.columns if col.startswith('target_')]
 data = data.dropna(subset=target_cols)

 logger.info(f"–ü–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö: {len(data)} –∑–∞–ø–∏—Å–µ–π")
 return data

 def create_features(self, df: pd.dataFrame) -> pd.dataFrame:
 """
 create –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ for —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏.

 Args:
 df: data with —Ü–µ–ª–µ–≤—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏

 Returns:
 dataFrame with –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø—Ä–∏sign–º–∏
 """
 logger.info("–°–æ–∑–¥–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏...")

 data = df.copy()

 # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã on basis —Ü–µ–Ω—ã
 if 'Close' in data.columns:
 # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ
 for window in [5, 10, 20]:
 data[f'sma_{window}'] = data['Close'].rolling(window).mean()
 data[f'close_sma_{window}_ratio'] = data['Close'] / data[f'sma_{window}']

 # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
 data['volatility_5'] = data['Close'].pct_change().rolling(5).std()
 data['volatility_20'] = data['Close'].pct_change().rolling(20).std()

 # RSI —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π
 delta = data['Close'].diff()
 gain = (delta.where(delta > 0, 0)).rolling(14).mean()
 loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
 rs = gain / loss
 data['rsi'] = 100 - (100 / (1 + rs))

 # –ü—Ä–∏–∑–Ω–∞–∫–∏ on basis SCHR —É—Ä–æ–≤–Ω–µ–π
 if all(col in data.columns for col in ['Close', 'predicted_high', 'predicted_low']):
 # –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ to —É—Ä–æ–≤–Ω–µ–π
 data['distance_to_high'] = (data['predicted_high'] - data['Close']) / data['Close']
 data['distance_to_low'] = (data['Close'] - data['predicted_low']) / data['Close']
 data['levels_spread'] = (data['predicted_high'] - data['predicted_low']) / data['Close']

 # –ü–æ–∑–∏—Ü–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —É—Ä–æ–≤–Ω–µ–π (0-1, –≥–¥–µ 0.5 = in —Å–µ—Ä–µ–¥–∏–Ω–µ)
 data['position_in_levels'] = (data['Close'] - data['predicted_low']) / (data['predicted_high'] - data['predicted_low'])

 # –ü—Ä–∏–∑–Ω–∞–∫–∏ on basis –¥–∞–≤–ª–µ–Ω–∏—è
 if 'pressure' in data.columns:
 # –õ–∞–≥–∏ –¥–∞–≤–ª–µ–Ω–∏—è
 for lag in [1, 2, 3]:
 data[f'pressure_lag_{lag}'] = data['pressure'].shift(lag)

 # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ –¥–∞–≤–ª–µ–Ω–∏—è
 for window in [3, 5, 10]:
 data[f'pressure_sma_{window}'] = data['pressure'].rolling(window).mean()

 if 'pressure_vector' in data.columns:
 # –õ–∞–≥–∏ –≤–µ–∫—Ç–æ—Ä–∞ –¥–∞–≤–ª–µ–Ω–∏—è
 for lag in [1, 2, 3]:
 data[f'pv_lag_{lag}'] = data['pressure_vector'].shift(lag)

 # –ò–∑–º–µ–Ω–µ–Ω–∏–µ sign –≤–µ–∫—Ç–æ—Ä–∞ –¥–∞–≤–ª–µ–Ω–∏—è
 data['pv_sign_change'] = (data['pressure_vector'] * data['pressure_vector'].shift(1) < 0).astype(int)

 # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –µ—Å–ª–∏ –µ—Å—Ç—å datetime –∏–Ω–¥–µ–∫—Å
 if isinstance(data.index, pd.DatetimeIndex):
 data['month'] = data.index.month
 data['quarter'] = data.index.quarter
 data['year'] = data.index.year

 # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ with NaN
 # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
 data = data.replace([np.inf, -np.inf], np.nan)

 # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ —É–¥–∞–ª–µ–Ω–∏—è
 # for —á–∏—Å–ª–æ–≤—ã—Ö columns –∑–∞–ø–æ–ª–Ω—è–µ–º –º–µ–¥–∏–∞–Ω–æ–π
 numeric_cols = data.select_dtypes(include=[np.number]).columns
 for col in numeric_cols:
 if data[col].isna().any():
 data[col] = data[col].fillna(data[col].median())

 # –£–¥–∞–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏ –≥–¥–µ –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è NaN
 data = data.dropna(how='all')

 # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –µ—Å—Ç—å NaN, –∑–∞–ø–æ–ª–Ω—è–µ–º 0
 data = data.fillna(0)

 # checking on –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
 if np.isinf(data.select_dtypes(include=[np.number])).any().any():
 logger.warning("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, –∑–∞–º–µ–Ω—è–µ–º on 0")
 data = data.replace([np.inf, -np.inf], 0)

 logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(data.columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, {len(data)} –∑–∞–ø–∏—Å–µ–π")
 return data

 def prepare_data_for_task(self, df: pd.dataFrame, task: str) -> Tuple[pd.dataFrame, str]:
 """
 –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö for –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–∏.

 Args:
 df: data with –ø—Ä–∏sign–º–∏ and —Ü–µ–ª–µ–≤—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏
 task: –ù–∞–∑–≤–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏

 Returns:
 Tuple[dataFrame, target_column]
 """
 target_mapping = {
 'pressure_vector_sign': 'target_pv_sign',
 'price_direction_1period': 'target_price_direction',
 'level_breakout': 'target_level_breakout'
 }

 target_col = target_mapping[task]

 if target_col not in df.columns:
 raise ValueError(f"–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è {target_col} not found–∞")

 # –£–¥–∞–ª—è–µ–º –¥—Ä—É–≥–∏–µ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
 other_targets = [col for col in target_mapping.values() if col != target_col]
 data = df.drop(columns=other_targets, errors='ignore')

 # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ –≥–¥–µ —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è NaN
 data = data.dropna(subset=[target_col])

 logger.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã data for –∑–∞–¥–∞—á–∏ {task}: {len(data)} –∑–∞–ø–∏—Å–µ–π")
 return data, target_col

 def train_model(self, df: pd.dataFrame, task: str, test_size: float = 0.2, progress=None, task_id=None) -> Dict[str, Any]:
 """
 –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ AutoGluon for –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–∏.

 Args:
 df: –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ data
 task: –ù–∞–∑–≤–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏
 test_size: –î–æ–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö

 Returns:
 –°–ª–æ–≤–∞—Ä—å with —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è
 """
 # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ for –∑–∞–¥–∞—á–∏: {task}
 task_name = task.replace('_', ' ').title()

 data, target_col = self.prepare_data_for_task(df, task)
 config = self.task_configs[task]

 # –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö (–≤–∞–∂–Ω–æ for –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤)
 split_idx = int(len(data) * (1 - test_size))
 train_data = data.iloc[:split_idx]
 test_data = data.iloc[split_idx:]

 # –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(train_data)} –∑–∞–ø–∏—Å–µ–π
 # –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(test_data)} –∑–∞–ø–∏—Å–µ–π

 # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å for –º–æ–¥–µ–ª–∏
 model_path = f"models/schr_levels_{task}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

 # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ AutoGluon
 predictor = TabularPredictor(
 label=target_col,
 problem_type=config['problem_type'],
 eval_metric=config['eval_metric'],
 path=model_path
 )

 # settings for MacBook M1 (–æ—Ç–∫–ª—é—á–∞–µ–º —Ç–æ–ª—å–∫–æ GPU –º–æ–¥–µ–ª–∏)
 fit_args = {
 'time_limit': config['time_limit'],
 'presets': 'best_quality',
 'excluded_model_types': [
 'NN_TORCH', 'NN_FASTAI', 'FASTAI', 'NeuralNetFastAI' # –¢–æ–ª—å–∫–æ GPU –º–æ–¥–µ–ª–∏
 ],
 'num_bag_folds': 5,
 'num_stack_levels': 1,
 'verbosity': 0,
 'ag_args_fit': {
 'Use_gpu': False,
 'num_gpus': 0
 },
 # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ settings for XGBoost and LightGBM
 'hyperparameters': {
 'XGB': {
 'n_jobs': 1,
 'n_estimators': 100,
 'max_depth': 6,
 'learning_rate': 0.1
 },
 'GBM': {
 'n_jobs': 1,
 'n_estimators': 100,
 'max_depth': 6,
 'learning_rate': 0.1,
 'verbose': -1
 }
 }
 }

 # –ï—Å–ª–∏ ray not available, Use sequential training
 if not RAY_available:
 logger.warning("Ray not available - Use sequential training")
 fit_args['num_bag_folds'] = 0 # –û—Ç–∫–ª—é—á–∞–µ–º bagging for –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
 fit_args['num_stack_levels'] = 0 # –û—Ç–∫–ª—é—á–∞–µ–º stacking

 # –ü–æ–¥–∞–≤–ª—è–µ–º –≤—ã–≤–æ–¥ AutoGluon
 devnull = suppress_autogluon_output()
 try:
 # –û–±–Ω–æ–≤–ª—è–µ–º progress bar –ø–æ—Å–ª–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Ray
 if progress and task_id:
 progress.update(task_id, description=f"üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Ray and –æ–±—É—á–µ–Ω–∏–µ {task_name}...")

 # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª—è–µ–º stdout/stderr for –ø–æ–¥–∞–≤–ª–µ–Ω–∏—è preset —Å–æ–æ–±—â–µ–Ω–∏–π
 old_stdout = sys.stdout
 old_stderr = sys.stderr
 sys.stdout = devnull
 sys.stderr = devnull

 predictor.fit(train_data, **fit_args)

 # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º stdout/stderr
 sys.stdout = old_stdout
 sys.stderr = old_stderr

 # –û–±–Ω–æ–≤–ª—è–µ–º progress bar –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
 if progress and task_id:
 progress.update(task_id, description=f"‚úÖ –û–±—É—á–µ–Ω–∏–µ {task_name} COMPLETED")

 finally:
 restore_output(devnull)

 # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è on —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
 Predictions = predictor.predict(test_data)
 probabilities = predictor.predict_proba(test_data) if predictor.can_predict_proba else None

 # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
 actual = test_data[target_col]

 if config['problem_type'] == 'binary':
 metrics = {
 'accuracy': accuracy_score(actual, Predictions),
 'precision': precision_score(actual, Predictions, average='weighted', zero_division=0),
 'recall': recall_score(actual, Predictions, average='weighted', zero_division=0),
 'f1': f1_score(actual, Predictions, average='weighted', zero_division=0)
 }
 else: # multiclass
 metrics = {
 'accuracy': accuracy_score(actual, Predictions),
 'precision': precision_score(actual, Predictions, average='weighted', zero_division=0),
 'recall': recall_score(actual, Predictions, average='weighted', zero_division=0),
 'f1': f1_score(actual, Predictions, average='weighted', zero_division=0)
 }

 # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å and —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
 self.models[task] = predictor

 results = {
 'task': task,
 'model_path': model_path,
 'metrics': metrics,
 'Predictions': Predictions,
 'probabilities': probabilities,
 'actual': actual,
 'feature_importance': predictor.feature_importance(test_data),
 'leaderboard': predictor.leaderboard(test_data, silent=True)
 }

 self.results[task] = results

 logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å for –∑–∞–¥–∞—á–∏ {task} –æ–±—É—á–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
 logger.info(f"üìä –¢–æ—á–Ω–æ—Å—Ç—å: {metrics['accuracy']:.4f}")

 return results

 def walk_forward_validation(self, df: pd.dataFrame, task: str, n_splits: int = 5) -> Dict[str, Any]:
 """
 Walk Forward –≤–∞–ª–∏–¥–∞—Ü–∏—è for –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏.

 Args:
 df: data for –≤–∞–ª–∏–¥–∞—Ü–∏–∏
 task: –ù–∞–∑–≤–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏
 n_splits: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–π

 Returns:
 –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏
 """
 # Walk Forward –≤–∞–ª–∏–¥–∞—Ü–∏—è (–±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π)

 data, target_col = self.prepare_data_for_task(df, task)
 config = self.task_configs[task]

 tscv = TimeSeriesSplit(n_splits=n_splits)
 fold_results = []

 for fold, (train_idx, test_idx) in enumerate(tscv.split(data)):
 logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º fold {fold + 1}/{n_splits}")

 train_data = data.iloc[train_idx]
 test_data = data.iloc[test_idx]

 # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å on fold
 model_path = f"models/wf_{task}_fold_{fold}_{datetime.now().strftime('%H%M%S')}"

 predictor = TabularPredictor(
 label=target_col,
 problem_type=config['problem_type'],
 eval_metric=config['eval_metric'],
 path=model_path
 )

 # –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ for –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (—Ç–æ–ª—å–∫–æ –±–µ–∑ GPU)
 wf_fit_args = {
 'time_limit': 600, # 10 minutes on fold
 'presets': 'medium_quality_faster_train',
 'excluded_model_types': [
 'NN_TORCH', 'NN_FASTAI', 'FASTAI', 'NeuralNetFastAI' # –¢–æ–ª—å–∫–æ GPU –º–æ–¥–µ–ª–∏
 ],
 'verbosity': 0,
 'ag_args_fit': {
 'Use_gpu': False,
 'num_gpus': 0
 },
 # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ settings for XGBoost and LightGBM
 'hyperparameters': {
 'XGB': {
 'n_jobs': 1,
 'n_estimators': 50,
 'max_depth': 4,
 'learning_rate': 0.1
 },
 'GBM': {
 'n_jobs': 1,
 'n_estimators': 50,
 'max_depth': 4,
 'learning_rate': 0.1,
 'verbose': -1
 }
 }
 }

 # –ï—Å–ª–∏ ray not available, Use sequential training
 if not RAY_available:
 wf_fit_args['num_bag_folds'] = 0
 wf_fit_args['num_stack_levels'] = 0

 # –ü–æ–¥–∞–≤–ª—è–µ–º –≤—ã–≤–æ–¥ AutoGluon
 devnull = suppress_autogluon_output()
 try:
 predictor.fit(train_data, **wf_fit_args)
 finally:
 restore_output(devnull)

 # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
 Predictions = predictor.predict(test_data)
 actual = test_data[target_col]

 # –ú–µ—Ç—Ä–∏–∫–∏ for fold
 accuracy = accuracy_score(actual, Predictions)
 fold_results.append({
 'fold': fold,
 'accuracy': accuracy,
 'train_size': len(train_data),
 'test_size': len(test_data)
 })

 logger.info(f"Fold {fold + 1} accuracy: {accuracy:.4f}")

 # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
 accuracies = [r['accuracy'] for r in fold_results]
 wf_results = {
 'task': task,
 'n_splits': n_splits,
 'fold_results': fold_results,
 'mean_accuracy': np.mean(accuracies),
 'std_accuracy': np.std(accuracies),
 'min_accuracy': np.min(accuracies),
 'max_accuracy': np.max(accuracies)
 }

 # Walk Forward –≤–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞
 logger.info(f"üìä –°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å: {wf_results['mean_accuracy']:.4f} ¬± {wf_results['std_accuracy']:.4f}")

 return wf_results

 def monte_carlo_validation(self, df: pd.dataFrame, task: str, n_iterations: int = 100, test_size: float = 0.2) -> Dict[str, Any]:
 """
 Monte Carlo –≤–∞–ª–∏–¥–∞—Ü–∏—è for –æ—Ü–µ–Ω–∫–∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏.

 Args:
 df: data for –≤–∞–ª–∏–¥–∞—Ü–∏–∏
 task: –ù–∞–∑–≤–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏
 n_iterations: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π
 test_size: –î–æ–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö

 Returns:
 –†–µ–∑—É–ª—å—Ç–∞—Ç—ã Monte Carlo –≤–∞–ª–∏–¥–∞—Ü–∏–∏
 """
 # Monte Carlo –≤–∞–ª–∏–¥–∞—Ü–∏—è (–±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π)

 data, target_col = self.prepare_data_for_task(df, task)
 config = self.task_configs[task]

 accuracies = []

 for i in range(n_iterations):
 if i % 10 == 0:
 logger.info(f"–ò—Ç–µ—Ä–∞—Ü–∏—è {i + 1}/{n_iterations}")

 # –°–ª—É—á–∞–π–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ with —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞
 split_idx = int(len(data) * (1 - test_size))
 # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—ã–π —Å–¥–≤–∏–≥ in –ø—Ä–µ–¥–µ–ª–∞—Ö 10% –¥–∞–Ω–Ω—ã—Ö
 max_shift = int(len(data) * 0.1)
 shift = np.random.randint(-max_shift, max_shift)
 split_idx = max(int(len(data) * 0.5), min(int(len(data) * 0.9), split_idx + shift))

 train_data = data.iloc[:split_idx]
 test_data = data.iloc[split_idx:]

 if len(test_data) < 10: # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏
 continue

 # –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
 model_path = f"models/mc_{task}_iter_{i}_{datetime.now().strftime('%H%M%S')}"

 try:
 predictor = TabularPredictor(
 label=target_col,
 problem_type=config['problem_type'],
 eval_metric=config['eval_metric'],
 path=model_path
 )

 mc_fit_args = {
 'time_limit': 300, # 5 minutes on –∏—Ç–µ—Ä–∞—Ü–∏—é
 'presets': 'medium_quality_faster_train',
 'excluded_model_types': [
 'NN_TORCH', 'NN_FASTAI', 'FASTAI', 'NeuralNetFastAI' # –¢–æ–ª—å–∫–æ GPU –º–æ–¥–µ–ª–∏
 ],
 'verbosity': 0,
 'ag_args_fit': {
 'Use_gpu': False,
 'num_gpus': 0
 },
 # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ settings for XGBoost and LightGBM
 'hyperparameters': {
 'XGB': {
 'n_jobs': 1,
 'n_estimators': 30,
 'max_depth': 3,
 'learning_rate': 0.1
 },
 'GBM': {
 'n_jobs': 1,
 'n_estimators': 30,
 'max_depth': 3,
 'learning_rate': 0.1,
 'verbose': -1
 }
 }
 }

 # –ï—Å–ª–∏ ray not available, Use sequential training
 if not RAY_available:
 mc_fit_args['num_bag_folds'] = 0
 mc_fit_args['num_stack_levels'] = 0

 # –ü–æ–¥–∞–≤–ª—è–µ–º –≤—ã–≤–æ–¥ AutoGluon
 devnull = suppress_autogluon_output()
 try:
 predictor.fit(train_data, **mc_fit_args)
 finally:
 restore_output(devnull)

 Predictions = predictor.predict(test_data)
 actual = test_data[target_col]
 accuracy = accuracy_score(actual, Predictions)
 accuracies.append(accuracy)

 except Exception as e:
 logger.warning(f"–û—à–∏–±–∫–∞ in –∏—Ç–µ—Ä–∞—Ü–∏–∏ {i}: {e}")
 continue

 if not accuracies:
 raise ValueError("not —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–π —É—Å–ø–µ—à–Ω–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏")

 # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
 mc_results = {
 'task': task,
 'n_iterations': len(accuracies),
 'accuracies': accuracies,
 'mean_accuracy': np.mean(accuracies),
 'std_accuracy': np.std(accuracies),
 'min_accuracy': np.min(accuracies),
 'max_accuracy': np.max(accuracies),
 'percentile_5': np.percentile(accuracies, 5),
 'percentile_95': np.percentile(accuracies, 95),
 'stability_score': 1 - (np.std(accuracies) / np.mean(accuracies)) # –ß–µ–º –±–ª–∏–∂–µ –∫ 1, —Ç–µ–º —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ
 }

 # Monte Carlo –≤–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞
 logger.info(f"üìä –°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å: {mc_results['mean_accuracy']:.4f} ¬± {mc_results['std_accuracy']:.4f}")
 logger.info(f"üìä –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å: {mc_results['stability_score']:.4f}")

 return mc_results

 def run_complete_Analysis(self, symbol: str = "BTCUSD", Timeframe: str = "MN1") -> Dict[str, Any]:
 """
 Launch –ø–æ–ª–Ω–æ–≥–æ Analysis for –≤—Å–µ—Ö —Ç—Ä–µ—Ö –∑–∞–¥–∞—á.

 Args:
 symbol: Trading symbol
 Timeframe: Timeframe

 Returns:
 –ü–æ–ª–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã Analysis
 """
 console.print(f"üöÄ Launch–∞–µ–º –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ for {symbol} {Timeframe}", style="bold blue")

 # –°–æ–∑–¥–∞–µ–º progress bar
 with Progress(
 SpinnerColumn(),
 TextColumn("[progress.description]{task.description}"),
 BarColumn(),
 MofNCompleteColumn(),
 TimeElapsedColumn(),
 TimeRemainingColumn(),
 console=console
 ) as progress:

 # 1. Loading data
 task1 = progress.add_task("üìÅ Loading data...", total=1)
 raw_data = self.load_schr_data(symbol, Timeframe)
 progress.update(task1, COMPLETED=1)

 # 2. create —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö and –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
 task2 = progress.add_task("üîß create –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...", total=2)
 data_with_targets = self.create_target_variables(raw_data)
 progress.update(task2, advance=1)
 final_data = self.create_features(data_with_targets)
 progress.update(task2, COMPLETED=2)

 console.print(f"üìä –ò—Ç–æ–≥–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {len(final_data)} –∑–∞–ø–∏—Å–µ–π, {len(final_data.columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤", style="green")

 complete_results = {
 'symbol': symbol,
 'Timeframe': Timeframe,
 'data_info': {
 'total_records': len(final_data),
 'features_count': len(final_data.columns),
 'date_range': (final_data.index.min(), final_data.index.max()) if isinstance(final_data.index, pd.DatetimeIndex) else None
 },
 'models': {},
 'validations': {}
 }

 # 3. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π for –≤—Å–µ—Ö –∑–∞–¥–∞—á
 tasks = List(self.task_configs.keys())
 task_progress = progress.add_task("ü§ñ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...", total=len(tasks))

 for i, task in enumerate(tasks):
 # –°–æ–∑–¥–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π progress bar for –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏
 task_name = task.replace('_', ' ').title()
 task_progress_Detailed = progress.add_task(
 f"üéØ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–¥–∞—á—É: {task_name}",
 total=3
 )

 try:
 # –û–±—É—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏
 progress.update(task_progress_Detailed, description=f"ü§ñ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ {task_name}...")
 model_results = self.train_model(final_data, task, progress=progress, task_id=task_progress_Detailed)
 complete_results['models'][task] = model_results
 progress.update(task_progress_Detailed, advance=1)

 # Walk Forward –≤–∞–ª–∏–¥–∞—Ü–∏—è
 progress.update(task_progress_Detailed, description=f"üîÑ Walk Forward –≤–∞–ª–∏–¥–∞—Ü–∏—è {task_name}...")
 wf_results = self.walk_forward_validation(final_data, task, n_splits=3)
 complete_results['validations'][f'{task}_walk_forward'] = wf_results
 progress.update(task_progress_Detailed, advance=1)

 # Monte Carlo –≤–∞–ª–∏–¥–∞—Ü–∏—è
 progress.update(task_progress_Detailed, description=f"üé≤ Monte Carlo –≤–∞–ª–∏–¥–∞—Ü–∏—è {task_name}...")
 mc_results = self.monte_carlo_validation(final_data, task, n_iterations=20)
 complete_results['validations'][f'{task}_monte_carlo'] = mc_results
 progress.update(task_progress_Detailed, COMPLETED=3)

 progress.update(task_progress, advance=1)

 except Exception as e:
 console.print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–¥–∞—á–∏ {task}: {e}", style="red")
 complete_results['models'][task] = {'error': str(e)}
 progress.update(task_progress, advance=1)

 # 4. –°–≤–æ–¥–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
 self._generate_summary_Report(complete_results)

 logger.info("üéâ –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
 return complete_results

 def _generate_summary_Report(self, results: Dict[str, Any]):
 """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–≤–æ–¥–Ω–æ–≥–æ Report–∞."""
 logger.info("\n" + "="*80)
 logger.info("üìã –°–í–û–î–ù–´–ô Report on –ú–û–î–ï–õ–Ø–ú SCHR LEVELS")
 logger.info("="*80)

 for task, model_results in results['models'].items():
 if 'error' in model_results:
 logger.info(f"‚ùå {task}: –û–®–ò–ë–ö–ê - {model_results['error']}")
 continue

 metrics = model_results['metrics']
 logger.info(f"\nüéØ –ó–ê–î–ê–ß–ê: {task}")
 logger.info(f" üìä –¢–æ—á–Ω–æ—Å—Ç—å: {metrics['accuracy']:.4f}")
 logger.info(f" üìä Precision: {metrics['precision']:.4f}")
 logger.info(f" üìä Recall: {metrics['recall']:.4f}")
 logger.info(f" üìä F1-score: {metrics['f1']:.4f}")

 # Walk Forward —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
 wf_key = f'{task}_walk_forward'
 if wf_key in results['validations']:
 wf = results['validations'][wf_key]
 logger.info(f" üîÑ Walk Forward: {wf['mean_accuracy']:.4f} ¬± {wf['std_accuracy']:.4f}")

 # Monte Carlo —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
 mc_key = f'{task}_monte_carlo'
 if mc_key in results['validations']:
 mc = results['validations'][mc_key]
 logger.info(f" üé≤ Monte Carlo: {mc['mean_accuracy']:.4f} ¬± {mc['std_accuracy']:.4f}")
 logger.info(f" üé≤ –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å: {mc['stability_score']:.4f}")

 logger.info("\n" + "="*80)

 def predict(self, data: pd.dataFrame, task: str) -> pd.Series:
 """
 –ü—Ä–æ—Å—Ç—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è for —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

 Args:
 data: data for –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
 task: –ù–∞–∑–≤–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏

 Returns:
 –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
 """
 try:
 # Loading –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
 model_path = f"models/schr_levels_{task}_{self.timestamp}"
 predictor = TabularPredictor.load(model_path)

 # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
 Predictions = predictor.predict(data)
 return Predictions

 except Exception as e:
 logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
 raise

 def predict_for_trading(self, new_data: pd.dataFrame, task: str) -> Dict[str, Any]:
 """
 –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è for —Ä–µ–∞–ª—å–Ω–æ–π —Ç–æ—Ä–≥–æ–≤–ª–∏.

 Args:
 new_data: –ù–æ–≤—ã–µ data for –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
 task: –ó–∞–¥–∞—á–∞ for –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è

 Returns:
 –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è with –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏
 """
 if task not in self.models:
 raise ValueError(f"–ú–æ–¥–µ–ª—å for –∑–∞–¥–∞—á–∏ {task} not –æ–±—É—á–µ–Ω–∞")

 predictor = self.models[task]

 # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ for –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–±–µ–∑ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö)
 features_data = self.create_features(new_data)

 # checking, —á—Ç–æ data not –ø—É—Å—Ç—ã–µ
 if len(features_data) == 0:
 raise ValueError("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö for –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

 # –£–¥–∞–ª—è–µ–º —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
 target_cols = [col for col in features_data.columns if col.startswith('target_')]
 features_data = features_data.drop(columns=target_cols, errors='ignore')

 # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
 Predictions = predictor.predict(features_data)
 probabilities = predictor.predict_proba(features_data) if predictor.can_predict_proba else None

 return {
 'Predictions': Predictions,
 'probabilities': probabilities,
 'confidence': probabilities.max(axis=1) if probabilities is not None else None
 }

 def save_models(self, save_path: str = "models/schr_levels_production/"):
 """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π for –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞."""
 save_path = Path(save_path)
 save_path.mkdir(parents=True, exist_ok=True)

 for task, predictor in self.models.items():
 model_file = save_path / f"{task}_model.pkl"
 joblib.dump(predictor, model_file)
 logger.info(f"üíæ –ú–æ–¥–µ–ª—å {task} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_file}")

 # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
 results_file = save_path / "Analysis_results.pkl"
 joblib.dump(self.results, results_file)
 logger.info(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã Analysis —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_file}")

 def load_models(self, load_path: str = "models/schr_levels_production/"):
 """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."""
 load_path = Path(load_path)

 for task in self.task_configs.keys():
 model_file = load_path / f"{task}_model.pkl"
 if model_file.exists():
 self.models[task] = joblib.load(model_file)
 logger.info(f"üìÇ –ú–æ–¥–µ–ª—å {task} –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_file}")

 # Loading —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
 results_file = load_path / "Analysis_results.pkl"
 if results_file.exists():
 self.results = joblib.load(results_file)
 logger.info(f"üìÇ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã Analysis –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {results_file}")




def parse_arguments():
 """–ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏."""
 parser = argparse.ArgumentParser(
 description="SCHR Levels AutoML Pipeline - Comprehensive solution for creating ML models",
 formatter_class=argparse.RawDescriptionHelpFormatter,
 epilog="""
examples –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
 python schr-levels-gluon.py # –ê–Ω–∞–ª–∏–∑ on —É–º–æ–ª—á–∞–Ω–∏—é (BTCUSD MN1)
 python schr-levels-gluon.py -f data/GBPUSD.parquet # –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
 python schr-levels-gluon.py -s EURUSD -t W1 # –ê–Ω–∞–ª–∏–∑ EURUSD –Ω–µ–¥–µ–ª—å–Ω—ã–µ data
 python schr-levels-gluon.py --symbol GBPUSD --Timeframe D1 # –ê–Ω–∞–ª–∏–∑ GBPUSD –¥–Ω–µ–≤–Ω—ã–µ data
 """
 )

 parser.add_argument(
 '-f', '--file',
 type=str,
 help='–ü—É—Ç—å –∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É —Ñ–∞–π–ª—É –¥–∞–Ω–Ω—ã—Ö for Analysis'
 )

 parser.add_argument(
 '-s', '--symbol',
 type=str,
 default='BTCUSD',
 help='Trading symbol (on —É–º–æ–ª—á–∞–Ω–∏—é: BTCUSD)'
 )

 parser.add_argument(
 '-t', '--Timeframe',
 type=str,
 default='MN1',
 help='Timeframe (on —É–º–æ–ª—á–∞–Ω–∏—é: MN1)'
 )

 parser.add_argument(
 '--data-path',
 type=str,
 default='data/cache/csv_converted/',
 help='Path to folder with data (on —É–º–æ–ª—á–∞–Ω–∏—é: data/cache/csv_converted/)'
 )

 parser.add_argument(
 '--models-path',
 type=str,
 default='models',
 help='Path to folder for —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π (on —É–º–æ–ª—á–∞–Ω–∏—é: models)'
 )

 return parser.parse_args()


def main():
 """–û—Å–Ω–æ–≤–Ω–∞—è function with –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π CLI –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤."""
 args = parse_arguments()

 try:
 # –°–æ–∑–¥–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω with –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
 pipeline = SCHRLevelsAutoMLPipeline(
 data_path=args.data_path,
 data_file=args.file
 )

 # Launch–∞–µ–º –∞–Ω–∞–ª–∏–∑
 if args.file:
 console.print(f"üöÄ Launch–∞–µ–º –∞–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞: {args.file}", style="bold blue")
 results = pipeline.run_complete_Analysis("CUSTOM", "CUSTOM")
 else:
 console.print(f"üöÄ Launch–∞–µ–º –∞–Ω–∞–ª–∏–∑ for {args.symbol} {args.Timeframe}", style="bold blue")
 results = pipeline.run_complete_Analysis(args.symbol, args.Timeframe)

 # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
 pipeline.save_models()

 # example –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (Loading –Ω–æ–≤—ã–µ data)
 console.print("üîÆ –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è...", style="blue")
 if args.file:
 new_data = pipeline.load_schr_data().tail(10)
 else:
 new_data = pipeline.load_schr_data(args.symbol, args.Timeframe).tail(10)

 # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ for –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
 new_data = pipeline.create_features(new_data)

 # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è for –≤—Å–µ—Ö –∑–∞–¥–∞—á
 for task in pipeline.task_configs.keys():
 if task in pipeline.models:
 try:
 Prediction_results = pipeline.predict_for_trading(new_data, task)
 console.print(f"üîÆ Prediction for {task}: {Prediction_results['Predictions']}", style="green")
 if Prediction_results['probabilities'] is not None:
 console.print(f"üîÆ –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏: {Prediction_results['probabilities'].values}", style="cyan")
 except Exception as e:
 console.print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è for {task}: {e}", style="red")

 console.print("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!", style="bold green")

 except Exception as e:
 console.print(f"‚ùå –û—à–∏–±–∫–∞ in –æ—Å–Ω–æ–≤–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ: {e}", style="bold red")
 raise


if __name__ == "__main__":
 main()
